{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navrat/semi_sup_learning_text_clf/blob/main/Semi_Supervised_self_training_sklearn_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C_pmDcpcfyaM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as Data\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa3rhWCfRR_v",
        "outputId": "86e0b262-1dc8-4eb1-8775-0de1185dfbb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.21.6\n",
            "1.0.2\n"
          ]
        }
      ],
      "source": [
        "print(np.__version__)\n",
        "import sklearn\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CINtLiQgPlK",
        "outputId": "ae1144a6-a6f6-46a7-9f9b-821d17fb7de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7epwVF9xRFok"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy==1.16.2\n",
        "# import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I0Z7355mPVhd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EGkMFYdhOv01"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas==1.2.1\n",
        "# import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ueImsZ8KKQKo"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade scikit-learn  # Do this to use sklearn SelfTrainingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IH0_ckVheY4I"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# np.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "feLsjKVQg-mk"
      },
      "outputs": [],
      "source": [
        "# %cd \"/content/drive/My Drive/Colab Notebooks/MixText-master\"\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9J5BgqtbO9Q8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wfoKofTO9DH",
        "outputId": "4f7ffcbd-4f9a-4d1f-f5cb-5aba78b1d810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314 documents\n",
            "20 categories\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data = fetch_20newsgroups(subset='train', categories=None)\n",
        "print(\"%d documents\" % len(data.filenames))\n",
        "print(\"%d categories\" % len(data.target_names))\n",
        "print()\n",
        "\n",
        "# Parameters\n",
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "# Supervised Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(**sdg_params)),\n",
        "])\n",
        "\n",
        "# SelfTraining Pipeline\n",
        "st_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\n",
        "])\n",
        "\n",
        "\n",
        "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hhD_iWF4SUS",
        "outputId": "a96b6358-514e-4c51-cc9e-086a826a0821"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "type(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ociXTaQEYuJW",
        "outputId": "6cd60f95-1122-47aa-884f-9fa067e93a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.709\n",
            "Accuracy Score:  0.7090844821491693\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2569 new labels.\n",
            "End of iteration 2, added 761 new labels.\n",
            "End of iteration 3, added 211 new labels.\n",
            "End of iteration 4, added 69 new labels.\n",
            "End of iteration 5, added 35 new labels.\n",
            "End of iteration 6, added 17 new labels.\n",
            "End of iteration 7, added 14 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.796\n",
            "Accuracy Score:  0.7956875220926122\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2193 new labels.\n",
            "End of iteration 2, added 700 new labels.\n",
            "End of iteration 3, added 240 new labels.\n",
            "End of iteration 4, added 100 new labels.\n",
            "End of iteration 5, added 38 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 18 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.794\n",
            "Accuracy Score:  0.7939201131141747\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1881 new labels.\n",
            "End of iteration 2, added 637 new labels.\n",
            "End of iteration 3, added 237 new labels.\n",
            "End of iteration 4, added 80 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 19 new labels.\n",
            "End of iteration 8, added 14 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 13 new labels.\n",
            "Micro-averaged F1 score on test set: 0.796\n",
            "Accuracy Score:  0.7956875220926122\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1548 new labels.\n",
            "End of iteration 2, added 555 new labels.\n",
            "End of iteration 3, added 211 new labels.\n",
            "End of iteration 4, added 66 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 25 new labels.\n",
            "End of iteration 7, added 17 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.783\n",
            "Accuracy Score:  0.782608695652174\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1174 new labels.\n",
            "End of iteration 2, added 377 new labels.\n",
            "End of iteration 3, added 166 new labels.\n",
            "End of iteration 4, added 109 new labels.\n",
            "End of iteration 5, added 58 new labels.\n",
            "End of iteration 6, added 30 new labels.\n",
            "End of iteration 7, added 13 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.764\n",
            "Accuracy Score:  0.7638741604807352\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.807\n",
            "Accuracy Score:  0.806998939554613\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3109 new labels.\n",
            "End of iteration 2, added 456 new labels.\n",
            "End of iteration 3, added 99 new labels.\n",
            "End of iteration 4, added 45 new labels.\n",
            "End of iteration 5, added 19 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.848\n",
            "Accuracy Score:  0.8480028278543655\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2772 new labels.\n",
            "End of iteration 2, added 483 new labels.\n",
            "End of iteration 3, added 110 new labels.\n",
            "End of iteration 4, added 41 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 11 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.854\n",
            "Accuracy Score:  0.8536585365853658\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2411 new labels.\n",
            "End of iteration 2, added 453 new labels.\n",
            "End of iteration 3, added 149 new labels.\n",
            "End of iteration 4, added 43 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 13 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.849\n",
            "Accuracy Score:  0.8494167550371156\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2032 new labels.\n",
            "End of iteration 2, added 383 new labels.\n",
            "End of iteration 3, added 126 new labels.\n",
            "End of iteration 4, added 44 new labels.\n",
            "End of iteration 5, added 25 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.847\n",
            "Accuracy Score:  0.8472958642629904\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1545 new labels.\n",
            "End of iteration 2, added 309 new labels.\n",
            "End of iteration 3, added 86 new labels.\n",
            "End of iteration 4, added 42 new labels.\n",
            "End of iteration 5, added 16 new labels.\n",
            "End of iteration 6, added 16 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 9 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.835\n",
            "Accuracy Score:  0.8352774832096147\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.836\n",
            "Accuracy Score:  0.8363379285966772\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3327 new labels.\n",
            "End of iteration 2, added 327 new labels.\n",
            "End of iteration 3, added 64 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.8688582537999293\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3024 new labels.\n",
            "End of iteration 2, added 320 new labels.\n",
            "End of iteration 3, added 75 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 17 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.8695652173913043\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2699 new labels.\n",
            "End of iteration 2, added 324 new labels.\n",
            "End of iteration 3, added 88 new labels.\n",
            "End of iteration 4, added 26 new labels.\n",
            "End of iteration 5, added 13 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.868\n",
            "Accuracy Score:  0.8677978084128667\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2300 new labels.\n",
            "End of iteration 2, added 298 new labels.\n",
            "End of iteration 3, added 74 new labels.\n",
            "End of iteration 4, added 23 new labels.\n",
            "End of iteration 5, added 9 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 8 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.860\n",
            "Accuracy Score:  0.8600212089077413\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1750 new labels.\n",
            "End of iteration 2, added 255 new labels.\n",
            "End of iteration 3, added 70 new labels.\n",
            "End of iteration 4, added 19 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.856\n",
            "Accuracy Score:  0.856486390950866\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.858\n",
            "Accuracy Score:  0.8582537999293036\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3458 new labels.\n",
            "End of iteration 2, added 254 new labels.\n",
            "End of iteration 3, added 42 new labels.\n",
            "End of iteration 4, added 20 new labels.\n",
            "End of iteration 5, added 6 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.885\n",
            "Accuracy Score:  0.8851184164015553\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3175 new labels.\n",
            "End of iteration 2, added 259 new labels.\n",
            "End of iteration 3, added 49 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.883\n",
            "Accuracy Score:  0.8829975256274302\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2840 new labels.\n",
            "End of iteration 2, added 269 new labels.\n",
            "End of iteration 3, added 49 new labels.\n",
            "End of iteration 4, added 15 new labels.\n",
            "End of iteration 5, added 13 new labels.\n",
            "End of iteration 6, added 8 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 8 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.879\n",
            "Accuracy Score:  0.879462707670555\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2457 new labels.\n",
            "End of iteration 2, added 261 new labels.\n",
            "End of iteration 3, added 48 new labels.\n",
            "End of iteration 4, added 21 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.873\n",
            "Accuracy Score:  0.8727465535524921\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1879 new labels.\n",
            "End of iteration 2, added 200 new labels.\n",
            "End of iteration 3, added 41 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.8685047720042418\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.8699186991869918\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3547 new labels.\n",
            "End of iteration 2, added 208 new labels.\n",
            "End of iteration 3, added 45 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 4 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.895\n",
            "Accuracy Score:  0.8953693884764935\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3287 new labels.\n",
            "End of iteration 2, added 204 new labels.\n",
            "End of iteration 3, added 40 new labels.\n",
            "End of iteration 4, added 11 new labels.\n",
            "End of iteration 5, added 5 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.896\n",
            "Accuracy Score:  0.896429833863556\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2931 new labels.\n",
            "End of iteration 2, added 219 new labels.\n",
            "End of iteration 3, added 39 new labels.\n",
            "End of iteration 4, added 14 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 4 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 8 new labels.\n",
            "Micro-averaged F1 score on test set: 0.889\n",
            "Accuracy Score:  0.8893601979498056\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2543 new labels.\n",
            "End of iteration 2, added 207 new labels.\n",
            "End of iteration 3, added 44 new labels.\n",
            "End of iteration 4, added 14 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.888\n",
            "Accuracy Score:  0.887592788971368\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1926 new labels.\n",
            "End of iteration 2, added 169 new labels.\n",
            "End of iteration 3, added 42 new labels.\n",
            "End of iteration 4, added 18 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 11 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.883\n",
            "Accuracy Score:  0.8829975256274302\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SGD - Log Loss(Logistic regression).\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "      print(\"Supervised SGDClassifier on \"+str(n)+\"% of the training data:\")\n",
        "      \n",
        "      eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), threshold = t, verbose=True)),\n",
        "        ])\n",
        "        eval_and_print_metrics(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuaecprcLFwE",
        "outputId": "1ede6f84-c8d6-4a43-ac59-63b6a336ad3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314 documents\n",
            "20 categories\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = fetch_20newsgroups(subset='train', categories=None)\n",
        "print(\"%d documents\" % len(data.filenames))\n",
        "print(\"%d categories\" % len(data.target_names))\n",
        "print()\n",
        "\n",
        "\n",
        "def eval_and_print_metrics_df(clf, X_train, y_train, X_test, y_test, thresh = None, kbest = None):\n",
        "\n",
        "    dict1 = {}\n",
        "\n",
        "\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "    dict1['Labeled'] = len(X_train) - sum(1 for x in y_train if x == -1)\n",
        "    dict1['UnLabeled'] = sum(1 for x in y_train if x == -1)\n",
        "\n",
        "\n",
        "    \n",
        "    # if sum(1 for x in y_train if x == -1) == 0:\n",
        "    #     dict1['type'] = 'Supervised'\n",
        "    # else:\n",
        "    #     dict1['type'] = 'Semi-Supervised'\n",
        "\n",
        "    dict1['Threshold'] = thresh\n",
        "    dict1['K-Best'] = kbest\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    dict1['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "    return dict1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "agoXVj3bbmKv"
      },
      "outputs": [],
      "source": [
        "## Running individual sklearn algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RryyG0TK6Ci",
        "outputId": "9c7ca77d-a93e-4cdb-f7b9-3bd3e6263915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.545\n",
            "Accuracy Score:  0.545068928950159\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.055\n",
            "Accuracy Score:  0.055496641922940966\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.055\n",
            "Accuracy Score:  0.05514316012725345\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.054\n",
            "Accuracy Score:  0.0544361965358784\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.054\n",
            "Accuracy Score:  0.05408271474019088\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.053\n",
            "Accuracy Score:  0.053375751148815834\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.706\n",
            "Accuracy Score:  0.7062566277836692\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.063\n",
            "Accuracy Score:  0.06327324142806645\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.061\n",
            "Accuracy Score:  0.06115235065394132\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.060\n",
            "Accuracy Score:  0.060445387062566275\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.060\n",
            "Accuracy Score:  0.05973842347119123\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.060\n",
            "Accuracy Score:  0.05973842347119123\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.742\n",
            "Accuracy Score:  0.7423117709437964\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.080\n",
            "Accuracy Score:  0.07953340402969247\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.072\n",
            "Accuracy Score:  0.07246376811594203\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.075\n",
            "Accuracy Score:  0.07458465889006716\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.074\n",
            "Accuracy Score:  0.0735242135030046\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.106\n",
            "Accuracy Score:  0.10604453870625663\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.771\n",
            "Accuracy Score:  0.7709437963944857\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.112\n",
            "Accuracy Score:  0.11170024743725698\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.096\n",
            "Accuracy Score:  0.09579356663131848\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.083\n",
            "Accuracy Score:  0.08271474019088017\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.078\n",
            "Accuracy Score:  0.07811947684694238\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.074\n",
            "Accuracy Score:  0.07423117709437964\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.799\n",
            "Accuracy Score:  0.7992223400494874\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.143\n",
            "Accuracy Score:  0.14280664545775892\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.117\n",
            "Accuracy Score:  0.11664899257688228\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.105\n",
            "Accuracy Score:  0.10533757511488158\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.099\n",
            "Accuracy Score:  0.09932838458819371\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.094\n",
            "Accuracy Score:  0.09402615765288087\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_nb_ng = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "      print(\"Supervised MNB on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', MultinomialNB()),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_nb_ng = df_nb_ng.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for kb in kbest_list:\n",
        "        kbest = int(len(X_u50)/kb)\n",
        "        print(\"---------------------------------K-Best = \", kbest,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(MultinomialNB(), criterion = 'k_best', k_best = kbest, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test, thresh = None, kbest = kbest)\n",
        "        df_nb_ng = df_nb_ng.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "cG5CzJbvWELR",
        "outputId": "6d1fc184-e2fd-4106-c17f-ae4cc4322ffd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>K-Best</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>869.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.545069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.055497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.055143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.054436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.054083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.053376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.706257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.063273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.061152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.060445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.059738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.059738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.742312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.079533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.072464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.074585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.073524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.106045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.770944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.111700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.095794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.082715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.078119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.074231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.799222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.142807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.116649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.105338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.099328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.094026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Labeled  UnLabeled  Threshold  K-Best  Accuracy\n",
              "0     869.0        0.0        NaN     NaN  0.545069\n",
              "1     869.0     4307.0        NaN  1076.0  0.055497\n",
              "2     869.0     4307.0        NaN   861.0  0.055143\n",
              "3     869.0     4307.0        NaN   717.0  0.054436\n",
              "4     869.0     4307.0        NaN   615.0  0.054083\n",
              "5     869.0     4307.0        NaN   538.0  0.053376\n",
              "6    1695.0        0.0        NaN     NaN  0.706257\n",
              "7    1695.0     4307.0        NaN  1076.0  0.063273\n",
              "8    1695.0     4307.0        NaN   861.0  0.061152\n",
              "9    1695.0     4307.0        NaN   717.0  0.060445\n",
              "10   1695.0     4307.0        NaN   615.0  0.059738\n",
              "11   1695.0     4307.0        NaN   538.0  0.059738\n",
              "12   2541.0        0.0        NaN     NaN  0.742312\n",
              "13   2541.0     4307.0        NaN  1076.0  0.079533\n",
              "14   2541.0     4307.0        NaN   861.0  0.072464\n",
              "15   2541.0     4307.0        NaN   717.0  0.074585\n",
              "16   2541.0     4307.0        NaN   615.0  0.073524\n",
              "17   2541.0     4307.0        NaN   538.0  0.106045\n",
              "18   3349.0        0.0        NaN     NaN  0.770944\n",
              "19   3349.0     4307.0        NaN  1076.0  0.111700\n",
              "20   3349.0     4307.0        NaN   861.0  0.095794\n",
              "21   3349.0     4307.0        NaN   717.0  0.082715\n",
              "22   3349.0     4307.0        NaN   615.0  0.078119\n",
              "23   3349.0     4307.0        NaN   538.0  0.074231\n",
              "24   4178.0        0.0        NaN     NaN  0.799222\n",
              "25   4178.0     4307.0        NaN  1076.0  0.142807\n",
              "26   4178.0     4307.0        NaN   861.0  0.116649\n",
              "27   4178.0     4307.0        NaN   717.0  0.105338\n",
              "28   4178.0     4307.0        NaN   615.0  0.099328\n",
              "29   4178.0     4307.0        NaN   538.0  0.094026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df_nb_ng[['Labeled', 'UnLabeled', 'Threshold', 'K-Best', 'Accuracy']].sort_values(['Labeled', 'UnLabeled']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "BllNtJjrdu7b",
        "outputId": "42072664-09f9-434e-8343-21e229714347"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>K-Best</th>\n",
              "      <th>NaN</th>\n",
              "      <th>538.0</th>\n",
              "      <th>615.0</th>\n",
              "      <th>717.0</th>\n",
              "      <th>861.0</th>\n",
              "      <th>1076.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">869.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.545069</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.053376</td>\n",
              "      <td>0.054083</td>\n",
              "      <td>0.054436</td>\n",
              "      <td>0.055143</td>\n",
              "      <td>0.055497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1695.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.706257</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.059738</td>\n",
              "      <td>0.059738</td>\n",
              "      <td>0.060445</td>\n",
              "      <td>0.061152</td>\n",
              "      <td>0.063273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2541.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.742312</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.106045</td>\n",
              "      <td>0.073524</td>\n",
              "      <td>0.074585</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.079533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">3349.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.770944</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.074231</td>\n",
              "      <td>0.078119</td>\n",
              "      <td>0.082715</td>\n",
              "      <td>0.095794</td>\n",
              "      <td>0.111700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">4178.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.799222</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.094026</td>\n",
              "      <td>0.099328</td>\n",
              "      <td>0.105338</td>\n",
              "      <td>0.116649</td>\n",
              "      <td>0.142807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "K-Best               NaN       538.0     615.0     717.0     861.0     1076.0\n",
              "Labeled UnLabeled                                                            \n",
              "869.0   0.0        0.545069       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.053376  0.054083  0.054436  0.055143  0.055497\n",
              "1695.0  0.0        0.706257       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.059738  0.059738  0.060445  0.061152  0.063273\n",
              "2541.0  0.0        0.742312       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.106045  0.073524  0.074585  0.072464  0.079533\n",
              "3349.0  0.0        0.770944       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.074231  0.078119  0.082715  0.095794  0.111700\n",
              "4178.0  0.0        0.799222       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.094026  0.099328  0.105338  0.116649  0.142807"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_nb_ng.pivot(index=['Labeled', 'UnLabeled'], columns='K-Best')['Accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vADSNzoftiZ4",
        "outputId": "0101ba78-c504-45be-a284-b389a5d26e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.654\n",
            "Accuracy Score:  0.6535878402262284\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 634 new labels.\n",
            "End of iteration 2, added 421 new labels.\n",
            "End of iteration 3, added 476 new labels.\n",
            "End of iteration 4, added 724 new labels.\n",
            "End of iteration 5, added 842 new labels.\n",
            "End of iteration 6, added 877 new labels.\n",
            "End of iteration 7, added 266 new labels.\n",
            "End of iteration 8, added 57 new labels.\n",
            "End of iteration 9, added 10 new labels.\n",
            "Micro-averaged F1 score on test set: 0.198\n",
            "Accuracy Score:  0.19759632378932485\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 301 new labels.\n",
            "End of iteration 2, added 182 new labels.\n",
            "End of iteration 3, added 99 new labels.\n",
            "End of iteration 4, added 192 new labels.\n",
            "End of iteration 5, added 430 new labels.\n",
            "End of iteration 6, added 564 new labels.\n",
            "End of iteration 7, added 842 new labels.\n",
            "End of iteration 8, added 1152 new labels.\n",
            "End of iteration 9, added 499 new labels.\n",
            "End of iteration 10, added 37 new labels.\n",
            "Micro-averaged F1 score on test set: 0.124\n",
            "Accuracy Score:  0.12371862849063273\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 154 new labels.\n",
            "End of iteration 2, added 98 new labels.\n",
            "End of iteration 3, added 32 new labels.\n",
            "End of iteration 4, added 25 new labels.\n",
            "End of iteration 5, added 28 new labels.\n",
            "End of iteration 6, added 27 new labels.\n",
            "End of iteration 7, added 41 new labels.\n",
            "End of iteration 8, added 50 new labels.\n",
            "End of iteration 9, added 91 new labels.\n",
            "End of iteration 10, added 159 new labels.\n",
            "Micro-averaged F1 score on test set: 0.254\n",
            "Accuracy Score:  0.25379992930364087\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 61 new labels.\n",
            "End of iteration 2, added 41 new labels.\n",
            "End of iteration 3, added 23 new labels.\n",
            "End of iteration 4, added 34 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 8 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.574\n",
            "Accuracy Score:  0.5740544361965358\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 8 new labels.\n",
            "End of iteration 2, added 20 new labels.\n",
            "End of iteration 3, added 21 new labels.\n",
            "End of iteration 4, added 10 new labels.\n",
            "End of iteration 5, added 2 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.646\n",
            "Accuracy Score:  0.6458112407211029\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.725\n",
            "Accuracy Score:  0.7253446447507953\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1147 new labels.\n",
            "End of iteration 2, added 405 new labels.\n",
            "End of iteration 3, added 217 new labels.\n",
            "End of iteration 4, added 197 new labels.\n",
            "End of iteration 5, added 237 new labels.\n",
            "End of iteration 6, added 303 new labels.\n",
            "End of iteration 7, added 289 new labels.\n",
            "End of iteration 8, added 263 new labels.\n",
            "End of iteration 9, added 277 new labels.\n",
            "End of iteration 10, added 268 new labels.\n",
            "Micro-averaged F1 score on test set: 0.275\n",
            "Accuracy Score:  0.2753623188405797\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 664 new labels.\n",
            "End of iteration 2, added 229 new labels.\n",
            "End of iteration 3, added 113 new labels.\n",
            "End of iteration 4, added 85 new labels.\n",
            "End of iteration 5, added 72 new labels.\n",
            "End of iteration 6, added 101 new labels.\n",
            "End of iteration 7, added 118 new labels.\n",
            "End of iteration 8, added 178 new labels.\n",
            "End of iteration 9, added 212 new labels.\n",
            "End of iteration 10, added 234 new labels.\n",
            "Micro-averaged F1 score on test set: 0.341\n",
            "Accuracy Score:  0.3411099328384588\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 351 new labels.\n",
            "End of iteration 2, added 144 new labels.\n",
            "End of iteration 3, added 56 new labels.\n",
            "End of iteration 4, added 39 new labels.\n",
            "End of iteration 5, added 30 new labels.\n",
            "End of iteration 6, added 18 new labels.\n",
            "End of iteration 7, added 15 new labels.\n",
            "End of iteration 8, added 19 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 9 new labels.\n",
            "Micro-averaged F1 score on test set: 0.654\n",
            "Accuracy Score:  0.6542948038176034\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 167 new labels.\n",
            "End of iteration 2, added 48 new labels.\n",
            "End of iteration 3, added 34 new labels.\n",
            "End of iteration 4, added 32 new labels.\n",
            "End of iteration 5, added 29 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 11 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 8 new labels.\n",
            "Micro-averaged F1 score on test set: 0.705\n",
            "Accuracy Score:  0.7051961823966065\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 81 new labels.\n",
            "End of iteration 2, added 31 new labels.\n",
            "End of iteration 3, added 11 new labels.\n",
            "End of iteration 4, added 5 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.726\n",
            "Accuracy Score:  0.7264050901378579\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.759\n",
            "Accuracy Score:  0.7589254153411099\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1511 new labels.\n",
            "End of iteration 2, added 355 new labels.\n",
            "End of iteration 3, added 194 new labels.\n",
            "End of iteration 4, added 118 new labels.\n",
            "End of iteration 5, added 80 new labels.\n",
            "End of iteration 6, added 67 new labels.\n",
            "End of iteration 7, added 77 new labels.\n",
            "End of iteration 8, added 87 new labels.\n",
            "End of iteration 9, added 128 new labels.\n",
            "End of iteration 10, added 128 new labels.\n",
            "Micro-averaged F1 score on test set: 0.578\n",
            "Accuracy Score:  0.5782962177447861\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 939 new labels.\n",
            "End of iteration 2, added 272 new labels.\n",
            "End of iteration 3, added 93 new labels.\n",
            "End of iteration 4, added 68 new labels.\n",
            "End of iteration 5, added 60 new labels.\n",
            "End of iteration 6, added 44 new labels.\n",
            "End of iteration 7, added 48 new labels.\n",
            "End of iteration 8, added 39 new labels.\n",
            "End of iteration 9, added 16 new labels.\n",
            "End of iteration 10, added 23 new labels.\n",
            "Micro-averaged F1 score on test set: 0.675\n",
            "Accuracy Score:  0.6747967479674797\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 494 new labels.\n",
            "End of iteration 2, added 168 new labels.\n",
            "End of iteration 3, added 103 new labels.\n",
            "End of iteration 4, added 59 new labels.\n",
            "End of iteration 5, added 40 new labels.\n",
            "End of iteration 6, added 25 new labels.\n",
            "End of iteration 7, added 16 new labels.\n",
            "End of iteration 8, added 20 new labels.\n",
            "End of iteration 9, added 14 new labels.\n",
            "End of iteration 10, added 8 new labels.\n",
            "Micro-averaged F1 score on test set: 0.723\n",
            "Accuracy Score:  0.7225167903852951\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 255 new labels.\n",
            "End of iteration 2, added 81 new labels.\n",
            "End of iteration 3, added 54 new labels.\n",
            "End of iteration 4, added 38 new labels.\n",
            "End of iteration 5, added 20 new labels.\n",
            "End of iteration 6, added 26 new labels.\n",
            "End of iteration 7, added 18 new labels.\n",
            "End of iteration 8, added 10 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.738\n",
            "Accuracy Score:  0.7384234711912336\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 118 new labels.\n",
            "End of iteration 2, added 32 new labels.\n",
            "End of iteration 3, added 34 new labels.\n",
            "End of iteration 4, added 23 new labels.\n",
            "End of iteration 5, added 5 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 10 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.748\n",
            "Accuracy Score:  0.7479674796747967\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.784\n",
            "Accuracy Score:  0.7836691410392365\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1714 new labels.\n",
            "End of iteration 2, added 357 new labels.\n",
            "End of iteration 3, added 217 new labels.\n",
            "End of iteration 4, added 71 new labels.\n",
            "End of iteration 5, added 49 new labels.\n",
            "End of iteration 6, added 43 new labels.\n",
            "End of iteration 7, added 43 new labels.\n",
            "End of iteration 8, added 31 new labels.\n",
            "End of iteration 9, added 35 new labels.\n",
            "End of iteration 10, added 42 new labels.\n",
            "Micro-averaged F1 score on test set: 0.729\n",
            "Accuracy Score:  0.7285259809119831\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1106 new labels.\n",
            "End of iteration 2, added 268 new labels.\n",
            "End of iteration 3, added 101 new labels.\n",
            "End of iteration 4, added 83 new labels.\n",
            "End of iteration 5, added 62 new labels.\n",
            "End of iteration 6, added 47 new labels.\n",
            "End of iteration 7, added 32 new labels.\n",
            "End of iteration 8, added 27 new labels.\n",
            "End of iteration 9, added 22 new labels.\n",
            "End of iteration 10, added 21 new labels.\n",
            "Micro-averaged F1 score on test set: 0.734\n",
            "Accuracy Score:  0.7341816896429834\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 693 new labels.\n",
            "End of iteration 2, added 172 new labels.\n",
            "End of iteration 3, added 58 new labels.\n",
            "End of iteration 4, added 61 new labels.\n",
            "End of iteration 5, added 45 new labels.\n",
            "End of iteration 6, added 31 new labels.\n",
            "End of iteration 7, added 20 new labels.\n",
            "End of iteration 8, added 14 new labels.\n",
            "End of iteration 9, added 18 new labels.\n",
            "End of iteration 10, added 19 new labels.\n",
            "Micro-averaged F1 score on test set: 0.744\n",
            "Accuracy Score:  0.744079179922234\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 347 new labels.\n",
            "End of iteration 2, added 137 new labels.\n",
            "End of iteration 3, added 52 new labels.\n",
            "End of iteration 4, added 27 new labels.\n",
            "End of iteration 5, added 28 new labels.\n",
            "End of iteration 6, added 16 new labels.\n",
            "End of iteration 7, added 21 new labels.\n",
            "End of iteration 8, added 17 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 12 new labels.\n",
            "Micro-averaged F1 score on test set: 0.766\n",
            "Accuracy Score:  0.7656415694591728\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 159 new labels.\n",
            "End of iteration 2, added 39 new labels.\n",
            "End of iteration 3, added 21 new labels.\n",
            "End of iteration 4, added 17 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.780\n",
            "Accuracy Score:  0.7801343230823613\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.791\n",
            "Accuracy Score:  0.7910922587486744\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1800 new labels.\n",
            "End of iteration 2, added 344 new labels.\n",
            "End of iteration 3, added 157 new labels.\n",
            "End of iteration 4, added 89 new labels.\n",
            "End of iteration 5, added 66 new labels.\n",
            "End of iteration 6, added 40 new labels.\n",
            "End of iteration 7, added 23 new labels.\n",
            "End of iteration 8, added 30 new labels.\n",
            "End of iteration 9, added 23 new labels.\n",
            "End of iteration 10, added 21 new labels.\n",
            "Micro-averaged F1 score on test set: 0.765\n",
            "Accuracy Score:  0.7652880876634853\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1208 new labels.\n",
            "End of iteration 2, added 309 new labels.\n",
            "End of iteration 3, added 110 new labels.\n",
            "End of iteration 4, added 82 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 46 new labels.\n",
            "End of iteration 7, added 28 new labels.\n",
            "End of iteration 8, added 19 new labels.\n",
            "End of iteration 9, added 17 new labels.\n",
            "End of iteration 10, added 16 new labels.\n",
            "Micro-averaged F1 score on test set: 0.773\n",
            "Accuracy Score:  0.7727112053729233\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 725 new labels.\n",
            "End of iteration 2, added 230 new labels.\n",
            "End of iteration 3, added 72 new labels.\n",
            "End of iteration 4, added 52 new labels.\n",
            "End of iteration 5, added 36 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 36 new labels.\n",
            "End of iteration 8, added 19 new labels.\n",
            "End of iteration 9, added 16 new labels.\n",
            "End of iteration 10, added 16 new labels.\n",
            "Micro-averaged F1 score on test set: 0.771\n",
            "Accuracy Score:  0.7709437963944857\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 359 new labels.\n",
            "End of iteration 2, added 104 new labels.\n",
            "End of iteration 3, added 74 new labels.\n",
            "End of iteration 4, added 63 new labels.\n",
            "End of iteration 5, added 34 new labels.\n",
            "End of iteration 6, added 20 new labels.\n",
            "End of iteration 7, added 23 new labels.\n",
            "End of iteration 8, added 19 new labels.\n",
            "End of iteration 9, added 13 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.790\n",
            "Accuracy Score:  0.7900318133616119\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 161 new labels.\n",
            "End of iteration 2, added 43 new labels.\n",
            "End of iteration 3, added 29 new labels.\n",
            "End of iteration 4, added 17 new labels.\n",
            "End of iteration 5, added 14 new labels.\n",
            "End of iteration 6, added 18 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 13 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 7 new labels.\n",
            "Micro-averaged F1 score on test set: 0.793\n",
            "Accuracy Score:  0.7932131495227995\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Random Forest for NG\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "rf_params = dict(n_estimators=100, random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_rf_ng = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "\n",
        "      print(\"Supervised NBClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', RandomForestClassifier(**rf_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_rf_ng = df_rf_ng.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(RandomForestClassifier(**rf_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test, thresh = t, kbest = None)\n",
        "        df_rf_ng = df_rf_ng.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "xBxHOzCxtiV1",
        "outputId": "e10ba35d-03f9-43dd-c0be-b7cf5db89c0f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">869.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.653588</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.197596</td>\n",
              "      <td>0.123719</td>\n",
              "      <td>0.253800</td>\n",
              "      <td>0.574054</td>\n",
              "      <td>0.645811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1695.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.725345</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.275362</td>\n",
              "      <td>0.341110</td>\n",
              "      <td>0.654295</td>\n",
              "      <td>0.705196</td>\n",
              "      <td>0.726405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2541.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.758925</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.578296</td>\n",
              "      <td>0.674797</td>\n",
              "      <td>0.722517</td>\n",
              "      <td>0.738423</td>\n",
              "      <td>0.747967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">3349.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.783669</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.728526</td>\n",
              "      <td>0.734182</td>\n",
              "      <td>0.744079</td>\n",
              "      <td>0.765642</td>\n",
              "      <td>0.780134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">4178.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.791092</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.765288</td>\n",
              "      <td>0.772711</td>\n",
              "      <td>0.770944</td>\n",
              "      <td>0.790032</td>\n",
              "      <td>0.793213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Threshold               NaN       0.4       0.5       0.6       0.7       0.8\n",
              "Labeled UnLabeled                                                            \n",
              "869.0   0.0        0.653588       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.197596  0.123719  0.253800  0.574054  0.645811\n",
              "1695.0  0.0        0.725345       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.275362  0.341110  0.654295  0.705196  0.726405\n",
              "2541.0  0.0        0.758925       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.578296  0.674797  0.722517  0.738423  0.747967\n",
              "3349.0  0.0        0.783669       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.728526  0.734182  0.744079  0.765642  0.780134\n",
              "4178.0  0.0        0.791092       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.765288  0.772711  0.770944  0.790032  0.793213"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_rf_ng.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpeyvVvZYHkM"
      },
      "outputs": [],
      "source": [
        "## SGD (Logistic Regression) experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG-pp2wyYHZ6",
        "outputId": "e457f7bd-059d-4037-c69e-1badab864927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.709\n",
            "Accuracy Score:  0.7090844821491693\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2569 new labels.\n",
            "End of iteration 2, added 761 new labels.\n",
            "End of iteration 3, added 211 new labels.\n",
            "End of iteration 4, added 69 new labels.\n",
            "End of iteration 5, added 35 new labels.\n",
            "End of iteration 6, added 17 new labels.\n",
            "End of iteration 7, added 14 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.796\n",
            "Accuracy Score:  0.7956875220926122\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2193 new labels.\n",
            "End of iteration 2, added 700 new labels.\n",
            "End of iteration 3, added 240 new labels.\n",
            "End of iteration 4, added 100 new labels.\n",
            "End of iteration 5, added 38 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 18 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.794\n",
            "Accuracy Score:  0.7939201131141747\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1881 new labels.\n",
            "End of iteration 2, added 637 new labels.\n",
            "End of iteration 3, added 237 new labels.\n",
            "End of iteration 4, added 80 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 19 new labels.\n",
            "End of iteration 8, added 14 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 13 new labels.\n",
            "Micro-averaged F1 score on test set: 0.796\n",
            "Accuracy Score:  0.7956875220926122\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1548 new labels.\n",
            "End of iteration 2, added 555 new labels.\n",
            "End of iteration 3, added 211 new labels.\n",
            "End of iteration 4, added 66 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 25 new labels.\n",
            "End of iteration 7, added 17 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.783\n",
            "Accuracy Score:  0.782608695652174\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1174 new labels.\n",
            "End of iteration 2, added 377 new labels.\n",
            "End of iteration 3, added 166 new labels.\n",
            "End of iteration 4, added 109 new labels.\n",
            "End of iteration 5, added 58 new labels.\n",
            "End of iteration 6, added 30 new labels.\n",
            "End of iteration 7, added 13 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.764\n",
            "Accuracy Score:  0.7638741604807352\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.807\n",
            "Accuracy Score:  0.806998939554613\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3109 new labels.\n",
            "End of iteration 2, added 456 new labels.\n",
            "End of iteration 3, added 99 new labels.\n",
            "End of iteration 4, added 45 new labels.\n",
            "End of iteration 5, added 19 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.848\n",
            "Accuracy Score:  0.8480028278543655\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2772 new labels.\n",
            "End of iteration 2, added 483 new labels.\n",
            "End of iteration 3, added 110 new labels.\n",
            "End of iteration 4, added 41 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 11 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.854\n",
            "Accuracy Score:  0.8536585365853658\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2411 new labels.\n",
            "End of iteration 2, added 453 new labels.\n",
            "End of iteration 3, added 149 new labels.\n",
            "End of iteration 4, added 43 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 13 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.849\n",
            "Accuracy Score:  0.8494167550371156\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2032 new labels.\n",
            "End of iteration 2, added 383 new labels.\n",
            "End of iteration 3, added 126 new labels.\n",
            "End of iteration 4, added 44 new labels.\n",
            "End of iteration 5, added 25 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.847\n",
            "Accuracy Score:  0.8472958642629904\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1545 new labels.\n",
            "End of iteration 2, added 309 new labels.\n",
            "End of iteration 3, added 86 new labels.\n",
            "End of iteration 4, added 42 new labels.\n",
            "End of iteration 5, added 16 new labels.\n",
            "End of iteration 6, added 16 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 9 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.835\n",
            "Accuracy Score:  0.8352774832096147\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.836\n",
            "Accuracy Score:  0.8363379285966772\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3327 new labels.\n",
            "End of iteration 2, added 327 new labels.\n",
            "End of iteration 3, added 64 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.8688582537999293\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3024 new labels.\n",
            "End of iteration 2, added 320 new labels.\n",
            "End of iteration 3, added 75 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 17 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.8695652173913043\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2699 new labels.\n",
            "End of iteration 2, added 324 new labels.\n",
            "End of iteration 3, added 88 new labels.\n",
            "End of iteration 4, added 26 new labels.\n",
            "End of iteration 5, added 13 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.868\n",
            "Accuracy Score:  0.8677978084128667\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2300 new labels.\n",
            "End of iteration 2, added 298 new labels.\n",
            "End of iteration 3, added 74 new labels.\n",
            "End of iteration 4, added 23 new labels.\n",
            "End of iteration 5, added 9 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 8 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.860\n",
            "Accuracy Score:  0.8600212089077413\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1750 new labels.\n",
            "End of iteration 2, added 255 new labels.\n",
            "End of iteration 3, added 70 new labels.\n",
            "End of iteration 4, added 19 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.856\n",
            "Accuracy Score:  0.856486390950866\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.858\n",
            "Accuracy Score:  0.8582537999293036\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3458 new labels.\n",
            "End of iteration 2, added 254 new labels.\n",
            "End of iteration 3, added 42 new labels.\n",
            "End of iteration 4, added 20 new labels.\n",
            "End of iteration 5, added 6 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.885\n",
            "Accuracy Score:  0.8851184164015553\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3175 new labels.\n",
            "End of iteration 2, added 259 new labels.\n",
            "End of iteration 3, added 49 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.883\n",
            "Accuracy Score:  0.8829975256274302\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2840 new labels.\n",
            "End of iteration 2, added 269 new labels.\n",
            "End of iteration 3, added 49 new labels.\n",
            "End of iteration 4, added 15 new labels.\n",
            "End of iteration 5, added 13 new labels.\n",
            "End of iteration 6, added 8 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 8 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.879\n",
            "Accuracy Score:  0.879462707670555\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2457 new labels.\n",
            "End of iteration 2, added 261 new labels.\n",
            "End of iteration 3, added 48 new labels.\n",
            "End of iteration 4, added 21 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.873\n",
            "Accuracy Score:  0.8727465535524921\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1879 new labels.\n",
            "End of iteration 2, added 200 new labels.\n",
            "End of iteration 3, added 41 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.8685047720042418\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.8699186991869918\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3547 new labels.\n",
            "End of iteration 2, added 208 new labels.\n",
            "End of iteration 3, added 45 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 4 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.895\n",
            "Accuracy Score:  0.8953693884764935\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3287 new labels.\n",
            "End of iteration 2, added 204 new labels.\n",
            "End of iteration 3, added 40 new labels.\n",
            "End of iteration 4, added 11 new labels.\n",
            "End of iteration 5, added 5 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.896\n",
            "Accuracy Score:  0.896429833863556\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2931 new labels.\n",
            "End of iteration 2, added 219 new labels.\n",
            "End of iteration 3, added 39 new labels.\n",
            "End of iteration 4, added 14 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 4 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 8 new labels.\n",
            "Micro-averaged F1 score on test set: 0.889\n",
            "Accuracy Score:  0.8893601979498056\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2543 new labels.\n",
            "End of iteration 2, added 207 new labels.\n",
            "End of iteration 3, added 44 new labels.\n",
            "End of iteration 4, added 14 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.888\n",
            "Accuracy Score:  0.887592788971368\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1926 new labels.\n",
            "End of iteration 2, added 169 new labels.\n",
            "End of iteration 3, added 42 new labels.\n",
            "End of iteration 4, added 18 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 11 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.883\n",
            "Accuracy Score:  0.8829975256274302\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Parameters\n",
        "sgd_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_sgd_ng = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "\n",
        "      print(\"Supervised SGDClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', SGDClassifier(**sgd_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_sgd_ng = df_sgd_ng.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sgd_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test, thresh = t, kbest = None)\n",
        "        df_sgd_ng = df_sgd_ng.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "RSemkCRFYHFT",
        "outputId": "60e6904a-e9af-4d13-cc8e-37c29625abb2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-89b6d415-0290-45ab-a462-fd4387206a6c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>K-Best</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>869.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.709084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.793920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.782609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.763874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.806999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.848003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.853659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.849417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.847296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.835277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.836338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.868858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.869565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.867798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.860021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.856486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.858254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.885118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.882998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.879463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.872747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.868505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.869919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.895369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.896430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.889360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.887593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.882998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89b6d415-0290-45ab-a462-fd4387206a6c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89b6d415-0290-45ab-a462-fd4387206a6c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89b6d415-0290-45ab-a462-fd4387206a6c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Labeled  UnLabeled  Threshold  K-Best  Accuracy\n",
              "0     869.0        0.0        NaN     NaN  0.709084\n",
              "1     869.0     4307.0        0.4     NaN  0.795688\n",
              "2     869.0     4307.0        0.5     NaN  0.793920\n",
              "3     869.0     4307.0        0.6     NaN  0.795688\n",
              "4     869.0     4307.0        0.7     NaN  0.782609\n",
              "5     869.0     4307.0        0.8     NaN  0.763874\n",
              "6    1695.0        0.0        NaN     NaN  0.806999\n",
              "7    1695.0     4307.0        0.4     NaN  0.848003\n",
              "8    1695.0     4307.0        0.5     NaN  0.853659\n",
              "9    1695.0     4307.0        0.6     NaN  0.849417\n",
              "10   1695.0     4307.0        0.7     NaN  0.847296\n",
              "11   1695.0     4307.0        0.8     NaN  0.835277\n",
              "12   2541.0        0.0        NaN     NaN  0.836338\n",
              "13   2541.0     4307.0        0.4     NaN  0.868858\n",
              "14   2541.0     4307.0        0.5     NaN  0.869565\n",
              "15   2541.0     4307.0        0.6     NaN  0.867798\n",
              "16   2541.0     4307.0        0.7     NaN  0.860021\n",
              "17   2541.0     4307.0        0.8     NaN  0.856486\n",
              "18   3349.0        0.0        NaN     NaN  0.858254\n",
              "19   3349.0     4307.0        0.4     NaN  0.885118\n",
              "20   3349.0     4307.0        0.5     NaN  0.882998\n",
              "21   3349.0     4307.0        0.6     NaN  0.879463\n",
              "22   3349.0     4307.0        0.7     NaN  0.872747\n",
              "23   3349.0     4307.0        0.8     NaN  0.868505\n",
              "24   4178.0        0.0        NaN     NaN  0.869919\n",
              "25   4178.0     4307.0        0.4     NaN  0.895369\n",
              "26   4178.0     4307.0        0.5     NaN  0.896430\n",
              "27   4178.0     4307.0        0.6     NaN  0.889360\n",
              "28   4178.0     4307.0        0.7     NaN  0.887593\n",
              "29   4178.0     4307.0        0.8     NaN  0.882998"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sgd_ng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "X1M0UDWHlCry",
        "outputId": "9e12fca5-79d8-4bde-a0cf-6b905b4d02d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1b6eea15-f0e4-4305-a248-c94f5a8c8f49\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">869.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.709084</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795688</td>\n",
              "      <td>0.793920</td>\n",
              "      <td>0.795688</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.763874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1695.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.806999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.848003</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>0.849417</td>\n",
              "      <td>0.847296</td>\n",
              "      <td>0.835277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2541.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.836338</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.868858</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.867798</td>\n",
              "      <td>0.860021</td>\n",
              "      <td>0.856486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">3349.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.858254</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.885118</td>\n",
              "      <td>0.882998</td>\n",
              "      <td>0.879463</td>\n",
              "      <td>0.872747</td>\n",
              "      <td>0.868505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">4178.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.869919</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.895369</td>\n",
              "      <td>0.896430</td>\n",
              "      <td>0.889360</td>\n",
              "      <td>0.887593</td>\n",
              "      <td>0.882998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b6eea15-f0e4-4305-a248-c94f5a8c8f49')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b6eea15-f0e4-4305-a248-c94f5a8c8f49 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b6eea15-f0e4-4305-a248-c94f5a8c8f49');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Threshold               NaN       0.4       0.5       0.6       0.7       0.8\n",
              "Labeled UnLabeled                                                            \n",
              "869.0   0.0        0.709084       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.795688  0.793920  0.795688  0.782609  0.763874\n",
              "1695.0  0.0        0.806999       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.848003  0.853659  0.849417  0.847296  0.835277\n",
              "2541.0  0.0        0.836338       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.868858  0.869565  0.867798  0.860021  0.856486\n",
              "3349.0  0.0        0.858254       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.885118  0.882998  0.879463  0.872747  0.868505\n",
              "4178.0  0.0        0.869919       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.895369  0.896430  0.889360  0.887593  0.882998"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sgd_ng.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mNPVy6pYGpJ"
      },
      "source": [
        "Clear improvement in accuracies (pivot above) and micro-F1 (console output) can be seen upon using self training vs. training only on labeled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfoZCBxbzQ3t"
      },
      "outputs": [],
      "source": [
        "## MLP Classifier "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIautQx9zQ7l",
        "outputId": "f3f0c9da-df3c-4ad7-eb56-90775684c0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.516\n",
            "Accuracy Score:  0.5164369034994698\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1676 new labels.\n",
            "End of iteration 2, added 1256 new labels.\n",
            "End of iteration 3, added 556 new labels.\n",
            "End of iteration 4, added 365 new labels.\n",
            "End of iteration 5, added 127 new labels.\n",
            "End of iteration 6, added 75 new labels.\n",
            "End of iteration 7, added 30 new labels.\n",
            "End of iteration 8, added 21 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 33 new labels.\n",
            "Micro-averaged F1 score on test set: 0.561\n",
            "Accuracy Score:  0.56062212796041\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1247 new labels.\n",
            "End of iteration 2, added 753 new labels.\n",
            "End of iteration 3, added 418 new labels.\n",
            "End of iteration 4, added 186 new labels.\n",
            "End of iteration 5, added 215 new labels.\n",
            "End of iteration 6, added 149 new labels.\n",
            "End of iteration 7, added 183 new labels.\n",
            "End of iteration 8, added 291 new labels.\n",
            "End of iteration 9, added 261 new labels.\n",
            "End of iteration 10, added 227 new labels.\n",
            "Micro-averaged F1 score on test set: 0.491\n",
            "Accuracy Score:  0.4913396960056557\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 976 new labels.\n",
            "End of iteration 2, added 545 new labels.\n",
            "End of iteration 3, added 387 new labels.\n",
            "End of iteration 4, added 173 new labels.\n",
            "End of iteration 5, added 162 new labels.\n",
            "End of iteration 6, added 95 new labels.\n",
            "End of iteration 7, added 89 new labels.\n",
            "End of iteration 8, added 101 new labels.\n",
            "End of iteration 9, added 151 new labels.\n",
            "End of iteration 10, added 211 new labels.\n",
            "Micro-averaged F1 score on test set: 0.503\n",
            "Accuracy Score:  0.503004595263344\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 801 new labels.\n",
            "End of iteration 2, added 394 new labels.\n",
            "End of iteration 3, added 248 new labels.\n",
            "End of iteration 4, added 180 new labels.\n",
            "End of iteration 5, added 112 new labels.\n",
            "End of iteration 6, added 86 new labels.\n",
            "End of iteration 7, added 45 new labels.\n",
            "End of iteration 8, added 34 new labels.\n",
            "End of iteration 9, added 21 new labels.\n",
            "End of iteration 10, added 23 new labels.\n",
            "Micro-averaged F1 score on test set: 0.527\n",
            "Accuracy Score:  0.5270413573700954\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 640 new labels.\n",
            "End of iteration 2, added 314 new labels.\n",
            "End of iteration 3, added 153 new labels.\n",
            "End of iteration 4, added 98 new labels.\n",
            "End of iteration 5, added 61 new labels.\n",
            "End of iteration 6, added 56 new labels.\n",
            "End of iteration 7, added 27 new labels.\n",
            "End of iteration 8, added 42 new labels.\n",
            "End of iteration 9, added 41 new labels.\n",
            "End of iteration 10, added 28 new labels.\n",
            "Micro-averaged F1 score on test set: 0.508\n",
            "Accuracy Score:  0.5075998586072817\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.645\n",
            "Accuracy Score:  0.6454577589254153\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2398 new labels.\n",
            "End of iteration 2, added 934 new labels.\n",
            "End of iteration 3, added 171 new labels.\n",
            "End of iteration 4, added 64 new labels.\n",
            "End of iteration 5, added 54 new labels.\n",
            "End of iteration 6, added 21 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 10 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.682\n",
            "Accuracy Score:  0.6822198656769176\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1985 new labels.\n",
            "End of iteration 2, added 629 new labels.\n",
            "End of iteration 3, added 436 new labels.\n",
            "End of iteration 4, added 159 new labels.\n",
            "End of iteration 5, added 63 new labels.\n",
            "End of iteration 6, added 13 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.685\n",
            "Accuracy Score:  0.6850477200424178\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1693 new labels.\n",
            "End of iteration 2, added 542 new labels.\n",
            "End of iteration 3, added 221 new labels.\n",
            "End of iteration 4, added 101 new labels.\n",
            "End of iteration 5, added 38 new labels.\n",
            "End of iteration 6, added 16 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 28 new labels.\n",
            "End of iteration 9, added 23 new labels.\n",
            "End of iteration 10, added 13 new labels.\n",
            "Micro-averaged F1 score on test set: 0.684\n",
            "Accuracy Score:  0.6843407564510428\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1444 new labels.\n",
            "End of iteration 2, added 539 new labels.\n",
            "End of iteration 3, added 167 new labels.\n",
            "End of iteration 4, added 94 new labels.\n",
            "End of iteration 5, added 66 new labels.\n",
            "End of iteration 6, added 32 new labels.\n",
            "End of iteration 7, added 32 new labels.\n",
            "End of iteration 8, added 23 new labels.\n",
            "End of iteration 9, added 16 new labels.\n",
            "End of iteration 10, added 44 new labels.\n",
            "Micro-averaged F1 score on test set: 0.665\n",
            "Accuracy Score:  0.6645457758925415\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1233 new labels.\n",
            "End of iteration 2, added 429 new labels.\n",
            "End of iteration 3, added 154 new labels.\n",
            "End of iteration 4, added 69 new labels.\n",
            "End of iteration 5, added 28 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 14 new labels.\n",
            "End of iteration 8, added 16 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 30 new labels.\n",
            "Micro-averaged F1 score on test set: 0.659\n",
            "Accuracy Score:  0.6592435489572287\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.715\n",
            "Accuracy Score:  0.7154471544715447\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2861 new labels.\n",
            "End of iteration 2, added 596 new labels.\n",
            "End of iteration 3, added 235 new labels.\n",
            "End of iteration 4, added 49 new labels.\n",
            "End of iteration 5, added 10 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "End of iteration 7, added 8 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.718\n",
            "Accuracy Score:  0.7175680452456699\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2495 new labels.\n",
            "End of iteration 2, added 589 new labels.\n",
            "End of iteration 3, added 317 new labels.\n",
            "End of iteration 4, added 98 new labels.\n",
            "End of iteration 5, added 32 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.737\n",
            "Accuracy Score:  0.7373630258041711\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2195 new labels.\n",
            "End of iteration 2, added 575 new labels.\n",
            "End of iteration 3, added 205 new labels.\n",
            "End of iteration 4, added 55 new labels.\n",
            "End of iteration 5, added 125 new labels.\n",
            "End of iteration 6, added 105 new labels.\n",
            "End of iteration 7, added 34 new labels.\n",
            "End of iteration 8, added 12 new labels.\n",
            "End of iteration 9, added 30 new labels.\n",
            "End of iteration 10, added 12 new labels.\n",
            "Micro-averaged F1 score on test set: 0.756\n",
            "Accuracy Score:  0.7557440791799223\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1938 new labels.\n",
            "End of iteration 2, added 494 new labels.\n",
            "End of iteration 3, added 137 new labels.\n",
            "End of iteration 4, added 53 new labels.\n",
            "End of iteration 5, added 61 new labels.\n",
            "End of iteration 6, added 55 new labels.\n",
            "End of iteration 7, added 34 new labels.\n",
            "End of iteration 8, added 18 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.723\n",
            "Accuracy Score:  0.7232237539766702\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1710 new labels.\n",
            "End of iteration 2, added 418 new labels.\n",
            "End of iteration 3, added 300 new labels.\n",
            "End of iteration 4, added 51 new labels.\n",
            "End of iteration 5, added 10 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.708\n",
            "Accuracy Score:  0.7076705549664192\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.748\n",
            "Accuracy Score:  0.7476139978791092\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3026 new labels.\n",
            "End of iteration 2, added 757 new labels.\n",
            "End of iteration 3, added 143 new labels.\n",
            "End of iteration 4, added 35 new labels.\n",
            "End of iteration 5, added 15 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.768\n",
            "Accuracy Score:  0.767762460233298\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2716 new labels.\n",
            "End of iteration 2, added 589 new labels.\n",
            "End of iteration 3, added 186 new labels.\n",
            "End of iteration 4, added 19 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.767\n",
            "Accuracy Score:  0.767055496641923\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2449 new labels.\n",
            "End of iteration 2, added 520 new labels.\n",
            "End of iteration 3, added 96 new labels.\n",
            "End of iteration 4, added 53 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 53 new labels.\n",
            "End of iteration 7, added 13 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.767\n",
            "Accuracy Score:  0.7674089784376105\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2216 new labels.\n",
            "End of iteration 2, added 488 new labels.\n",
            "End of iteration 3, added 72 new labels.\n",
            "End of iteration 4, added 27 new labels.\n",
            "End of iteration 5, added 35 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.763\n",
            "Accuracy Score:  0.7628137150936727\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1989 new labels.\n",
            "End of iteration 2, added 446 new labels.\n",
            "End of iteration 3, added 121 new labels.\n",
            "End of iteration 4, added 58 new labels.\n",
            "End of iteration 5, added 17 new labels.\n",
            "End of iteration 6, added 63 new labels.\n",
            "End of iteration 7, added 25 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.770\n",
            "Accuracy Score:  0.7698833510074231\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.762\n",
            "Accuracy Score:  0.7617532697066101\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3298 new labels.\n",
            "End of iteration 2, added 346 new labels.\n",
            "End of iteration 3, added 69 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.789\n",
            "Accuracy Score:  0.7889713679745494\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2976 new labels.\n",
            "End of iteration 2, added 658 new labels.\n",
            "End of iteration 3, added 76 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.802\n",
            "Accuracy Score:  0.8020501944149876\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2713 new labels.\n",
            "End of iteration 2, added 363 new labels.\n",
            "End of iteration 3, added 83 new labels.\n",
            "End of iteration 4, added 20 new labels.\n",
            "End of iteration 5, added 34 new labels.\n",
            "End of iteration 6, added 10 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 168 new labels.\n",
            "End of iteration 10, added 18 new labels.\n",
            "Micro-averaged F1 score on test set: 0.793\n",
            "Accuracy Score:  0.7925061859314245\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2483 new labels.\n",
            "End of iteration 2, added 331 new labels.\n",
            "End of iteration 3, added 134 new labels.\n",
            "End of iteration 4, added 21 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.785\n",
            "Accuracy Score:  0.7847295864262991\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2250 new labels.\n",
            "End of iteration 2, added 445 new labels.\n",
            "End of iteration 3, added 47 new labels.\n",
            "End of iteration 4, added 9 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.786\n",
            "Accuracy Score:  0.7857900318133616\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# MLP for NG\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Parameters\n",
        "mlp_params = dict(hidden_layer_sizes=(100,), max_iter=100,activation = 'relu',solver='lbfgs',random_state=1,learning_rate_init=0.01,\n",
        "                  learning_rate='adaptive') # regularization is by default based on alpha =0.0001\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_ng = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised MLPClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline      \n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('scale', StandardScaler(with_mean=False)),\n",
        "          ('clf', MLPClassifier(**mlp_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('scale', StandardScaler(with_mean=False)),\n",
        "            ('clf', SelfTrainingClassifier(MLPClassifier(**mlp_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_VM-7NEqzQ_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "c029e466-9c80-477e-fdb9-0183ef6a07c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Threshold               NaN       0.4       0.5       0.6       0.7       0.8\n",
              "Labeled UnLabeled                                                            \n",
              "869.0   0.0        0.516437       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.560622  0.491340  0.503005  0.527041  0.507600\n",
              "1695.0  0.0        0.645458       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.682220  0.685048  0.684341  0.664546  0.659244\n",
              "2541.0  0.0        0.715447       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.717568  0.737363  0.755744  0.723224  0.707671\n",
              "3349.0  0.0        0.747614       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.767762  0.767055  0.767409  0.762814  0.769883\n",
              "4178.0  0.0        0.761753       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.788971  0.802050  0.792506  0.784730  0.785790"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8006637a-f419-481e-b6ab-56b5d5d1d745\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">869.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.516437</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.560622</td>\n",
              "      <td>0.491340</td>\n",
              "      <td>0.503005</td>\n",
              "      <td>0.527041</td>\n",
              "      <td>0.507600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1695.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.645458</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.682220</td>\n",
              "      <td>0.685048</td>\n",
              "      <td>0.684341</td>\n",
              "      <td>0.664546</td>\n",
              "      <td>0.659244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2541.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.715447</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.717568</td>\n",
              "      <td>0.737363</td>\n",
              "      <td>0.755744</td>\n",
              "      <td>0.723224</td>\n",
              "      <td>0.707671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">3349.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.747614</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.767762</td>\n",
              "      <td>0.767055</td>\n",
              "      <td>0.767409</td>\n",
              "      <td>0.762814</td>\n",
              "      <td>0.769883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">4178.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.761753</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.788971</td>\n",
              "      <td>0.802050</td>\n",
              "      <td>0.792506</td>\n",
              "      <td>0.784730</td>\n",
              "      <td>0.785790</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8006637a-f419-481e-b6ab-56b5d5d1d745')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8006637a-f419-481e-b6ab-56b5d5d1d745 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8006637a-f419-481e-b6ab-56b5d5d1d745');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df_mlp_ng.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u51-CcA0kNi"
      },
      "source": [
        "Even for an MLP, the same situation holds true. Micro-F1 and Accuracy show improvements using selftrainingclassifier and usually the higher tresholds result in better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Sw9pCKezRPn"
      },
      "outputs": [],
      "source": [
        "## End of relevant notebook reference - Logistic Regression and Multi-layer perceptron based self training classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMUWFZuielQN",
        "outputId": "21f42d21-7563-4c96-e1da-aa07c49cf50d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def train_val_split(labels, n_labeled_per_class, unlabeled_per_class, n_labels, seed=0):\n",
        "    \"\"\"Split the original training set into labeled training set, unlabeled training set, development set\n",
        "\n",
        "    Arguments:\n",
        "        labels {list} -- List of labeles for original training set\n",
        "        n_labeled_per_class {int} -- Number of labeled data per class\n",
        "        unlabeled_per_class {int} -- Number of unlabeled data per class\n",
        "        n_labels {int} -- The number of classes\n",
        "\n",
        "    Keyword Arguments:\n",
        "        seed {int} -- [random seed of np.shuffle] (default: {0})\n",
        "\n",
        "    Returns:\n",
        "        [list] -- idx for labeled training set, unlabeled training set, development set\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_idxs = []\n",
        "    train_unlabeled_idxs = []\n",
        "    val_idxs = []\n",
        "\n",
        "    for i in range(n_labels):\n",
        "        idxs = np.where(labels == i)[0]\n",
        "        np.random.shuffle(idxs)\n",
        "        if n_labels == 2:\n",
        "            # IMDB\n",
        "            \n",
        "            \n",
        "            \n",
        "            n_unlabeled_per_class = unlabeled_per_class   #10, 100, 500, 1000, 2500\n",
        "            train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            train_unlabeled_idxs.extend(idxs[500: 500 + n_unlabeled_per_class])\n",
        "            val_idxs.extend(idxs[-2000:])\n",
        "        \n",
        "        \n",
        "        \n",
        "            # train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            # train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            # train_unlabeled_idxs.extend(\n",
        "            #     idxs[500: 500 + 5000])\n",
        "            # val_idxs.extend(idxs[-2000:])\n",
        "            \n",
        "\n",
        "    return train_labeled_idxs, train_unlabeled_idxs, val_idxs\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "#Removes Punctuations\n",
        "def remove_punctuations(data):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    data=punct_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes HTML syntaxes\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes URL data\n",
        "def remove_url(data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def clean(train_df):\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_html(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_url(z))\n",
        "    # train_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: word_tokenize(z))\n",
        "    \n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # train_df['review']=train_df['review'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "    train_df['review']=train_df['review'].apply(lambda x: ' '.join(x))\n",
        "    \n",
        "    return train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ8OCqGFirni"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Parameters\n",
        "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "# Supervised Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(**sdg_params)),\n",
        "])\n",
        "\n",
        "# # Supervised Pipeline\n",
        "# pipeline = Pipeline([\n",
        "#     ('vect', CountVectorizer(**vectorizer_params)),\n",
        "#     ('tfidf', TfidfTransformer()),\n",
        "#     ('clf', SVC(probability=True, gamma=\"auto\")),\n",
        "# ])\n",
        "\n",
        "# SelfTraining Pipeline\n",
        "st_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\n",
        "])\n",
        "\n",
        "# # SelfTraining Pipeline\n",
        "# st_pipeline = Pipeline([\n",
        "#     ('vect', CountVectorizer(**vectorizer_params)),\n",
        "#     ('tfidf', TfidfTransformer()),\n",
        "#     ('clf', SelfTrainingClassifier(SVC(probability=True, gamma=\"auto\") )),\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# LabelSpreading Pipeline\n",
        "ls_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    # LabelSpreading does not support dense matrices\n",
        "    ('todense', FunctionTransformer(lambda x: x.todense())),\n",
        "    ('clf', LabelSpreading()),\n",
        "])\n",
        "\n",
        "\n",
        "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # if -1 in y_train:\n",
        "    #   print(\"Y-PRED-PROBA\", clf.predict_proba(X_train))\n",
        "\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "def eval_and_print_metrics_df(clf, X_train, y_train, X_test, y_test, thresh = None, kbest = None):\n",
        "\n",
        "    dict1 = {}\n",
        "\n",
        "\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "    dict1['Labeled'] = len(X_train) - sum(1 for x in y_train if x == -1)\n",
        "    dict1['UnLabeled'] = sum(1 for x in y_train if x == -1)\n",
        "\n",
        "\n",
        "    \n",
        "    # if sum(1 for x in y_train if x == -1) == 0:\n",
        "    #     dict1['type'] = 'Supervised'\n",
        "    # else:\n",
        "    #     dict1['type'] = 'Semi-Supervised'\n",
        "\n",
        "    dict1['Threshold'] = thresh\n",
        "    dict1['K-Best'] = kbest\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    dict1['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "    return dict1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "KIercxEGaqqO",
        "outputId": "f8552a93-57e1-4a3a-bd5f-bc6c14d62115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-944f71bd1a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/imdb_data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/imdb_data/train.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # n_list = [10, 200, 500]\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised SGDClassifier on \", 2*n,\" of the training data:\")\n",
        "      eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # y_train[~y_mask] = -1\n",
        "      print(\"SelfTrainingClassifier on \", 2*n,\" of the training data (rest \"\n",
        "          \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "        # SelfTraining Pipeline\n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), threshold = t, verbose=True)),\n",
        "        ])\n",
        "\n",
        "        eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "is55ANYy3DLW",
        "outputId": "325674a0-26b9-43fb-ef95-4c8dde2debe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-16c43c8199f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mclean_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m       \u001b[0mclean_test_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m       \u001b[0mclean_unlabeled_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_unlabeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-afc6adf05b70>\u001b[0m in \u001b[0;36mclean\u001b[0;34m(train_df)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# train_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# lemmatizer = WordNetLemmatizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-afc6adf05b70>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# train_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# lemmatizer = WordNetLemmatizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # n_list = [10, 200, 500]\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 2500\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised SGDClassifier on \", 2*n,\" of the training data:\")\n",
        "      eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # y_train[~y_mask] = -1\n",
        "      print(\"SelfTrainingClassifier on \", 2*n,\" of the training data (rest \"\n",
        "          \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "        # SelfTraining Pipeline\n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), threshold = t, verbose=True)),\n",
        "        ])\n",
        "\n",
        "        eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ85gvK_O--y",
        "outputId": "48580a47-8551-46d1-b9ca-58a6415f8fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.590\n",
            "Accuracy Score:  0.59016\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.685\n",
            "Accuracy Score:  0.68456\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.795\n",
            "Accuracy Score:  0.79504\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.838\n",
            "Accuracy Score:  0.83752\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.858\n",
            "Accuracy Score:  0.85776\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.495\n",
            "Accuracy Score:  0.49504\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.4928\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes for IMDB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_nb_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised NBClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', MultinomialNB()),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_nb_imdb_2k = df_nb_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for kb in kbest_list:\n",
        "        kbest = int((unlabeled_per_class*2)/kb)\n",
        "        print(\"---------------------------------K-Best = \", kbest,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(MultinomialNB(), criterion = 'k_best', k_best = kbest, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = None, kbest = kbest)\n",
        "        df_nb_imdb_2k = df_nb_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QghN8bR6lc3h"
      },
      "outputs": [],
      "source": [
        "display(df_nb_imdb_2k[['Labeled', 'UnLabeled', 'Threshold', 'K-Best', 'Accuracy']].sort_values(['Labeled', 'UnLabeled']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "65I9Z92jjfB7",
        "outputId": "902404c0-264a-4e38-98b6-c10676ba322a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>K-Best</th>\n",
              "      <th>NaN</th>\n",
              "      <th>250.0</th>\n",
              "      <th>285.0</th>\n",
              "      <th>333.0</th>\n",
              "      <th>400.0</th>\n",
              "      <th>500.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.59016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.68456</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.79504</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.83752</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.85776</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49280</td>\n",
              "      <td>0.49504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "K-Best               NaN      250.0    285.0    333.0    400.0    500.0\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.59016      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "100.0   0.0        0.68456      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "400.0   0.0        0.79504      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "1000.0  0.0        0.83752      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "2000.0  0.0        0.85776      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49280  0.49504"
            ]
          },
          "execution_count": 33,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_nb_imdb_2k.pivot(index=['Labeled', 'UnLabeled'], columns='K-Best')['Accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOZugkdxjeqG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jiSLJIujemX",
        "outputId": "13c75573-ae98-44d6-a689-987d0403ee95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.590\n",
            "Accuracy Score:  0.59016\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.685\n",
            "Accuracy Score:  0.68456\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.795\n",
            "Accuracy Score:  0.79504\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.838\n",
            "Accuracy Score:  0.83752\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.858\n",
            "Accuracy Score:  0.85776\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes for IMDB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_nb_imdb_5k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 2500\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised NBClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', MultinomialNB()),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_nb_imdb_5k = df_nb_imdb_5k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for kb in kbest_list:\n",
        "        kbest = int((unlabeled_per_class*2)/kb)\n",
        "        print(\"---------------------------------K-Best = \", kbest,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(MultinomialNB(), criterion = 'k_best', k_best = kbest, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = None, kbest = kbest)\n",
        "        df_nb_imdb_5k = df_nb_imdb_5k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "dNyJjKTVjeix",
        "outputId": "99e0cf83-44a4-4adc-b8f1-19e644008aeb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>K-Best</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.59016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.68456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>400.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.79504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.83752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.85776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Labeled  UnLabeled  Threshold  K-Best  Accuracy\n",
              "0      20.0        0.0        NaN     NaN   0.59016\n",
              "1      20.0     5000.0        NaN  1250.0   0.49256\n",
              "2      20.0     5000.0        NaN  1000.0   0.49256\n",
              "3      20.0     5000.0        NaN   833.0   0.49256\n",
              "4      20.0     5000.0        NaN   714.0   0.49256\n",
              "5      20.0     5000.0        NaN   625.0   0.49256\n",
              "6     100.0        0.0        NaN     NaN   0.68456\n",
              "7     100.0     5000.0        NaN  1250.0   0.49256\n",
              "8     100.0     5000.0        NaN  1000.0   0.49256\n",
              "9     100.0     5000.0        NaN   833.0   0.49256\n",
              "10    100.0     5000.0        NaN   714.0   0.49256\n",
              "11    100.0     5000.0        NaN   625.0   0.49256\n",
              "12    400.0        0.0        NaN     NaN   0.79504\n",
              "13    400.0     5000.0        NaN  1250.0   0.49256\n",
              "14    400.0     5000.0        NaN  1000.0   0.49256\n",
              "15    400.0     5000.0        NaN   833.0   0.49256\n",
              "16    400.0     5000.0        NaN   714.0   0.49256\n",
              "17    400.0     5000.0        NaN   625.0   0.49256\n",
              "18   1000.0        0.0        NaN     NaN   0.83752\n",
              "19   1000.0     5000.0        NaN  1250.0   0.49256\n",
              "20   1000.0     5000.0        NaN  1000.0   0.49256\n",
              "21   1000.0     5000.0        NaN   833.0   0.49256\n",
              "22   1000.0     5000.0        NaN   714.0   0.49256\n",
              "23   1000.0     5000.0        NaN   625.0   0.49256\n",
              "24   2000.0        0.0        NaN     NaN   0.85776\n",
              "25   2000.0     5000.0        NaN  1250.0   0.49256\n",
              "26   2000.0     5000.0        NaN  1000.0   0.49256\n",
              "27   2000.0     5000.0        NaN   833.0   0.49256\n",
              "28   2000.0     5000.0        NaN   714.0   0.49256\n",
              "29   2000.0     5000.0        NaN   625.0   0.49256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df_nb_imdb_5k[['Labeled', 'UnLabeled', 'Threshold', 'K-Best', 'Accuracy']].sort_values(['Labeled', 'UnLabeled']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "PoS_KIuWjedf",
        "outputId": "04a13bbf-a3c5-4b4d-efd8-d2dd18eeda39"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>K-Best</th>\n",
              "      <th>NaN</th>\n",
              "      <th>625.0</th>\n",
              "      <th>714.0</th>\n",
              "      <th>833.0</th>\n",
              "      <th>1000.0</th>\n",
              "      <th>1250.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.59016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.68456</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.79504</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.83752</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.85776</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "K-Best              NaN      625.0    714.0    833.0    1000.0   1250.0\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.59016      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "100.0   0.0        0.68456      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "400.0   0.0        0.79504      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "1000.0  0.0        0.83752      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "2000.0  0.0        0.85776      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256"
            ]
          },
          "execution_count": 36,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_nb_imdb_5k.pivot(index=['Labeled', 'UnLabeled'], columns='K-Best')['Accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk60A1w-pxrV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJbPC72BjeZ4",
        "outputId": "7c14f91a-c3e1-4455-c32c-def6e3230733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.621\n",
            "Accuracy Score:  0.6208\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.508\n",
            "Accuracy Score:  0.50824\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1990 new labels.\n",
            "End of iteration 2, added 10 new labels.\n",
            "Micro-averaged F1 score on test set: 0.508\n",
            "Accuracy Score:  0.50816\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 461 new labels.\n",
            "End of iteration 2, added 1539 new labels.\n",
            "Micro-averaged F1 score on test set: 0.507\n",
            "Accuracy Score:  0.50744\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 8 new labels.\n",
            "End of iteration 2, added 1236 new labels.\n",
            "End of iteration 3, added 756 new labels.\n",
            "Micro-averaged F1 score on test set: 0.507\n",
            "Accuracy Score:  0.50744\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "Micro-averaged F1 score on test set: 0.589\n",
            "Accuracy Score:  0.58864\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.638\n",
            "Accuracy Score:  0.63768\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.578\n",
            "Accuracy Score:  0.57832\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1989 new labels.\n",
            "End of iteration 2, added 11 new labels.\n",
            "Micro-averaged F1 score on test set: 0.573\n",
            "Accuracy Score:  0.57312\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 470 new labels.\n",
            "End of iteration 2, added 1124 new labels.\n",
            "End of iteration 3, added 372 new labels.\n",
            "End of iteration 4, added 32 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.513\n",
            "Accuracy Score:  0.51256\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 25 new labels.\n",
            "End of iteration 2, added 388 new labels.\n",
            "End of iteration 3, added 1558 new labels.\n",
            "End of iteration 4, added 29 new labels.\n",
            "Micro-averaged F1 score on test set: 0.507\n",
            "Accuracy Score:  0.50744\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "Micro-averaged F1 score on test set: 0.643\n",
            "Accuracy Score:  0.64296\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.770\n",
            "Accuracy Score:  0.77016\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.769\n",
            "Accuracy Score:  0.76912\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1991 new labels.\n",
            "End of iteration 2, added 9 new labels.\n",
            "Micro-averaged F1 score on test set: 0.765\n",
            "Accuracy Score:  0.7652\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 645 new labels.\n",
            "End of iteration 2, added 569 new labels.\n",
            "End of iteration 3, added 290 new labels.\n",
            "End of iteration 4, added 258 new labels.\n",
            "End of iteration 5, added 157 new labels.\n",
            "End of iteration 6, added 35 new labels.\n",
            "End of iteration 7, added 17 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.585\n",
            "Accuracy Score:  0.58488\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 49 new labels.\n",
            "End of iteration 2, added 134 new labels.\n",
            "End of iteration 3, added 224 new labels.\n",
            "End of iteration 4, added 455 new labels.\n",
            "End of iteration 5, added 684 new labels.\n",
            "End of iteration 6, added 277 new labels.\n",
            "End of iteration 7, added 44 new labels.\n",
            "End of iteration 8, added 13 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.555\n",
            "Accuracy Score:  0.55544\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "Micro-averaged F1 score on test set: 0.780\n",
            "Accuracy Score:  0.78024\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.812\n",
            "Accuracy Score:  0.81184\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.802\n",
            "Accuracy Score:  0.8016\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1994 new labels.\n",
            "End of iteration 2, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.799\n",
            "Accuracy Score:  0.79896\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 965 new labels.\n",
            "End of iteration 2, added 281 new labels.\n",
            "End of iteration 3, added 86 new labels.\n",
            "End of iteration 4, added 35 new labels.\n",
            "End of iteration 5, added 24 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 19 new labels.\n",
            "End of iteration 8, added 17 new labels.\n",
            "End of iteration 9, added 18 new labels.\n",
            "End of iteration 10, added 10 new labels.\n",
            "Micro-averaged F1 score on test set: 0.750\n",
            "Accuracy Score:  0.75048\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 238 new labels.\n",
            "End of iteration 2, added 139 new labels.\n",
            "End of iteration 3, added 84 new labels.\n",
            "End of iteration 4, added 75 new labels.\n",
            "End of iteration 5, added 57 new labels.\n",
            "End of iteration 6, added 71 new labels.\n",
            "End of iteration 7, added 69 new labels.\n",
            "End of iteration 8, added 51 new labels.\n",
            "End of iteration 9, added 58 new labels.\n",
            "End of iteration 10, added 71 new labels.\n",
            "Micro-averaged F1 score on test set: 0.628\n",
            "Accuracy Score:  0.62816\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 15 new labels.\n",
            "End of iteration 2, added 1 new labels.\n",
            "End of iteration 3, added 6 new labels.\n",
            "End of iteration 4, added 2 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.808\n",
            "Accuracy Score:  0.80808\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.838\n",
            "Accuracy Score:  0.83816\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.828\n",
            "Accuracy Score:  0.82768\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1994 new labels.\n",
            "End of iteration 2, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.825\n",
            "Accuracy Score:  0.82504\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1097 new labels.\n",
            "End of iteration 2, added 179 new labels.\n",
            "End of iteration 3, added 62 new labels.\n",
            "End of iteration 4, added 28 new labels.\n",
            "End of iteration 5, added 20 new labels.\n",
            "End of iteration 6, added 14 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 13 new labels.\n",
            "End of iteration 9, added 9 new labels.\n",
            "End of iteration 10, added 9 new labels.\n",
            "Micro-averaged F1 score on test set: 0.798\n",
            "Accuracy Score:  0.7976\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 344 new labels.\n",
            "End of iteration 2, added 109 new labels.\n",
            "End of iteration 3, added 69 new labels.\n",
            "End of iteration 4, added 28 new labels.\n",
            "End of iteration 5, added 23 new labels.\n",
            "End of iteration 6, added 9 new labels.\n",
            "End of iteration 7, added 11 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 11 new labels.\n",
            "End of iteration 10, added 15 new labels.\n",
            "Micro-averaged F1 score on test set: 0.797\n",
            "Accuracy Score:  0.7968\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 32 new labels.\n",
            "End of iteration 2, added 19 new labels.\n",
            "End of iteration 3, added 11 new labels.\n",
            "End of iteration 4, added 6 new labels.\n",
            "End of iteration 5, added 2 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.833\n",
            "Accuracy Score:  0.8328\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Random Forest for IMDB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "rf_params = dict(n_estimators=1000, random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_rf_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised NBClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', RandomForestClassifier(**rf_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_rf_imdb_2k = df_rf_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(RandomForestClassifier(**rf_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_rf_imdb_2k = df_rf_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "03U0nK6QjeWW",
        "outputId": "99c4b262-1dee-42a3-f9a5-4f2d161b69d4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.62080</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.50824</td>\n",
              "      <td>0.50816</td>\n",
              "      <td>0.50744</td>\n",
              "      <td>0.50744</td>\n",
              "      <td>0.58864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.63768</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.57832</td>\n",
              "      <td>0.57312</td>\n",
              "      <td>0.51256</td>\n",
              "      <td>0.50744</td>\n",
              "      <td>0.64296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.77016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.76912</td>\n",
              "      <td>0.76520</td>\n",
              "      <td>0.58488</td>\n",
              "      <td>0.55544</td>\n",
              "      <td>0.78024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.81184</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.80160</td>\n",
              "      <td>0.79896</td>\n",
              "      <td>0.75048</td>\n",
              "      <td>0.62816</td>\n",
              "      <td>0.80808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.83816</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.82768</td>\n",
              "      <td>0.82504</td>\n",
              "      <td>0.79760</td>\n",
              "      <td>0.79680</td>\n",
              "      <td>0.83280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Threshold              NaN      0.4      0.5      0.6      0.7      0.8\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.62080      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.50824  0.50816  0.50744  0.50744  0.58864\n",
              "100.0   0.0        0.63768      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.57832  0.57312  0.51256  0.50744  0.64296\n",
              "400.0   0.0        0.77016      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.76912  0.76520  0.58488  0.55544  0.78024\n",
              "1000.0  0.0        0.81184      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.80160  0.79896  0.75048  0.62816  0.80808\n",
              "2000.0  0.0        0.83816      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.82768  0.82504  0.79760  0.79680  0.83280"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_rf_imdb_2k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkTOtCsntDcW"
      },
      "outputs": [],
      "source": [
        "# SVM for IMDB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "svm_params = dict(alpha=1e-5, penalty='l2', loss='hinge', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_svm_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised NBClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', SGDClassifier(**svm_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_svm_imdb_2k = df_svm_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**svm_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_svm_imdb_2k = df_svm_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMan7Lr8tDYU"
      },
      "outputs": [],
      "source": [
        "df_svm_imdb_2k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeftB64PtDRu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZvRUkMOtDNc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmBA7WuZtDJO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5G1zJKztDDZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNq_ITAZtCrL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRZzA8s-jeRK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzBUezG7jeHD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHunKjVfjd8A"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRj9_mvzO-6W",
        "outputId": "52683dab-b8d5-428d-a7b7-f75ec7740d5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "SelfTrainingClassifier on  400  of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2376 new labels.\n",
            "End of iteration 2, added 1084 new labels.\n",
            "End of iteration 3, added 280 new labels.\n",
            "End of iteration 4, added 95 new labels.\n",
            "End of iteration 5, added 45 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 8 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 9 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.805\n",
            "Accuracy Score:  0.8052\n",
            "----------\n",
            "\n",
            "t =  0.8 \n",
            " [0 0 0 ... 0 1 1]\n",
            "t =  0.8 \n",
            " [[0.89715549 0.10284451]\n",
            " [0.57326692 0.42673308]\n",
            " [0.93212653 0.06787347]\n",
            " ...\n",
            " [0.74134373 0.25865627]\n",
            " [0.00151803 0.99848197]\n",
            " [0.08927711 0.91072289]]\n",
            "Number of equal values between y_pred and y_pred(calculated) when we calculate y_pred from y_proba with threshold = 0.8:  10733\n",
            "Number of equal values between y_pred and y_pred(calculated) when we calculate y_pred from y_proba with threshold = 0.5:  12500\n"
          ]
        }
      ],
      "source": [
        "#Checking if threshold parameter for SelfTrainingClassifier is used for adding examples selectively or as a threshold to predict_proba.\n",
        "#If it is used for predict_proba, then we should compare it with the supervised version with the same threshold - not with supervised(threshold=0.5).\n",
        "\n",
        "def eval_and_print_metrics_2(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_pred_proba = clf.predict_proba(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "    return y_pred, y_pred_proba\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # n_list = [10, 200, 500]\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    n_list = [200]\n",
        "    threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "    threshold = [0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 2500\n",
        "\n",
        "      data_path = './data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      # X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      # print(\"Supervised SGDClassifier on \", 2*n,\" of the training data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # y_train[~y_mask] = -1\n",
        "      print(\"SelfTrainingClassifier on \", 2*n,\" of the training data (rest \"\n",
        "          \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "        # SelfTraining Pipeline\n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), threshold = t, verbose=True)),\n",
        "        ])\n",
        "\n",
        "        y_pred, y_proba = eval_and_print_metrics_2(st_pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "        print(\"t = \",t, '\\n', y_pred)\n",
        "        print(\"t = \",t, '\\n', y_proba)\n",
        "\n",
        "        print(\"Number of equal values between y_pred and y_pred(calculated) when we calculate y_pred from y_proba with threshold = 0.8: \", sum(np.equal((y_proba[:,1] >= 0.8).astype(int), y_pred)))\n",
        "        print(\"Number of equal values between y_pred and y_pred(calculated) when we calculate y_pred from y_proba with threshold = 0.5: \", sum(np.equal((y_proba[:,1] >= 0.5).astype(int), y_pred)))\n",
        "\n",
        "\n",
        "#Conclusion: Threshold is used as a part of training but not to calculate pred from pred_proba.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fgeG6CPO-pI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seQL9VTJT_2Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SFZmBLT_k8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W-koYtZT_QX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2jQptP0O-mN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3m-NGmPO8w6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUTplK_BDYOZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1GBsvghfYax"
      },
      "outputs": [],
      "source": [
        "def train_val_split(labels, n_labeled_per_class, unlabeled_per_class, n_labels, seed=0):\n",
        "    \"\"\"Split the original training set into labeled training set, unlabeled training set, development set\n",
        "\n",
        "    Arguments:\n",
        "        labels {list} -- List of labeles for original training set\n",
        "        n_labeled_per_class {int} -- Number of labeled data per class\n",
        "        unlabeled_per_class {int} -- Number of unlabeled data per class\n",
        "        n_labels {int} -- The number of classes\n",
        "\n",
        "    Keyword Arguments:\n",
        "        seed {int} -- [random seed of np.shuffle] (default: {0})\n",
        "\n",
        "    Returns:\n",
        "        [list] -- idx for labeled training set, unlabeled training set, development set\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_idxs = []\n",
        "    train_unlabeled_idxs = []\n",
        "    val_idxs = []\n",
        "\n",
        "    for i in range(n_labels):\n",
        "        idxs = np.where(labels == i)[0]\n",
        "        np.random.shuffle(idxs)\n",
        "        if n_labels == 2:\n",
        "            # IMDB\n",
        "            \n",
        "            \n",
        "            \n",
        "            n_unlabeled_per_class = 1000   #10, 100, 500, 1000, 2500\n",
        "            train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            train_unlabeled_idxs.extend(idxs[500: 500 + n_unlabeled_per_class])\n",
        "            val_idxs.extend(idxs[-2000:])\n",
        "        \n",
        "        \n",
        "        \n",
        "            # train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            # train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            # train_unlabeled_idxs.extend(\n",
        "            #     idxs[500: 500 + 5000])\n",
        "            # val_idxs.extend(idxs[-2000:])\n",
        "            \n",
        "\n",
        "    return train_labeled_idxs, train_unlabeled_idxs, val_idxs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7w3zYcujTcM",
        "outputId": "c9f4f08e-e210-4ae3-aa95-15864b018c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 9 µs, sys: 0 ns, total: 9 µs\n",
            "Wall time: 13.4 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import re\n",
        "#Removes Punctuations\n",
        "def remove_punctuations(data):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    data=punct_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes HTML syntaxes\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes URL data\n",
        "def remove_url(data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes Emojis\n",
        "def remove_emoji(data):\n",
        "    emoji_clean= re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    data=emoji_clean.sub(r'',data)\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjo6YhKpk5AN"
      },
      "outputs": [],
      "source": [
        "def clean(train_df):\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_html(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_url(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: word_tokenize(z))\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "#     train_df['review']=train_df['review'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "    train_df['review']=train_df['review'].apply(lambda x: ' '.join(x))\n",
        "    \n",
        "    return train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-mWoJi6nvI-"
      },
      "outputs": [],
      "source": [
        "def make_feature_vec(words, model, num_features):\n",
        "    \"\"\"\n",
        "    Average the word vectors for a set of words\n",
        "    \"\"\"\n",
        "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
        "    nwords = 0.\n",
        "    index2word_set = set(model.wv.index2word)  # words known to the model\n",
        "\n",
        "    for word in words:\n",
        "        if word in index2word_set: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vec = np.add(feature_vec,model[word])\n",
        "    \n",
        "    feature_vec = np.divide(feature_vec, nwords)\n",
        "    return feature_vec\n",
        "\n",
        "\n",
        "def get_avg_feature_vecs(reviews, model, num_features):\n",
        "    \"\"\"\n",
        "    Calculate average feature vectors for all reviews\n",
        "    \"\"\"\n",
        "    counter = 0\n",
        "    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n",
        "    \n",
        "    for review in reviews:\n",
        "#         print(counter)\n",
        "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return review_feature_vecs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3ZTQV4tFEne",
        "outputId": "761811d0-5fad-40b5-c56f-1f7c3f95db8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def get_data_tfidf(n, arg1=1):\n",
        "\n",
        "  n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "  unlabeled_per_class = 1000\n",
        "\n",
        "\n",
        "  data_path = './data/imdb_data/'\n",
        "  train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "  test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "  print(\"Read data successful\", train_df.shape, test_df.shape, \"n_labeled = \", n_labeled_per_class*2)\n",
        "  print(train_df.head(5))\n",
        "\n",
        "\n",
        "  train_labels = np.array([v for v in train_df[1]])\n",
        "  train_text = np.array([v for v in train_df[0]])\n",
        "  print(train_labels[:3])\n",
        "  print(train_text[:3])\n",
        "\n",
        "  test_labels = np.array([u for u in test_df[1]])\n",
        "  test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  n_labels = 2\n",
        "  # Split the labeled training set, unlabeled training set, development set\n",
        "  train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "      train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "  print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "  \n",
        "\n",
        "\n",
        "  df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "  print(df_train.shape)\n",
        "  # df_train.head()\n",
        "\n",
        "  df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "  print(df_test.shape)\n",
        "\n",
        "  if arg1!=1:\n",
        "    return df_test\n",
        "  # df_test.head()\n",
        "\n",
        "  df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "  print(df_unlabeled.shape)\n",
        "  # df_unlabeled.head()\n",
        "\n",
        "  clean_train_df = clean(df_train)\n",
        "  clean_test_df = clean(df_test)\n",
        "  clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "  texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "  # Create Corpus\n",
        "  # model = word2vec.Word2Vec(texts, min_count=1)\n",
        "\n",
        "  NGRAM_RANGE = (1, 2)\n",
        "  TOP_K = 20000\n",
        "  TOKEN_MODE = 'word'\n",
        "  MIN_DOC_FREQ = 2\n",
        "\n",
        "  kwargs = {\n",
        "      'ngram_range' : NGRAM_RANGE,\n",
        "      'dtype' : 'int32',\n",
        "      'strip_accents' : 'unicode',\n",
        "      'decode_error' : 'replace',\n",
        "      'analyzer' : TOKEN_MODE,\n",
        "      'min_df' : MIN_DOC_FREQ,\n",
        "  }\n",
        "\n",
        "  tfidf_vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "\n",
        "  ##save the modeled words produced from Word2Vec\n",
        "  # model.save('word2vec_model.bin')\n",
        "\n",
        "\n",
        "  # loaded_model=KeyedVectors.load('word2vec_model.bin')\n",
        "  # print(\"Loaded word2vec model:\", loaded_model)\n",
        "\n",
        "  # num_features = 100 # See Word2Vec(vocab=213802, size=100, alpha=0.025) in above cell.\n",
        "  # feature_vec_train = get_avg_feature_vecs(clean_train_df['review'], loaded_model, num_features)\n",
        "  # feature_vec_test = get_avg_feature_vecs(clean_test_df['review'], loaded_model, num_features)\n",
        "  # feature_vec_unlabeled = get_avg_feature_vecs(clean_unlabeled_df['review'], loaded_model, num_features)\n",
        "\n",
        "  feature_vec_full_train = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "  feature_vec_train = tfidf_vectorizer.transform(clean_train_df['review'])\n",
        "  feature_vec_test = tfidf_vectorizer.transform(clean_test_df['review'])\n",
        "  feature_vec_unlabeled = tfidf_vectorizer.transform(clean_unlabeled_df['review'])\n",
        "\n",
        "\n",
        "  TOP_K = 20000\n",
        "  selector = SelectKBest(f_classif, k=min(TOP_K, feature_vec_train.shape[1]))\n",
        "\n",
        "  selector.fit(feature_vec_train, np.array([i for i in list(df_train.sentiment)]))\n",
        "\n",
        "  feature_vec_train = selector.transform(feature_vec_train).astype('float32').toarray()\n",
        "  feature_vec_test = selector.transform(feature_vec_test).astype('float32').toarray()\n",
        "  feature_vec_unlabeled = selector.transform(feature_vec_unlabeled).astype('float32').toarray()\n",
        "\n",
        "\n",
        "  print(\"TYPE\", type(feature_vec_train))\n",
        "  feature_vec_full_train = np.concatenate((feature_vec_train, feature_vec_unlabeled))\n",
        "  print(\"Shape of train data: \",feature_vec_full_train.shape)\n",
        "\n",
        "  labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "  return feature_vec_full_train, feature_vec_test, labels, test_labels, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6Inu13xELz0",
        "outputId": "14a91ff9-27ec-4ffe-f664-c9e43b9a4f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " code\t     LICENSE\t\t\t'Running MixText v5 ISIS Data.ipynb'\n",
            " data\t     README.md\t\t\t'Semi-Supervised sklearn.ipynb'\n",
            " data_temp  'Running MixText v3.ipynb'\t word2vec_model.bin\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNOdScrBIyQ3",
        "outputId": "1c3d6868-fbc9-462b-f259-ef2e91e5363e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read data successful (37500, 2) (12500, 2) n_labeled =  20\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 20, Unlabeled 2000, Val 4000, Test 12500\n",
            "(20, 2)\n",
            "(12500, 2)\n",
            "(2000, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1804: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TYPE <class 'numpy.ndarray'>\n",
            "Shape of train data:  (2020, 20000)\n",
            "TRAIN VEC:  (2020, 20000)\n",
            "10 SAVED!!\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  400\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 400, Unlabeled 2000, Val 4000, Test 12500\n",
            "(400, 2)\n",
            "(12500, 2)\n",
            "(2000, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1804: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TYPE <class 'numpy.ndarray'>\n",
            "Shape of train data:  (2400, 20000)\n",
            "TRAIN VEC:  (2400, 20000)\n",
            "200 SAVED!!\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  1000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 1000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(1000, 2)\n",
            "(12500, 2)\n",
            "(2000, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1804: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TYPE <class 'numpy.ndarray'>\n",
            "Shape of train data:  (3000, 20000)\n",
            "TRAIN VEC:  (3000, 20000)\n",
            "500 SAVED!!\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  2000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 2000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(2000, 2)\n",
            "(12500, 2)\n",
            "(2000, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1804: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TYPE <class 'numpy.ndarray'>\n",
            "Shape of train data:  (4000, 20000)\n",
            "TRAIN VEC:  (4000, 20000)\n",
            "1000 SAVED!!\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  4800\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 4800, Unlabeled 2000, Val 4000, Test 12500\n",
            "(4800, 2)\n",
            "(12500, 2)\n",
            "(2000, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1804: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TYPE <class 'numpy.ndarray'>\n",
            "Shape of train data:  (6800, 20000)\n",
            "TRAIN VEC:  (6800, 20000)\n",
            "2400 SAVED!!\n"
          ]
        }
      ],
      "source": [
        "# For tf-idf\n",
        "\n",
        "n_list = [10, 200, 500, 1000, 2400]\n",
        "# n_list=[10]\n",
        "\n",
        "for i in n_list:\n",
        "\n",
        "  train_vec, test_vec, labels, test_labels, df_test = get_data_tfidf(i)\n",
        "\n",
        "  print(\"TRAIN VEC: \", train_vec.shape)\n",
        "  np.save('data_temp/tf-idf/train_vec_'+str(i), train_vec)\n",
        "  np.save('data_temp/tf-idf/test_vec_'+str(i), test_vec)\n",
        "  np.save('data_temp/tf-idf/labels_'+str(i), labels)\n",
        "  np.save('data_temp/tf-idf/test_labels_'+str(i), test_labels)\n",
        "\n",
        "  print(i, 'SAVED!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92XsyvGLO55s",
        "outputId": "91d9c046-48b3-49cf-d5c3-34cfc4e1836e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read data successful (37500, 2) (12500, 2) n_labeled =  20\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 20, Unlabeled 2000, Val 4000, Test 12500\n",
            "(20, 2)\n",
            "(12500, 2)\n",
            "HELLO! (2020, 20000)\n",
            "Accuracy for  20  labeled data is: 54.040\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  400\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 400, Unlabeled 2000, Val 4000, Test 12500\n",
            "(400, 2)\n",
            "(12500, 2)\n",
            "HELLO! (2400, 20000)\n",
            "Accuracy for  400  labeled data is: 67.456\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  1000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 1000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(1000, 2)\n",
            "(12500, 2)\n",
            "HELLO! (3000, 20000)\n",
            "Accuracy for  1000  labeled data is: 68.272\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  2000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 2000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(2000, 2)\n",
            "(12500, 2)\n",
            "HELLO! (4000, 20000)\n",
            "Accuracy for  2000  labeled data is: 52.896\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  4800\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 4800, Unlabeled 2000, Val 4000, Test 12500\n",
            "(4800, 2)\n",
            "(12500, 2)\n",
            "HELLO! (6800, 20000)\n",
            "Accuracy for  4800  labeled data is: 51.088\n"
          ]
        }
      ],
      "source": [
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "n_list = [10, 200, 500, 1000, 2400]\n",
        "\n",
        "acc_list = {}\n",
        "\n",
        "for i in n_list:\n",
        "\n",
        "  df_test = get_data_tfidf(i, 10)\n",
        "\n",
        "  train_vec = np.load('data_temp/tf-idf/train_vec_'+str(i)+'.npy')\n",
        "  test_vec = np.load('data_temp/tf-idf/test_vec_'+str(i)+'.npy')\n",
        "  labels = np.load('data_temp/tf-idf/labels_'+str(i)+'.npy')\n",
        "  # test_labels = np.load('data_temp/test_labels_'+str(i)+'.npy')\n",
        "\n",
        "  print(\"HELLO!\", train_vec.shape)\n",
        "\n",
        "  label_prop_model = LabelSpreading()\n",
        "  label_prop_model.fit(train_vec, labels)\n",
        "\n",
        "  test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "  y_hat = label_prop_model.predict(test_vec)\n",
        "  score = accuracy_score(test_labels, y_hat)\n",
        "\n",
        "  acc_list[i] = score*100\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOwQR-52QZFX",
        "outputId": "db47194a-909d-43a0-892c-6e627d4ff35a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.04196495, 0.07739466, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.02940054, 0.        , 0.05313623, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]], dtype=float32)"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_vec = np.load('data_temp/tf-idf/train_vec_'+str(10)+'.npy')\n",
        "train_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id0QPBz6O56B",
        "outputId": "e0326327-dec8-47b7-a5fd-7d8bb0ff3eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------Label Spreading---------\n",
            "Accuracy for  20  labeled data is: 54.040\n",
            "Accuracy for  400  labeled data is: 67.456\n",
            "Accuracy for  1000  labeled data is: 68.272\n",
            "Accuracy for  2000  labeled data is: 52.896\n",
            "Accuracy for  4800  labeled data is: 51.088\n"
          ]
        }
      ],
      "source": [
        "print('---------Label Spreading---------')\n",
        "for i in n_list:\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % acc_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOBK02YSO56D",
        "outputId": "c6dba665-7fb6-41ad-9793-d11b28f08789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read data successful (37500, 2) (12500, 2) n_labeled =  20\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 20, Unlabeled 2000, Val 4000, Test 12500\n",
            "(20, 2)\n",
            "(12500, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/semi_supervised/_label_propagation.py:292: ConvergenceWarning: max_iter=1000 was reached without convergence.\n",
            "  category=ConvergenceWarning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for  20  labeled data is: 50.824\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  400\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 400, Unlabeled 2000, Val 4000, Test 12500\n",
            "(400, 2)\n",
            "(12500, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/semi_supervised/_label_propagation.py:292: ConvergenceWarning: max_iter=1000 was reached without convergence.\n",
            "  category=ConvergenceWarning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for  400  labeled data is: 49.272\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  1000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 1000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(1000, 2)\n",
            "(12500, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/semi_supervised/_label_propagation.py:292: ConvergenceWarning: max_iter=1000 was reached without convergence.\n",
            "  category=ConvergenceWarning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for  1000  labeled data is: 49.592\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  2000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 2000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(2000, 2)\n",
            "(12500, 2)\n",
            "Accuracy for  2000  labeled data is: 78.264\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  4800\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 4800, Unlabeled 2000, Val 4000, Test 12500\n",
            "(4800, 2)\n",
            "(12500, 2)\n",
            "Accuracy for  4800  labeled data is: 52.776\n"
          ]
        }
      ],
      "source": [
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "n_list = [10, 200, 500, 1000, 2400]\n",
        "\n",
        "acc_list = {}\n",
        "\n",
        "for i in n_list:\n",
        "\n",
        "  df_test = get_data_tfidf(i, 10)\n",
        "\n",
        "  train_vec = np.load('data_temp/tf-idf/train_vec_'+str(i)+'.npy')\n",
        "  test_vec = np.load('data_temp/tf-idf/test_vec_'+str(i)+'.npy')\n",
        "  labels = np.load('data_temp/tf-idf/labels_'+str(i)+'.npy')\n",
        "  # test_labels = np.load('data_temp/test_labels_'+str(i)+'.npy')\n",
        "\n",
        "\n",
        "  label_prop_model = LabelPropagation()\n",
        "  label_prop_model.fit(train_vec, labels)\n",
        "\n",
        "  test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "  y_hat = label_prop_model.predict(test_vec)\n",
        "  score = accuracy_score(test_labels, y_hat)\n",
        "\n",
        "  acc_list[i] = score*100\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EisfYWxO56D",
        "outputId": "0601095a-91dd-4e84-9961-c0b342e05670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------Label Propagation---------\n",
            "Accuracy for  20  labeled data is: 50.824\n",
            "Accuracy for  400  labeled data is: 49.272\n",
            "Accuracy for  1000  labeled data is: 49.592\n",
            "Accuracy for  2000  labeled data is: 78.264\n",
            "Accuracy for  4800  labeled data is: 52.776\n"
          ]
        }
      ],
      "source": [
        "print('---------Label Propagation---------')\n",
        "for i in n_list:\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % acc_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aoZrVHEO56E",
        "outputId": "66716847-b6ae-4eb5-b463-82673e33bc8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read data successful (37500, 2) (12500, 2) n_labeled =  20\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 20, Unlabeled 2000, Val 4000, Test 12500\n",
            "(20, 2)\n",
            "(12500, 2)\n",
            "Accuracy for  20  labeled data is: 63.672\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  400\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 400, Unlabeled 2000, Val 4000, Test 12500\n",
            "(400, 2)\n",
            "(12500, 2)\n",
            "Accuracy for  400  labeled data is: 76.496\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  1000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 1000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(1000, 2)\n",
            "(12500, 2)\n",
            "Accuracy for  1000  labeled data is: 76.712\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  2000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 2000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(2000, 2)\n",
            "(12500, 2)\n",
            "Accuracy for  2000  labeled data is: 80.968\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  4800\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 4800, Unlabeled 2000, Val 4000, Test 12500\n",
            "(4800, 2)\n",
            "(12500, 2)\n",
            "Accuracy for  4800  labeled data is: 79.168\n"
          ]
        }
      ],
      "source": [
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "n_list = [10, 200, 500, 1000, 2400]\n",
        "\n",
        "acc_list = {}\n",
        "\n",
        "for i in n_list:\n",
        "\n",
        "  df_test = get_data_tfidf(i, 10)\n",
        "\n",
        "  train_vec = np.load('data_temp/tf-idf/train_vec_'+str(i)+'.npy')\n",
        "  test_vec = np.load('data_temp/tf-idf/test_vec_'+str(i)+'.npy')\n",
        "  labels = np.load('data_temp/tf-idf/labels_'+str(i)+'.npy')\n",
        "  # test_labels = np.load('data_temp/test_labels_'+str(i)+'.npy')\n",
        "\n",
        "  \n",
        "  svc = SVC(probability=True, gamma=\"auto\") \n",
        "\n",
        "\n",
        "  label_prop_model = SelfTrainingClassifier(svc)\n",
        "  label_prop_model.fit(train_vec, labels)\n",
        "\n",
        "  test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "  y_hat = label_prop_model.predict(test_vec)\n",
        "  score = accuracy_score(test_labels, y_hat)\n",
        "\n",
        "  acc_list[i] = score*100\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOUdPIalO56F",
        "outputId": "02d346fe-1681-4aca-8bb2-eb6a69686131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------Self Training Classifier with SVM---------\n",
            "Accuracy for  20  labeled data is: 63.672\n",
            "Accuracy for  400  labeled data is: 76.496\n",
            "Accuracy for  1000  labeled data is: 76.712\n",
            "Accuracy for  2000  labeled data is: 80.968\n",
            "Accuracy for  4800  labeled data is: 79.168\n"
          ]
        }
      ],
      "source": [
        "print('---------Self Training Classifier with SVM---------')\n",
        "for i in n_list:\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % acc_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC3hIAr0Nymo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xmedDtuNztH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8KmItXINzj8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO4leU_WNzaj",
        "outputId": "fb33e700-9e33-4c43-8c71-5520de6475cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read data successful (37500, 2) (12500, 2) n_labeled =  20\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 20, Unlabeled 2000, Val 4000, Test 12500\n",
            "(20, 2)\n",
            "(12500, 2)\n",
            "SHAPE:  (2020, 20000) SIZE:  (2020,)\n",
            "New SHAPE:  (20, 20000) New SIZE:  (20,) [-1  0  1] [0 1]\n",
            "Accuracy for  20  labeled data is: 63.672\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  400\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 400, Unlabeled 2000, Val 4000, Test 12500\n",
            "(400, 2)\n",
            "(12500, 2)\n",
            "SHAPE:  (2400, 20000) SIZE:  (2400,)\n",
            "New SHAPE:  (400, 20000) New SIZE:  (400,) [-1  0  1] [0 1]\n",
            "Accuracy for  400  labeled data is: 76.496\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  1000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 1000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(1000, 2)\n",
            "(12500, 2)\n",
            "SHAPE:  (3000, 20000) SIZE:  (3000,)\n",
            "New SHAPE:  (1000, 20000) New SIZE:  (1000,) [-1  0  1] [0 1]\n",
            "Accuracy for  1000  labeled data is: 76.712\n",
            "Read data successful (37500, 2) (12500, 2) n_labeled =  2000\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n",
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n",
            "#Labeled: 2000, Unlabeled 2000, Val 4000, Test 12500\n",
            "(2000, 2)\n",
            "(12500, 2)\n",
            "SHAPE:  (4000, 20000) SIZE:  (4000,)\n",
            "New SHAPE:  (2000, 20000) New SIZE:  (2000,) [-1  0  1] [0 1]\n",
            "Accuracy for  2000  labeled data is: 80.968\n"
          ]
        }
      ],
      "source": [
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Without Unlabeled Data\n",
        "\n",
        "n_list = [10, 200, 500, 1000, 2400]\n",
        "n_list = [10, 200, 500, 1000]\n",
        "acc_list_2 = {}\n",
        "\n",
        "for i in n_list:\n",
        "\n",
        "  df_test = get_data_tfidf(i, 10)\n",
        "\n",
        "  train_vec = np.load('data_temp/tf-idf/train_vec_'+str(i)+'.npy')\n",
        "  test_vec = np.load('data_temp/tf-idf/test_vec_'+str(i)+'.npy')\n",
        "  labels = np.load('data_temp/tf-idf/labels_'+str(i)+'.npy')\n",
        "  # test_labels = np.load('data_temp/test_labels_'+str(i)+'.npy')\n",
        "\n",
        "  \n",
        "  svc = SVC(probability=True, gamma=\"auto\") \n",
        "\n",
        "  print(\"SHAPE: \", train_vec.shape, \"SIZE: \", labels.shape)\n",
        "  print(\"New SHAPE: \", train_vec[:2*i].shape, \"New SIZE: \", labels[:2*i].shape, np.unique(labels), np.unique(labels[:2*i]))\n",
        "\n",
        "  svc.fit(train_vec[:2*i], labels[:2*i])\n",
        "\n",
        "  test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "  y_hat = svc.predict(test_vec)\n",
        "  score = accuracy_score(test_labels, y_hat)\n",
        "\n",
        "  acc_list_2[i] = score*100\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqSmXx-oNzO2",
        "outputId": "f45aa101-b9ab-4760-d08c-1b4139dbe84a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------Label Spreading---------\n",
            "Accuracy for  20  labeled data is: 63.672\n",
            "Accuracy for  400  labeled data is: 76.496\n",
            "Accuracy for  1000  labeled data is: 76.712\n",
            "Accuracy for  2000  labeled data is: 80.968\n"
          ]
        }
      ],
      "source": [
        "print('---------Label Spreading---------')\n",
        "for i in n_list:\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % acc_list_2[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwK3q6lgztTl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IwjQeJDztFB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WRjVlZSNzBj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLi7DnF4oUN2",
        "outputId": "5c92617e-9471-41d5-8921-891cd88b5c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def get_data(n, arg1=1):\n",
        "\n",
        "  n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "  unlabeled_per_class = 1000\n",
        "\n",
        "\n",
        "  data_path = './data/imdb_data/'\n",
        "  train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "  test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "  print(\"Read data successful\", train_df.shape, test_df.shape, \"n_labeled = \", n_labeled_per_class*2)\n",
        "  print(train_df.head(5))\n",
        "\n",
        "\n",
        "  train_labels = np.array([v for v in train_df[1]])\n",
        "  train_text = np.array([v for v in train_df[0]])\n",
        "  print(train_labels[:3])\n",
        "  print(train_text[:3])\n",
        "\n",
        "  test_labels = np.array([u for u in test_df[1]])\n",
        "  test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  n_labels = 2\n",
        "  # Split the labeled training set, unlabeled training set, development set\n",
        "  train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "      train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "  print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "  \n",
        "\n",
        "\n",
        "  df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "  print(df_train.shape)\n",
        "  # df_train.head()\n",
        "\n",
        "  df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "  print(df_test.shape)\n",
        "\n",
        "  if arg1!=1:\n",
        "    return df_test\n",
        "  # df_test.head()\n",
        "\n",
        "  df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "  print(df_unlabeled.shape)\n",
        "  # df_unlabeled.head()\n",
        "\n",
        "  clean_train_df = clean(df_train)\n",
        "  clean_test_df = clean(df_test)\n",
        "  clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "  texts = (clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)).apply(lambda x:x.split())\n",
        "\n",
        "  # Create Corpus\n",
        "  embedding_vector_size = 300\n",
        "  model = word2vec.Word2Vec(sentences = texts, size = embedding_vector_size, min_count=3, window=5, workers=4)\n",
        "\n",
        "  ##save the modeled words produced from Word2Vec\n",
        "  model.save('word2vec_model.bin')\n",
        "\n",
        "\n",
        "  loaded_model=KeyedVectors.load('word2vec_model.bin')\n",
        "  print(\"Loaded word2vec model:\", loaded_model)\n",
        "\n",
        "  num_features = 300 # See Word2Vec(vocab=213802, size=100, alpha=0.025) in above cell.\n",
        "  feature_vec_train = get_avg_feature_vecs(clean_train_df['review'], loaded_model, num_features)\n",
        "  feature_vec_test = get_avg_feature_vecs(clean_test_df['review'], loaded_model, num_features)\n",
        "  feature_vec_unlabeled = get_avg_feature_vecs(clean_unlabeled_df['review'], loaded_model, num_features)\n",
        "\n",
        "\n",
        "\n",
        "  feature_vec_full_train = np.concatenate((feature_vec_train, feature_vec_unlabeled))\n",
        "  print(\"Shape of train data: \",feature_vec_full_train.shape)\n",
        "\n",
        "  labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "  return feature_vec_full_train, feature_vec_test, labels, test_labels, df_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "lp4at3iLBlJK",
        "outputId": "56963360-814d-455d-c1da-3070e8d1c83b"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-d1bec0f8001e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    For word2vec\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# For word2vec\n",
        "\n",
        "n_list = [10, 200, 500, 1000, 2400]\n",
        "\n",
        "for i in n_list:\n",
        "\n",
        "  train_vec, test_vec, labels, test_labels, df_test = get_data(i)\n",
        "\n",
        "  np.save('data_temp/train_vec_'+str(i), train_vec)\n",
        "  np.save('data_temp/test_vec_'+str(i), test_vec)\n",
        "  np.save('data_temp/labels_'+str(i), labels)\n",
        "  np.save('data_temp/test_labels_'+str(i), test_labels)\n",
        "\n",
        "  print(i, 'SAVED!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F55rEyMDjS9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLx-p9QG6Qep"
      },
      "outputs": [],
      "source": [
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "n_list = [10, 200, 500, 1000, 2400]\n",
        "\n",
        "acc_list = {}\n",
        "\n",
        "for i in n_list:\n",
        "\n",
        "  df_test = get_data(i, 10)\n",
        "\n",
        "  train_vec = np.load('data_temp/train_vec_'+str(i)+'.npy')\n",
        "  test_vec = np.load('data_temp/test_vec_'+str(i)+'.npy')\n",
        "  labels = np.load('data_temp/labels_'+str(i)+'.npy')\n",
        "  # test_labels = np.load('data_temp/test_labels_'+str(i)+'.npy')\n",
        "\n",
        "\n",
        "  label_prop_model = LabelSpreading()\n",
        "  label_prop_model.fit(train_vec, labels)\n",
        "\n",
        "  test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "  y_hat = label_prop_model.predict(test_vec)\n",
        "  score = accuracy_score(test_labels, y_hat)\n",
        "\n",
        "  acc_list[i] = score*100\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV0n6QSr6S6I",
        "outputId": "5e4e86fc-2bfa-4f46-995f-b21c3089d7f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------Label Spreading---------\n",
            "Accuracy for  20  labeled data is: 49.584\n",
            "Accuracy for  400  labeled data is: 53.720\n",
            "Accuracy for  1000  labeled data is: 54.448\n",
            "Accuracy for  2000  labeled data is: 54.480\n",
            "Accuracy for  4800  labeled data is: 54.536\n"
          ]
        }
      ],
      "source": [
        "print('---------Label Spreading---------')\n",
        "for i in n_list:\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % acc_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPzaKWo_6S0V"
      },
      "outputs": [],
      "source": [
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "n_list = [10, 200, 500, 1000, 2400]\n",
        "\n",
        "acc_list = {}\n",
        "\n",
        "for i in n_list:\n",
        "\n",
        "  df_test = get_data(i, 10)\n",
        "\n",
        "  train_vec = np.load('data_temp/train_vec_'+str(i)+'.npy')\n",
        "  test_vec = np.load('data_temp/test_vec_'+str(i)+'.npy')\n",
        "  labels = np.load('data_temp/labels_'+str(i)+'.npy')\n",
        "  # test_labels = np.load('data_temp/test_labels_'+str(i)+'.npy')\n",
        "\n",
        "\n",
        "  label_prop_model = LabelPropagation()\n",
        "  label_prop_model.fit(train_vec, labels)\n",
        "\n",
        "  test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "  y_hat = label_prop_model.predict(test_vec)\n",
        "  score = accuracy_score(test_labels, y_hat)\n",
        "\n",
        "  acc_list[i] = score*100\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ5gbBgM6Sxu",
        "outputId": "629f416e-e4b1-448f-9549-77c1609371b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------Label Propagation---------\n",
            "Accuracy for  20  labeled data is: 49.256\n",
            "Accuracy for  400  labeled data is: 51.400\n",
            "Accuracy for  1000  labeled data is: 54.152\n",
            "Accuracy for  2000  labeled data is: 54.272\n",
            "Accuracy for  4800  labeled data is: 54.400\n"
          ]
        }
      ],
      "source": [
        "print('---------Label Propagation---------')\n",
        "for i in n_list:\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % acc_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ151v0C6SuQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "n_list = [10, 200, 500, 1000, 2400]\n",
        "\n",
        "acc_list = {}\n",
        "\n",
        "for i in n_list:\n",
        "\n",
        "  df_test = get_data(i, 10)\n",
        "\n",
        "  train_vec = np.load('data_temp/train_vec_'+str(i)+'.npy')\n",
        "  test_vec = np.load('data_temp/test_vec_'+str(i)+'.npy')\n",
        "  labels = np.load('data_temp/labels_'+str(i)+'.npy')\n",
        "  # test_labels = np.load('data_temp/test_labels_'+str(i)+'.npy')\n",
        "\n",
        "  \n",
        "  svc = SVC(probability=True, gamma=\"auto\") \n",
        "\n",
        "\n",
        "  label_prop_model = SelfTrainingClassifier(svc)\n",
        "  label_prop_model.fit(train_vec, labels)\n",
        "\n",
        "  test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "  y_hat = label_prop_model.predict(test_vec)\n",
        "  score = accuracy_score(test_labels, y_hat)\n",
        "\n",
        "  acc_list[i] = score*100\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECPpdjBKAnPG",
        "outputId": "a0d5df93-e3b8-46c2-ea24-4226077dee06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------Self Training Classifier with SVM---------\n",
            "Accuracy for  20  labeled data is: 52.928\n",
            "Accuracy for  400  labeled data is: 53.400\n",
            "Accuracy for  1000  labeled data is: 54.120\n",
            "Accuracy for  2000  labeled data is: 54.944\n",
            "Accuracy for  4800  labeled data is: 55.224\n"
          ]
        }
      ],
      "source": [
        "print('---------Self Training Classifier with SVM---------')\n",
        "for i in n_list:\n",
        "  print('Accuracy for ', i*2,  ' labeled data is: %.3f' % acc_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf62S88CAnL0",
        "outputId": "ab19e4d2-11b8-43e7-eff7-1bd06209ba12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "System:\n",
            "    python: 3.7.10 (default, May  3 2021, 02:48:31)  [GCC 7.5.0]\n",
            "executable: /usr/bin/python3\n",
            "   machine: Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic\n",
            "\n",
            "Python dependencies:\n",
            "          pip: 19.3.1\n",
            "   setuptools: 56.1.0\n",
            "      sklearn: 0.24.2\n",
            "        numpy: 1.19.5\n",
            "        scipy: 1.4.1\n",
            "       Cython: 0.29.23\n",
            "       pandas: 1.1.5\n",
            "   matplotlib: 3.2.2\n",
            "       joblib: 1.0.1\n",
            "threadpoolctl: 2.1.0\n",
            "\n",
            "Built with OpenMP: True\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "sklearn.show_versions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbiiO_mjAnDK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKwojdH5AnAn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqVeK_8MAm9X"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHBsuQ9w6Sru"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTd4DuHg6Sfv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LADiZQhdhZ7s"
      },
      "outputs": [],
      "source": [
        "n_labeled_per_class = 200       #10, 200, 500, 1000, 2400\n",
        "unlabeled_per_class = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvL1TsRSgHtZ",
        "outputId": "a6dc4268-85d7-44b1-8d11-19faaf8890d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read data successful (37500, 2) (12500, 2) n_labeled =  200\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n"
          ]
        }
      ],
      "source": [
        "data_path = './data/imdb_data/'\n",
        "train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "print(\"Read data successful\", train_df.shape, test_df.shape, \"n_labeled = \", n_labeled_per_class)\n",
        "print(train_df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd--NF6UhLiW",
        "outputId": "a641ef90-9ddd-4b41-e863-754166bff6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0]\n",
            "['I figure this to be an \"alternate reality\" teen flick...More precisely a Ferris Bueller type character as the leader of a cheat ring . Yeah, I know it\\'s not meant to compared to Ferris Bueller, at least not in a \"oranges-to-oranges\" way, but it will none-the-less.<br /><br />Bottom-line: It\\'s galaxies away from even being even a minor classic. It is watchable, though only if you\\'re not expecting very much. That said, the main character has some charm, but the premise wears thin because the writing just isn\\'t clever. The movie just did not deliver enough laughs, twists, or tension to keep my interest. <br /><br />To be honest I did continue watching...Watching with hopes to see if anything suddenly clicked. It didn\\'t. So, stylish as it is, I wouldn\\'t recommend this movie. BTW, it seems odd to see Mary Tyler Moore as the principal. She\\'s truly miscast, I hope the paycheck was inordinately big.'\n",
            " \"This is the kind of movie that wants to be good but sucks. First thing, what the hell are those punk trying to do with the school? I think the kids doesn't seem to realize the gravity of the situation. Deker guy say to the girl that they under his responsibility when she ask why he wants to go back for them but right after this he gives a gun to the wheel chair dude and wants him to go alone repair the phone line. Where is the responsibility there? I understand poor actors must pay their food but why not just give them the money that takes to make a stupid movie like that or give that money to a charity. Oh yea and none of them knows how to aim. The stupid punk guy shoots in the cafeteria nowhere like a crazy. They all want to look professional but they all suck. One more thing I don't believe that there's no emergency exit in the school the kids are trying several doors but they all locked. What happens if there's a fire and the dumass security guard is dead? It is illegal to not have an emergency exit in school. Anyway there's a lot more to say but it would be too long. I spent some time of my life to watch a crap.\"\n",
            " \"This was by far the worst movie I've ever seen. And thats compared to Alexander, Fortress 2 and The new world.<br /><br />I should go back to blockbuster and ask for my money back along with compensation as it was a truly traumatic experience. For the first ten minutes i was changing the zoom on my widescreen TV because the actors seemed to be out of screen. I didn't think it was possible to make such a bad film in this day and age, i was wrong. While typing this message, I've thought of a good reason to buy this movie. A joke present at Xmas. I'm blaming the Mrs for this one as she picked it, thanks babe.<br /><br />Be warned.......A true shocker all round!!!!!!\"]\n"
          ]
        }
      ],
      "source": [
        "train_labels = np.array([v for v in train_df[1]])\n",
        "train_text = np.array([v for v in train_df[0]])\n",
        "print(train_labels[:3])\n",
        "print(train_text[:3])\n",
        "\n",
        "test_labels = np.array([u for u in test_df[1]])\n",
        "test_text = np.array([v for v in test_df[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad1fPvp9hpHc",
        "outputId": "48e59edb-455c-487d-bd15-c6b14de298a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#Labeled: 400, Unlabeled 2000, Val 4000, Test 12500\n"
          ]
        }
      ],
      "source": [
        "n_labels = 2\n",
        "# Split the labeled training set, unlabeled training set, development set\n",
        "train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "    train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "# Build the dataset class for each set\n",
        "# train_labeled_dataset = loader_labeled(\n",
        "#     train_text[train_labeled_idxs], train_labels[train_labeled_idxs], tokenizer, max_seq_len, train_aug)\n",
        "# train_unlabeled_dataset = loader_unlabeled(\n",
        "#     train_text[train_unlabeled_idxs], train_unlabeled_idxs, tokenizer, max_seq_len, Translator(data_path))\n",
        "# val_dataset = loader_labeled(\n",
        "#     train_text[val_idxs], train_labels[val_idxs], tokenizer, max_seq_len)\n",
        "# test_dataset = loader_labeled(\n",
        "#     test_text, test_labels, tokenizer, max_seq_len)\n",
        "\n",
        "print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "    train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "dzvzyVJMh69Q",
        "outputId": "dc1dda1e-c73b-4039-93c0-eed61fa584e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(400, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>okay, let's cut to the chase - there's no way ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I saw Hurlyburly on Broadway and liked it a gr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>First of all, I'd like to tell you that I'm in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Harlan Knowles (Lance Henriksen) brings a grou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ugh. Pretty awful.&lt;br /&gt;&lt;br /&gt;Linnea Quigley g...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  okay, let's cut to the chase - there's no way ...          0\n",
              "1  I saw Hurlyburly on Broadway and liked it a gr...          0\n",
              "2  First of all, I'd like to tell you that I'm in...          0\n",
              "3  Harlan Knowles (Lance Henriksen) brings a grou...          0\n",
              "4  Ugh. Pretty awful.<br /><br />Linnea Quigley g...          0"
            ]
          },
          "execution_count": 42,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "print(df_train.shape)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "IqGkHDuRhvBz",
        "outputId": "8c8c3421-121b-4de3-b211-94645e8d46dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12500, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I really liked this Summerslam due to the look...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not many television shows appeal to quite as m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The film quickly gets to a major chase scene w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jane Austen would definitely approve of this o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Expectations were somewhat high for me when I ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  I really liked this Summerslam due to the look...          1\n",
              "1  Not many television shows appeal to quite as m...          1\n",
              "2  The film quickly gets to a major chase scene w...          0\n",
              "3  Jane Austen would definitely approve of this o...          1\n",
              "4  Expectations were somewhat high for me when I ...          0"
            ]
          },
          "execution_count": 43,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "print(df_test.shape)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "MWJ5ye9LpPsl",
        "outputId": "f316306f-54b9-4d7a-df55-78c40c35dfb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2000, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This is the epitome of bad 80's film-making, u...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(Only minor spoilers except as noted).&lt;br /&gt;&lt;b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'Dead Letter Office' is a low-budget film abou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Anyone who visited drive-ins in the 1950s, 60s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Feature of early 21 century cinema of lets pit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  This is the epitome of bad 80's film-making, u...          0\n",
              "1  (Only minor spoilers except as noted).<br /><b...          0\n",
              "2  'Dead Letter Office' is a low-budget film abou...          0\n",
              "3  Anyone who visited drive-ins in the 1950s, 60s...          0\n",
              "4  Feature of early 21 century cinema of lets pit...          0"
            ]
          },
          "execution_count": 44,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "print(df_unlabeled.shape)\n",
        "df_unlabeled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5pOwJL7k2oE"
      },
      "outputs": [],
      "source": [
        "#Lemmatize the dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KC8EnsslnU2",
        "outputId": "9f26021e-7eee-43d6-ea14-fa7a5d2cfd59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 48,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk2T6XVbk8Q0",
        "outputId": "0644b4d4-6d65-4e9d-d773-2177da0852fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 13.2 s, sys: 221 ms, total: 13.4 s\n",
            "Wall time: 13.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "clean_train_df = clean(df_train)\n",
        "clean_test_df = clean(df_test)\n",
        "clean_unlabeled_df = clean(df_unlabeled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8V-vCBSmgNM"
      },
      "outputs": [],
      "source": [
        "# texts = (df_train['review'].append(df_test['review'], ignore_index=True)).apply(lambda x:x.split())\n",
        "texts = (clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)).apply(lambda x:x.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI3wRK-jllcP",
        "outputId": "017f4c6c-4f6b-4f19-b858-5d7d17e24e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 15.6 s, sys: 199 ms, total: 15.8 s\n",
            "Wall time: 10.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import word2vec\n",
        "# Create Corpus\n",
        "model = word2vec.Word2Vec(texts, min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km6-cTYMmeGs",
        "outputId": "efbdc037-1713-4e70-a285-a527a3d53876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['okay', 'lets', 'cut', 'to', 'the']\n"
          ]
        }
      ],
      "source": [
        "word_li=list(model.wv.vocab)\n",
        "print(word_li[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Zdcw55np1X"
      },
      "outputs": [],
      "source": [
        "##save the modeled words produced from Word2Vec\n",
        "model.save('word2vec_model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDDI7NWgntHt",
        "outputId": "819bc702-d3f5-48bf-e98f-0d7c3f14ef14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=39344, size=100, alpha=0.025)\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "loaded_model=KeyedVectors.load('word2vec_model.bin')\n",
        "print(loaded_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lmF6Kxkn0jw",
        "outputId": "2650a46d-aef9-412e-b435-6f7adbd77076"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 7s, sys: 1.51 s, total: 2min 9s\n",
            "Wall time: 2min 8s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "num_features = 100 # See Word2Vec(vocab=213802, size=100, alpha=0.025) in above cell.\n",
        "feature_vec_train = get_avg_feature_vecs(clean_train_df['review'], loaded_model, num_features)\n",
        "feature_vec_test = get_avg_feature_vecs(clean_test_df['review'], loaded_model, num_features)\n",
        "feature_vec_unlabeled = get_avg_feature_vecs(clean_unlabeled_df['review'], loaded_model, num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQU3xbJKn6-w",
        "outputId": "3f3dfc93-dda4-4659-b3d8-543af2f33c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(400, 100) (12500, 100) (2000, 100)\n"
          ]
        }
      ],
      "source": [
        "print(feature_vec_train.shape, feature_vec_test.shape, feature_vec_unlabeled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oLS0n34Ywuq",
        "outputId": "d9a368f7-a6d1-4ca9-d12f-b9025500c539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(type(feature_vec_train), type(feature_vec_test), type(feature_vec_unlabeled))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npqGvc1LxkFF",
        "outputId": "439f01c1-7229-4057-ec13-0fbf99f32dd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2400, 100)"
            ]
          },
          "execution_count": 59,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_vec_full_train = np.concatenate((feature_vec_train, feature_vec_unlabeled))\n",
        "feature_vec_full_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCxBTB1tn9SV",
        "outputId": "89dd801a-806c-48f3-e0c9-ec3572e3f517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0  0  0 ... -1 -1 -1]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2400"
            ]
          },
          "execution_count": 60,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "print(labels)\n",
        "len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvRaIpSUyE-B",
        "outputId": "be9bd31c-21ec-4e66-9327-cebbc5afc1f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 49.592\n"
          ]
        }
      ],
      "source": [
        "#n_labeled=20\n",
        "%%time\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "label_prop_model = LabelSpreading()\n",
        "label_prop_model.fit(feature_vec_full_train, labels)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "y_hat = label_prop_model.predict(feature_vec_test)\n",
        "score = accuracy_score(test_labels, y_hat)\n",
        "print('Accuracy: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAwtBoQ6zBNA",
        "outputId": "6cfa8802-94a5-409e-d258-c81b01dee49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 53.744\n",
            "CPU times: user 3.31 s, sys: 1.82 s, total: 5.14 s\n",
            "Wall time: 2.74 s\n"
          ]
        }
      ],
      "source": [
        "#n_labeled=400\n",
        "%%time\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "label_prop_model = LabelSpreading()\n",
        "label_prop_model.fit(feature_vec_full_train, labels)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "y_hat = label_prop_model.predict(feature_vec_test)\n",
        "score = accuracy_score(test_labels, y_hat)\n",
        "print('Accuracy: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6csbRILzgid"
      },
      "outputs": [],
      "source": [
        "#n_labeled=1000\n",
        "%%time\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "label_prop_model = LabelSpreading()\n",
        "label_prop_model.fit(feature_vec_full_train, labels)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "y_hat = label_prop_model.predict(feature_vec_test)\n",
        "score = accuracy_score(test_labels, y_hat)\n",
        "print('Accuracy: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8D4LSQQ0GVR"
      },
      "outputs": [],
      "source": [
        "#n_labeled=2000\n",
        "%%time\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "label_prop_model = LabelSpreading()\n",
        "label_prop_model.fit(feature_vec_full_train, labels)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "y_hat = label_prop_model.predict(feature_vec_test)\n",
        "score = accuracy_score(test_labels, y_hat)\n",
        "print('Accuracy: %.3f' % (score*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrUaqSlPznGO"
      },
      "outputs": [],
      "source": [
        "#n_labeled=4800\n",
        "%%time\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "label_prop_model = LabelSpreading()\n",
        "label_prop_model.fit(feature_vec_full_train, labels)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_labels = np.array([i for i in list(df_test.sentiment)])\n",
        "y_hat = label_prop_model.predict(feature_vec_test)\n",
        "score = accuracy_score(test_labels, y_hat)\n",
        "print('Accuracy: %.3f' % (score*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Semi-Supervised self training sklearn_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
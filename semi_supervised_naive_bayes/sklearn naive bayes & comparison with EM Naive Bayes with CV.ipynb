{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://github.com/jerry-shijieli/Text_Classification_Using_EM_And_Semisupervied_Learning/blob/master/code/EM_NB_text_classification_v3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and libraries\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import nltk as nk\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "# from Semi_EM_NB import Semi_EM_MultinomialNB\n",
    "%run Semi_EM_NB.ipynb\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data set with class labels \n",
    "train_Xy = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "test_Xy = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(sentence):\n",
    "    result = ''\n",
    "    poster = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopword_set = set(stopwords.words('english'))\n",
    "    wordlist = re.sub(r\"\\n|(\\\\(.*?){)|}|[!$%^&*#()_+|~\\-={}\\[\\]:\\\";'<>?,.\\/\\\\]|[0-9]|[@]\", ' ', sentence) # remove punctuation\n",
    "    wordlist = re.sub('\\s+', ' ', wordlist) # remove extra space\n",
    "    wordlist_normal = [poster.stem(word.lower()) for word in wordlist.split()] # restore word to its original form (stemming)\n",
    "    wordlist_normal = [lemmatizer.lemmatize(word, pos='v') for word in wordlist_normal] # restore word to its root form (lemmatization)\n",
    "    wordlist_clean = [word for word in wordlist_normal if word not in stopword_set] # remove stopwords\n",
    "    result = ' '.join(wordlist_clean)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess train and test text data\n",
    "train_Xy.data_clean = map(remove_noise, train_Xy.data)\n",
    "test_Xy.data_clean = map(remove_noise, test_Xy.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 28591) (7532, 28591)\n"
     ]
    }
   ],
   "source": [
    "# Convert all text data into tf-idf vectors \n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.95, ngram_range=(1,2))\n",
    "# vectorizer = TfidfVectorizer()\n",
    "train_vec = vectorizer.fit_transform(train_Xy.data_clean)\n",
    "test_vec = vectorizer.transform(test_Xy.data_clean)\n",
    "print(train_vec.shape, test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2262, 28591) (9052, 28591)\n"
     ]
    }
   ],
   "source": [
    "# Divide train data set into labeled and unlabeled data sets\n",
    "n_train_data = train_vec.shape[0]\n",
    "split_ratio = 0.2 # labeled vs total(labeled+unlabeled)\n",
    "X_l, X_u, y_l, y_u = train_test_split(train_vec, train_Xy.target, train_size=split_ratio, stratify=train_Xy.target)\n",
    "print(X_l.shape, X_u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(clf, data_X, data_y, unlabeled=None, n_folds=5):\n",
    "    print('=' * 80)\n",
    "    print(\"Validation: \")\n",
    "    print(clf)\n",
    "    kf = StratifiedKFold(n_splits=n_folds)\n",
    "    start_time = time()\n",
    "    train_accuracies= list() # training accuracy\n",
    "    fold_count = 1\n",
    "    original_clf = deepcopy(clf)\n",
    "    for train_ids, valid_ids in kf.split(data_X, data_y):\n",
    "        cv_clf = deepcopy(original_clf)\n",
    "        print(\"Fold # %d\" % fold_count)\n",
    "        fold_count += 1\n",
    "        train_X, train_y, valid_X, valid_y = data_X[train_ids], data_y[train_ids], data_X[valid_ids], data_y[valid_ids]\n",
    "        if unlabeled==None:\n",
    "            cv_clf.fit(train_X, train_y)\n",
    "        else:\n",
    "            cv_clf.fit(train_X, train_y, unlabeled)\n",
    "        pred = cv_clf.predict(valid_X)\n",
    "        train_accuracies.append(metrics.accuracy_score(valid_y, pred))\n",
    "    train_time = time() - start_time\n",
    "    print(\"Validation time: %0.3f seconds\" % train_time)\n",
    "    print(\"Average training accuracy: %0.3f\" % np.mean(np.array(train_accuracies)))\n",
    "    return train_accuracies, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.122 seconds\n",
      "Average training accuracy: 0.668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6775599128540305,\n",
       "  0.649890590809628,\n",
       "  0.7120879120879121,\n",
       "  0.6592427616926503,\n",
       "  0.6402714932126696],\n",
       " 0.12244248390197754)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation for Naive Bayes classifier \n",
    "# using labeled data set only\n",
    "nb_clf = MultinomialNB(alpha=1e-2)\n",
    "cross_validation(nb_clf, X_l, y_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "Fold # 1\n",
      "Initial expected log likelihood = -5640060.023\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -5193666.978\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -5192338.230\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -5192089.145\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -5192089.145\n",
      "Fold # 2\n",
      "Initial expected log likelihood = -5643146.147\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -5197182.405\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -5194837.307\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -5194576.083\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -5194576.083\n",
      "Fold # 3\n",
      "Initial expected log likelihood = -5639953.609\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -5198405.282\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -5196805.531\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -5196645.714\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -5196645.714\n",
      "Fold # 4\n",
      "Initial expected log likelihood = -5635301.561\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -5192901.086\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -5191563.473\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -5191362.339\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -5191362.339\n",
      "Fold # 5\n",
      "Initial expected log likelihood = -5635883.016\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -5192594.752\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -5191394.112\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -5191221.327\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -5191221.043\n",
      "EM iteration #5\n",
      "\tExpected log likelihood = -5191221.043\n",
      "Validation time: 40.447 seconds\n",
      "Average training accuracy: 0.686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.690631808278867,\n",
       "  0.687089715536105,\n",
       "  0.7362637362637363,\n",
       "  0.6592427616926503,\n",
       "  0.6561085972850679],\n",
       " 40.447001695632935)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation for semisupervised EM Naive Bayes classifier \n",
    "# using both labeled and unlabeled data set\n",
    "em_nb_clf = Semi_EM_MultinomialNB(alpha=1e-2) # semi supervised EM based Naive Bayes classifier\n",
    "cross_validation(em_nb_clf, X_l, y_l, X_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.52      0.30      0.38       319\n",
      "           comp.graphics       0.50      0.61      0.55       389\n",
      " comp.os.ms-windows.misc       0.59      0.48      0.53       394\n",
      "comp.sys.ibm.pc.hardware       0.55      0.61      0.58       392\n",
      "   comp.sys.mac.hardware       0.61      0.57      0.59       385\n",
      "          comp.windows.x       0.67      0.66      0.67       395\n",
      "            misc.forsale       0.70      0.65      0.68       390\n",
      "               rec.autos       0.65      0.61      0.63       396\n",
      "         rec.motorcycles       0.73      0.63      0.67       398\n",
      "      rec.sport.baseball       0.85      0.76      0.80       397\n",
      "        rec.sport.hockey       0.57      0.89      0.69       399\n",
      "               sci.crypt       0.72      0.66      0.69       396\n",
      "         sci.electronics       0.56      0.45      0.50       393\n",
      "                 sci.med       0.74      0.62      0.67       396\n",
      "               sci.space       0.66      0.67      0.66       394\n",
      "  soc.religion.christian       0.52      0.74      0.61       398\n",
      "      talk.politics.guns       0.49      0.68      0.57       364\n",
      "   talk.politics.mideast       0.71      0.78      0.75       376\n",
      "      talk.politics.misc       0.46      0.35      0.40       310\n",
      "      talk.religion.misc       0.30      0.21      0.25       251\n",
      "\n",
      "                accuracy                           0.61      7532\n",
      "               macro avg       0.60      0.60      0.59      7532\n",
      "            weighted avg       0.61      0.61      0.60      7532\n",
      "\n",
      "0.6087360594795539\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original NB classifier using test data set\n",
    "nb_clf = MultinomialNB(alpha=1e-2).fit(X_l, y_l)\n",
    "pred = nb_clf.predict(test_vec)\n",
    "print(metrics.classification_report(test_Xy.target, pred, target_names=test_Xy.target_names))\n",
    "# pprint(metrics.confusion_matrix(test_Xy.target, pred))\n",
    "print(metrics.accuracy_score(test_Xy.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial expected log likelihood = -5602758.945\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -5192818.083\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -5191720.451\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -5191615.689\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -5191615.689\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.65      0.26      0.37       319\n",
      "           comp.graphics       0.51      0.67      0.58       389\n",
      " comp.os.ms-windows.misc       0.63      0.45      0.53       394\n",
      "comp.sys.ibm.pc.hardware       0.54      0.68      0.60       392\n",
      "   comp.sys.mac.hardware       0.68      0.56      0.62       385\n",
      "          comp.windows.x       0.72      0.73      0.72       395\n",
      "            misc.forsale       0.79      0.67      0.72       390\n",
      "               rec.autos       0.73      0.64      0.68       396\n",
      "         rec.motorcycles       0.69      0.68      0.69       398\n",
      "      rec.sport.baseball       0.91      0.75      0.82       397\n",
      "        rec.sport.hockey       0.56      0.91      0.69       399\n",
      "               sci.crypt       0.79      0.69      0.73       396\n",
      "         sci.electronics       0.67      0.48      0.56       393\n",
      "                 sci.med       0.81      0.71      0.76       396\n",
      "               sci.space       0.73      0.74      0.73       394\n",
      "  soc.religion.christian       0.46      0.86      0.60       398\n",
      "      talk.politics.guns       0.48      0.75      0.58       364\n",
      "   talk.politics.mideast       0.78      0.77      0.77       376\n",
      "      talk.politics.misc       0.54      0.39      0.46       310\n",
      "      talk.religion.misc       0.47      0.10      0.16       251\n",
      "\n",
      "                accuracy                           0.64      7532\n",
      "               macro avg       0.66      0.62      0.62      7532\n",
      "            weighted avg       0.66      0.64      0.63      7532\n",
      "\n",
      "0.6404673393520978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate semi-supervised EM NB classifier using test data set\n",
    "em_nb_clf = Semi_EM_MultinomialNB(alpha=1e-2).fit(X_l, y_l, X_u)\n",
    "pred = em_nb_clf.predict(test_vec)\n",
    "print(metrics.classification_report(test_Xy.target, pred, target_names=test_Xy.target_names))\n",
    "# pprint(metrics.confusion_matrix(test_Xy.target, pred))\n",
    "print(metrics.accuracy_score(test_Xy.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most informative features \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud \n",
    "%matplotlib inline\n",
    "def show_topK(classifier, vectorizer, categories, K=10):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "#     fig, axes = plt.subplots(figsize=(50, 40), nrows=5, ncols=4)\n",
    "    for i, category in enumerate(categories):\n",
    "        topK = np.argsort(classifier.coef_[i])[-K:]\n",
    "        text = \",\".join(feature_names[topK])\n",
    "        print(\"%s: %s\" % (category, text))\n",
    "#         wordcloud = WordCloud().generate(text)\n",
    "#         axes[i//4, i%4].imshow(wordcloud, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "#         axes[i//4, i%4].axis(\"off\")\n",
    "#         axes[i//4, i%4].set_title(category, fontweight=\"bold\", size=24)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: behavior,wa,delet,make,think,say,peopl,moral,thi,god\n",
      "comp.graphics: softwar,packag,imag,program,ani,thi,thank,file,use,graphic\n",
      "comp.os.ms-windows.misc: card,win,version,thi,problem,font,driver,use,file,window\n",
      "comp.sys.ibm.pc.hardware: control,ide,thi,disk,irq,scsi,card,use,monitor,drive\n",
      "comp.sys.mac.hardware: fpu,card,doe,drive,appl,thi,use,lc,simm,mac\n",
      "comp.windows.x: program,display,file,client,server,ani,use,thi,widget,window\n",
      "misc.forsale: mail,condit,manual,price,includ,new,ship,offer,sell,sale\n",
      "rec.autos: new,drive,like,wa,dealer,thi,auto,ford,engin,car\n",
      "rec.motorcycles: ani,rear,thi,shaft,wa,dod,dog,motorcycl,ride,bike\n",
      "rec.sport.baseball: basebal,player,hi,team,win,brave,cub,year,wa,game\n",
      "rec.sport.hockey: win,pittsburgh,playoff,wa,year,player,hockey,play,team,game\n",
      "sci.crypt: escrow,govern,nsa,use,clipper,secur,chip,thi,encrypt,key\n",
      "sci.electronics: amp,detector,ani,current,power,radar,line,thi,signal,use\n",
      "sci.med: dsl,shame surrend,cadr dsl,edu shame,pitt,gordon bank,pitt edu,geb,gordon,thi\n",
      "sci.space: like,year,moon,fund,thi,nasa,wa,orbit,launch,space\n",
      "soc.religion.christian: think,cathol,hi,sin,christian,thi,wa,jesu,church,god\n",
      "talk.politics.guns: ban,govern,make,right,firearm,weapon,peopl,wa,thi,gun\n",
      "talk.politics.mideast: thi,peopl,jew,turkish,kill,wa,isra,arab,armenian,israel\n",
      "talk.politics.misc: law,tax,make,homosexu,state,insur,wa,peopl,govern,thi\n",
      "talk.religion.misc: order,tyre,think,peopl,thi,jesu,wa,say,christian,god\n"
     ]
    }
   ],
   "source": [
    "show_topK(nb_clf, vectorizer, train_Xy.target_names, K=10) # keywords for each class by original NB classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: wa,object,make,atheist,peopl,think,say,god,thi,moral\n",
      "comp.graphics: know,anyon,program,ani,use,thi,imag,file,thank,graphic\n",
      "comp.os.ms-windows.misc: card,problem,thank,program,font,thi,use,driver,file,window\n",
      "comp.sys.ibm.pc.hardware: ide,problem,thi,disk,mb,bu,use,scsi,card,drive\n",
      "comp.sys.mac.hardware: know,ha,thank,card,use,drive,thi,simm,appl,mac\n",
      "comp.windows.x: display,ani,file,program,motif,widget,server,use,thi,window\n",
      "misc.forsale: email,price,condit,pleas,new,includ,ship,sell,offer,sale\n",
      "rec.autos: ani,new,dealer,buy,like,drive,engin,thi,wa,car\n",
      "rec.motorcycles: dog,rider,helmet,like,dod,thi,motorcycl,wa,ride,bike\n",
      "rec.sport.baseball: thi,win,hit,player,pitch,wa,hi,team,year,game\n",
      "rec.sport.hockey: nhl,win,playoff,season,player,hockey,wa,play,team,game\n",
      "sci.crypt: escrow,nsa,govern,secur,use,clipper,chip,thi,encrypt,key\n",
      "sci.electronics: like,wire,good,ani,amp,voltag,power,circuit,thi,use\n",
      "sci.med: gordon,geb,gordon bank,doctor,wa,food,patient,diseas,msg,thi\n",
      "sci.space: satellit,think,earth,moon,thi,launch,nasa,wa,orbit,space\n",
      "soc.religion.christian: bibl,believ,church,say,hi,jesu,wa,thi,christian,god\n",
      "talk.politics.guns: think,law,firearm,fbi,right,weapon,peopl,wa,thi,gun\n",
      "talk.politics.mideast: turkish,say,peopl,thi,jew,arab,wa,isra,armenian,israel\n",
      "talk.politics.misc: clinton,make,presid,say,homosexu,govern,wa,peopl,tax,thi\n",
      "talk.religion.misc: hi,bibl,peopl,think,jesu,wa,thi,say,christian,god\n"
     ]
    }
   ],
   "source": [
    "show_topK(em_nb_clf, vectorizer, train_Xy.target_names, K=10) # keywords for each class by semisupervised EM NB classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

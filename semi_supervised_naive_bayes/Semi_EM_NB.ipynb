{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.linalg import get_blas_funcs\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "\n",
    "class Semi_EM_MultinomialNB():\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for multinomial models for semi-supervised learning.\n",
    "    \n",
    "    Use both labeled and unlabeled data to train NB classifier, update parameters\n",
    "    using unlabeled data, and all data to evaluate performance of classifier. Optimize\n",
    "    classifier using Expectation-Maximization algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None, max_iter=30, tol=1e-6, print_log_lkh=True):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "        self.clf = MultinomialNB(alpha=self.alpha, fit_prior=self.fit_prior, class_prior=self.class_prior)\n",
    "        self.log_lkh = -np.inf # log likelihood\n",
    "        self.max_iter = max_iter # max number of EM iterations\n",
    "        self.tol = tol # tolerance of log likelihood increment\n",
    "        self.feature_log_prob_ = np.array([]) # Empirical log probability of features given a class, P(x_i|y).\n",
    "        self.coef_ = np.array([]) # Mirrors feature_log_prob_ for interpreting MultinomialNB as a linear model.\n",
    "        self.print_log_lkh = print_log_lkh # if True, print log likelihood during EM iterations\n",
    "\n",
    "    def fit(self, X_l, y_l, X_u):\n",
    "        \"\"\"\n",
    "        Initialize the parameter using labeled data only.\n",
    "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
    "        \"\"\"\n",
    "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
    "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
    "        # initialization (n_docs = n_ul_docs)\n",
    "        clf = deepcopy(self.clf)# build new copy of classifier\n",
    "        clf.fit(X_l, y_l) # use labeled data only to initialize classifier parameters\n",
    "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
    "        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
    "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
    "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) \n",
    "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
    "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
    "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "        self.clf = deepcopy(clf)\n",
    "        self.log_lkh = expectation\n",
    "        if self.print_log_lkh:\n",
    "            print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
    "        # Loop until log likelihood does not improve\n",
    "        iter_count = 0 # count EM iteration\n",
    "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
    "        # while (iter_count<self.max_iter):\n",
    "            iter_count += 1\n",
    "            if self.print_log_lkh:\n",
    "                print(\"EM iteration #%d\" % iter_count) # debug\n",
    "            # E-step: Estimate class membership of unlabeled documents\n",
    "            y_u = clf.predict(X_u)\n",
    "            # M-step: Re-estimate classifier parameters\n",
    "            X = vstack([X_l, X_u])\n",
    "            y = np.concatenate((y_l, y_u), axis=0)\n",
    "            clf.fit(X, y)\n",
    "            # check convergence: update log likelihood\n",
    "            p_c_d = clf.predict_proba(X_u)\n",
    "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "            b_w_d = (X_u > 0) # words in each document\n",
    "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
    "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) \n",
    "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
    "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "            if self.print_log_lkh:\n",
    "                print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
    "            if (expectation-self.log_lkh >= self.tol):\n",
    "                prev_log_lkh = self.log_lkh\n",
    "                self.log_lkh = expectation\n",
    "                self.clf = deepcopy(clf)\n",
    "            else:\n",
    "                break\n",
    "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
    "        self.coef_ = self.clf.coef_\n",
    "        return self\n",
    "\n",
    "    def fit_with_clustering(self, X_l, y_l, X_u, y_u=None):\n",
    "        \"\"\"\n",
    "        Initialize the parameter using both labeled and unlabeled data.\n",
    "        The classes of unlabeled data are assigned using similarity with labeled data.\n",
    "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
    "        The label propagation can only use dense matrix, so it is quite time consuming.\n",
    "        \"\"\"\n",
    "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
    "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
    "        # initialization (n_docs = n_ul_docs): \n",
    "        # assign class to unlabeled data using similarity with labeled data if y_u is not given\n",
    "        if (y_u==None):\n",
    "            label_prop_model = LabelSpreading(kernel='rbf', max_iter=5, n_jobs=-1)\n",
    "            y_u = np.array([-1.0]*n_ul_docs)\n",
    "            X = vstack([X_l, X_u])\n",
    "            y = np.concatenate((y_l, y_u), axis=0)\n",
    "            label_prop_model.fit(X.toarray(), y)\n",
    "            y_u = label_prop_model.predict(X_u.toarray())\n",
    "        y = np.concatenate((y_l, y_u), axis=0)\n",
    "        clf = deepcopy(self.clf)# build new copy of classifier\n",
    "        clf.fit(X, y) # use labeled data only to initialize classifier parameters\n",
    "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
    "        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
    "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
    "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) \n",
    "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
    "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
    "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "        self.clf = deepcopy(clf)\n",
    "        self.log_lkh = expectation\n",
    "        if self.print_log_lkh:\n",
    "            print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
    "        # Loop until log likelihood does not improve\n",
    "        iter_count = 0 # count EM iteration\n",
    "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
    "        # while (iter_count<self.max_iter):\n",
    "            iter_count += 1\n",
    "            if self.print_log_lkh:\n",
    "                print(\"EM iteration #%d\" % iter_count) # debug\n",
    "            # E-step: Estimate class membership of unlabeled documents\n",
    "            y_u = clf.predict(X_u)\n",
    "            # M-step: Re-estimate classifier parameters\n",
    "            X = vstack([X_l, X_u])\n",
    "            y = np.concatenate((y_l, y_u), axis=0)\n",
    "            clf.fit(X, y)\n",
    "            # check convergence: update log likelihood\n",
    "            p_c_d = clf.predict_proba(X_u)\n",
    "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "            b_w_d = (X_u > 0) # words in each document\n",
    "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
    "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) \n",
    "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
    "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "            if self.print_log_lkh:\n",
    "                print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
    "            if (expectation-self.log_lkh >= self.tol):\n",
    "                prev_log_lkh = self.log_lkh\n",
    "                self.log_lkh = expectation\n",
    "                self.clf = deepcopy(clf)\n",
    "            else:\n",
    "                break\n",
    "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
    "        self.coef_ = self.clf.coef_\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X_l, y_l, X_u=np.array([])):\n",
    "        \"\"\"\n",
    "        Initialize the parameter using labeled data only.\n",
    "        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.\n",
    "        This function can only be used after fit()\n",
    "        \"\"\"\n",
    "        n_ul_docs = X_u.shape[0] # number of unlabeled samples\n",
    "        n_l_docs = X_l.shape[0] # number of labeled samples\n",
    "        # initialization (n_docs = n_ul_docs)\n",
    "        clf = deepcopy(self.clf)# build new copy of classifier\n",
    "        clf.partial_fit(X_l, y_l) # use labeled data only to initialize classifier parameters\n",
    "        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration\n",
    "        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]\n",
    "        lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
    "        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) \n",
    "        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]\n",
    "        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]\n",
    "        expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "        self.clf = deepcopy(clf)\n",
    "        self.log_lkh = expectation\n",
    "        print(\"Initial expected log likelihood = %0.3f\\n\" % expectation)\n",
    "        # Loop until log likelihood does not improve\n",
    "        iter_count = 0 # count EM iteration\n",
    "        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):\n",
    "        # while (iter_count<self.max_iter):\n",
    "            iter_count += 1\n",
    "            print(\"EM iteration #%d\" % iter_count) # debug\n",
    "            # E-step: Estimate class membership of unlabeled documents\n",
    "            y_u = clf.predict(X_u)\n",
    "            # M-step: Re-estimate classifier parameters\n",
    "            X = vstack([X_l, X_u])\n",
    "            y = np.concatenate((y_l, y_u), axis=0)\n",
    "            clf.partial_fit(X, y)\n",
    "            # check convergence: update log likelihood\n",
    "            p_c_d = clf.predict_proba(X_u)\n",
    "            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]\n",
    "            b_w_d = (X_u > 0) # words in each document\n",
    "            lp_d_c = get_blas_funcs(\"gemm\", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]\n",
    "            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) \n",
    "            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]\n",
    "            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]\n",
    "            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]\n",
    "            expectation = get_blas_funcs(\"gemm\", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs\n",
    "            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() \n",
    "            print(\"\\tExpected log likelihood = %0.3f\" % expectation)\n",
    "            if (expectation-self.log_lkh >= self.tol):\n",
    "                prev_log_lkh = self.log_lkh\n",
    "                self.log_lkh = expectation\n",
    "                self.clf = deepcopy(clf)\n",
    "            else:\n",
    "                break\n",
    "        self.feature_log_prob_ = self.clf.feature_log_prob_\n",
    "        self.coef_ = self.clf.coef_\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.clf.score(X, y)\n",
    "\n",
    "    def get_params(deep=True):\n",
    "        return self.clf.get_params(deep)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.clf.__str__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

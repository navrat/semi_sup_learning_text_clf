{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navrat/semi_sup_learning_text_clf/blob/main/Semi_Supervised_self_training_LOGIT_and_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C_pmDcpcfyaM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as Data\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa3rhWCfRR_v",
        "outputId": "5743b124-b914-4fd1-cdf3-3c4048660c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.21.6\n",
            "1.0.2\n"
          ]
        }
      ],
      "source": [
        "print(np.__version__)\n",
        "import sklearn\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CINtLiQgPlK",
        "outputId": "a3622fd0-c45d-4768-882e-a84e7f3dbb28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount data from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ueImsZ8KKQKo"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade scikit-learn  # Do this to use sklearn SelfTrainingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IH0_ckVheY4I"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# np.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "feLsjKVQg-mk"
      },
      "outputs": [],
      "source": [
        "# %cd \"/content/drive/My Drive/Colab Notebooks/MixText-master\"\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9J5BgqtbO9Q8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wfoKofTO9DH",
        "outputId": "00364c1d-15b0-4158-9b30-284ce9f7f08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314 documents\n",
            "20 categories\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample dataset run \n",
        "data = fetch_20newsgroups(subset='train', categories=None)\n",
        "print(\"%d documents\" % len(data.filenames))\n",
        "print(\"%d categories\" % len(data.target_names))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hhD_iWF4SUS",
        "outputId": "a36d3909-84aa-4e3e-b3d0-8f1bae1762cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "type(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ociXTaQEYuJW",
        "outputId": "6cd60f95-1122-47aa-884f-9fa067e93a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.709\n",
            "Accuracy Score:  0.7090844821491693\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2569 new labels.\n",
            "End of iteration 2, added 761 new labels.\n",
            "End of iteration 3, added 211 new labels.\n",
            "End of iteration 4, added 69 new labels.\n",
            "End of iteration 5, added 35 new labels.\n",
            "End of iteration 6, added 17 new labels.\n",
            "End of iteration 7, added 14 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.796\n",
            "Accuracy Score:  0.7956875220926122\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2193 new labels.\n",
            "End of iteration 2, added 700 new labels.\n",
            "End of iteration 3, added 240 new labels.\n",
            "End of iteration 4, added 100 new labels.\n",
            "End of iteration 5, added 38 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 18 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.794\n",
            "Accuracy Score:  0.7939201131141747\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1881 new labels.\n",
            "End of iteration 2, added 637 new labels.\n",
            "End of iteration 3, added 237 new labels.\n",
            "End of iteration 4, added 80 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 19 new labels.\n",
            "End of iteration 8, added 14 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 13 new labels.\n",
            "Micro-averaged F1 score on test set: 0.796\n",
            "Accuracy Score:  0.7956875220926122\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1548 new labels.\n",
            "End of iteration 2, added 555 new labels.\n",
            "End of iteration 3, added 211 new labels.\n",
            "End of iteration 4, added 66 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 25 new labels.\n",
            "End of iteration 7, added 17 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.783\n",
            "Accuracy Score:  0.782608695652174\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1174 new labels.\n",
            "End of iteration 2, added 377 new labels.\n",
            "End of iteration 3, added 166 new labels.\n",
            "End of iteration 4, added 109 new labels.\n",
            "End of iteration 5, added 58 new labels.\n",
            "End of iteration 6, added 30 new labels.\n",
            "End of iteration 7, added 13 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.764\n",
            "Accuracy Score:  0.7638741604807352\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.807\n",
            "Accuracy Score:  0.806998939554613\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3109 new labels.\n",
            "End of iteration 2, added 456 new labels.\n",
            "End of iteration 3, added 99 new labels.\n",
            "End of iteration 4, added 45 new labels.\n",
            "End of iteration 5, added 19 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.848\n",
            "Accuracy Score:  0.8480028278543655\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2772 new labels.\n",
            "End of iteration 2, added 483 new labels.\n",
            "End of iteration 3, added 110 new labels.\n",
            "End of iteration 4, added 41 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 11 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.854\n",
            "Accuracy Score:  0.8536585365853658\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2411 new labels.\n",
            "End of iteration 2, added 453 new labels.\n",
            "End of iteration 3, added 149 new labels.\n",
            "End of iteration 4, added 43 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 13 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.849\n",
            "Accuracy Score:  0.8494167550371156\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2032 new labels.\n",
            "End of iteration 2, added 383 new labels.\n",
            "End of iteration 3, added 126 new labels.\n",
            "End of iteration 4, added 44 new labels.\n",
            "End of iteration 5, added 25 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.847\n",
            "Accuracy Score:  0.8472958642629904\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1545 new labels.\n",
            "End of iteration 2, added 309 new labels.\n",
            "End of iteration 3, added 86 new labels.\n",
            "End of iteration 4, added 42 new labels.\n",
            "End of iteration 5, added 16 new labels.\n",
            "End of iteration 6, added 16 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 9 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.835\n",
            "Accuracy Score:  0.8352774832096147\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.836\n",
            "Accuracy Score:  0.8363379285966772\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3327 new labels.\n",
            "End of iteration 2, added 327 new labels.\n",
            "End of iteration 3, added 64 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.8688582537999293\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3024 new labels.\n",
            "End of iteration 2, added 320 new labels.\n",
            "End of iteration 3, added 75 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 17 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.8695652173913043\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2699 new labels.\n",
            "End of iteration 2, added 324 new labels.\n",
            "End of iteration 3, added 88 new labels.\n",
            "End of iteration 4, added 26 new labels.\n",
            "End of iteration 5, added 13 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.868\n",
            "Accuracy Score:  0.8677978084128667\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2300 new labels.\n",
            "End of iteration 2, added 298 new labels.\n",
            "End of iteration 3, added 74 new labels.\n",
            "End of iteration 4, added 23 new labels.\n",
            "End of iteration 5, added 9 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 8 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.860\n",
            "Accuracy Score:  0.8600212089077413\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1750 new labels.\n",
            "End of iteration 2, added 255 new labels.\n",
            "End of iteration 3, added 70 new labels.\n",
            "End of iteration 4, added 19 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.856\n",
            "Accuracy Score:  0.856486390950866\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.858\n",
            "Accuracy Score:  0.8582537999293036\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3458 new labels.\n",
            "End of iteration 2, added 254 new labels.\n",
            "End of iteration 3, added 42 new labels.\n",
            "End of iteration 4, added 20 new labels.\n",
            "End of iteration 5, added 6 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.885\n",
            "Accuracy Score:  0.8851184164015553\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3175 new labels.\n",
            "End of iteration 2, added 259 new labels.\n",
            "End of iteration 3, added 49 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.883\n",
            "Accuracy Score:  0.8829975256274302\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2840 new labels.\n",
            "End of iteration 2, added 269 new labels.\n",
            "End of iteration 3, added 49 new labels.\n",
            "End of iteration 4, added 15 new labels.\n",
            "End of iteration 5, added 13 new labels.\n",
            "End of iteration 6, added 8 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 8 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.879\n",
            "Accuracy Score:  0.879462707670555\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2457 new labels.\n",
            "End of iteration 2, added 261 new labels.\n",
            "End of iteration 3, added 48 new labels.\n",
            "End of iteration 4, added 21 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.873\n",
            "Accuracy Score:  0.8727465535524921\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1879 new labels.\n",
            "End of iteration 2, added 200 new labels.\n",
            "End of iteration 3, added 41 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.8685047720042418\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.8699186991869918\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3547 new labels.\n",
            "End of iteration 2, added 208 new labels.\n",
            "End of iteration 3, added 45 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 4 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.895\n",
            "Accuracy Score:  0.8953693884764935\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3287 new labels.\n",
            "End of iteration 2, added 204 new labels.\n",
            "End of iteration 3, added 40 new labels.\n",
            "End of iteration 4, added 11 new labels.\n",
            "End of iteration 5, added 5 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.896\n",
            "Accuracy Score:  0.896429833863556\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2931 new labels.\n",
            "End of iteration 2, added 219 new labels.\n",
            "End of iteration 3, added 39 new labels.\n",
            "End of iteration 4, added 14 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 4 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 8 new labels.\n",
            "Micro-averaged F1 score on test set: 0.889\n",
            "Accuracy Score:  0.8893601979498056\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2543 new labels.\n",
            "End of iteration 2, added 207 new labels.\n",
            "End of iteration 3, added 44 new labels.\n",
            "End of iteration 4, added 14 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.888\n",
            "Accuracy Score:  0.887592788971368\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1926 new labels.\n",
            "End of iteration 2, added 169 new labels.\n",
            "End of iteration 3, added 42 new labels.\n",
            "End of iteration 4, added 18 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 11 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.883\n",
            "Accuracy Score:  0.8829975256274302\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample SGD - Log Loss(Logistic regression).\n",
        "if __name__ == \"__main__\":\n",
        "  # Parameters\n",
        "  sgd_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "  vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "      print(\"Supervised SGDClassifier on \"+str(n)+\"% of the training data:\")\n",
        "      \n",
        "      eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sgd_params), threshold = t, verbose=True)),\n",
        "        ])\n",
        "        eval_and_print_metrics(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GuaecprcLFwE"
      },
      "outputs": [],
      "source": [
        "# Creating the evaluation functiont to help generate metrics for both supervised and semi-supervised runs\n",
        "def eval_and_print_metrics_df(clf, X_train, y_train, X_test, y_test, thresh = None, kbest = None):\n",
        "\n",
        "    dict1 = {}\n",
        "\n",
        "\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "    dict1['Labeled'] = len(X_train) - sum(1 for x in y_train if x == -1)\n",
        "    dict1['UnLabeled'] = sum(1 for x in y_train if x == -1)\n",
        "\n",
        "\n",
        "    \n",
        "    # if sum(1 for x in y_train if x == -1) == 0:\n",
        "    #     dict1['type'] = 'Supervised'\n",
        "    # else:\n",
        "    #     dict1['type'] = 'Semi-Supervised'\n",
        "\n",
        "    dict1['Threshold'] = thresh\n",
        "    dict1['K-Best'] = kbest\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    dict1['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "    return dict1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting with NEWSGROUP dataset"
      ],
      "metadata": {
        "id": "22_AtjRwAjKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = fetch_20newsgroups(subset='train', categories=None)\n",
        "print(\"%d documents\" % len(data.filenames))\n",
        "print(\"%d categories\" % len(data.target_names))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX4WEffyAN9c",
        "outputId": "ab7eb955-5f62-4c15-95be-45943c7ea23f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314 documents\n",
            "20 categories\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.data[0:5]\n",
        "# Data column consists of the actual news article with headline as subject and body with a few extra identifiers that we ignore for now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmA9GFxIEgQ8",
        "outputId": "36d03f84-fd2e-43df-bba4-20f473969af2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
              " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\",\n",
              " 'From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\nSubject: PB questions...\\nOrganization: Purdue University Engineering Computer Network\\nDistribution: usa\\nLines: 36\\n\\nwell folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i\\'m in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni\\'m looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?\\n\\n* what\\'s the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i\\'ve only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering\\n---------------------------------------------------------------------------\\n\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\nNietzsche\\n',\n",
              " 'From: jgreen@amber (Joe Green)\\nSubject: Re: Weitek P9000 ?\\nOrganization: Harris Computer Systems Division\\nLines: 14\\nDistribution: world\\nNNTP-Posting-Host: amber.ssd.csd.harris.com\\nX-Newsreader: TIN [version 1.1 PL9]\\n\\nRobert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\\n> > Anyone know about the Weitek P9000 graphics chip?\\n> As far as the low-level stuff goes, it looks pretty nice.  It\\'s got this\\n> quadrilateral fill command that requires just the four points.\\n\\nDo you have Weitek\\'s address/phone number?  I\\'d like to get some information\\nabout this chip.\\n\\n--\\nJoe Green\\t\\t\\t\\tHarris Corporation\\njgreen@csd.harris.com\\t\\t\\tComputer Systems Division\\n\"The only thing that really scares me is a person with no sense of humor.\"\\n\\t\\t\\t\\t\\t\\t-- Jonathan Winters\\n',\n",
              " 'From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\nSubject: Re: Shuttle Launch Question\\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\nDistribution: sci\\nLines: 23\\n\\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear caution & warning memory.  Verify no unexpected\\n>>>errors. ...\".  I am wondering what an \"expected error\" might\\n>>>be.  Sorry if this is a really dumb question, but\\n> \\n> Parity errors in memory or previously known conditions that were waivered.\\n>    \"Yes that is an error, but we already knew about it\"\\n> I\\'d be curious as to what the real meaning of the quote is.\\n> \\n> tom\\n\\n\\nMy understanding is that the \\'expected errors\\' are basically\\nknown bugs in the warning system software - things are checked\\nthat don\\'t have the right values in yet because they aren\\'t\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n\\'ok, if you see a warning no. 213 before liftoff, ignore it\\'.\\n\\n - Jonathan\\n\\n\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.target)\n",
        "import collections\n",
        "collections.Counter(data.target)\n",
        "# Overall 20 labels i.e. 20 types of news classifications which look fairly balanced except in some cases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no8kZDolEgSM",
        "outputId": "247ec22e-cdab-4549-8125-9b69cac145de"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7 4 4 ... 3 1 8]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 480,\n",
              "         1: 584,\n",
              "         2: 591,\n",
              "         3: 590,\n",
              "         4: 578,\n",
              "         5: 593,\n",
              "         6: 585,\n",
              "         7: 594,\n",
              "         8: 598,\n",
              "         9: 597,\n",
              "         10: 600,\n",
              "         11: 595,\n",
              "         12: 591,\n",
              "         13: 594,\n",
              "         14: 593,\n",
              "         15: 599,\n",
              "         16: 546,\n",
              "         17: 564,\n",
              "         18: 465,\n",
              "         19: 377})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Running individual sklearn algorithms"
      ],
      "metadata": {
        "id": "F-B4hUWoAsl1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RryyG0TK6Ci",
        "outputId": "9c7ca77d-a93e-4cdb-f7b9-3bd3e6263915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.545\n",
            "Accuracy Score:  0.545068928950159\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.055\n",
            "Accuracy Score:  0.055496641922940966\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.055\n",
            "Accuracy Score:  0.05514316012725345\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.054\n",
            "Accuracy Score:  0.0544361965358784\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.054\n",
            "Accuracy Score:  0.05408271474019088\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.053\n",
            "Accuracy Score:  0.053375751148815834\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.706\n",
            "Accuracy Score:  0.7062566277836692\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.063\n",
            "Accuracy Score:  0.06327324142806645\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.061\n",
            "Accuracy Score:  0.06115235065394132\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.060\n",
            "Accuracy Score:  0.060445387062566275\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.060\n",
            "Accuracy Score:  0.05973842347119123\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.060\n",
            "Accuracy Score:  0.05973842347119123\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.742\n",
            "Accuracy Score:  0.7423117709437964\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.080\n",
            "Accuracy Score:  0.07953340402969247\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.072\n",
            "Accuracy Score:  0.07246376811594203\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.075\n",
            "Accuracy Score:  0.07458465889006716\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.074\n",
            "Accuracy Score:  0.0735242135030046\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.106\n",
            "Accuracy Score:  0.10604453870625663\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.771\n",
            "Accuracy Score:  0.7709437963944857\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.112\n",
            "Accuracy Score:  0.11170024743725698\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.096\n",
            "Accuracy Score:  0.09579356663131848\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.083\n",
            "Accuracy Score:  0.08271474019088017\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.078\n",
            "Accuracy Score:  0.07811947684694238\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.074\n",
            "Accuracy Score:  0.07423117709437964\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.799\n",
            "Accuracy Score:  0.7992223400494874\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1076 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1076 new labels.\n",
            "End of iteration 2, added 1076 new labels.\n",
            "End of iteration 3, added 1076 new labels.\n",
            "End of iteration 4, added 1076 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.143\n",
            "Accuracy Score:  0.14280664545775892\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  861 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 861 new labels.\n",
            "End of iteration 2, added 861 new labels.\n",
            "End of iteration 3, added 861 new labels.\n",
            "End of iteration 4, added 861 new labels.\n",
            "End of iteration 5, added 861 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.117\n",
            "Accuracy Score:  0.11664899257688228\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  717 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 717 new labels.\n",
            "End of iteration 2, added 717 new labels.\n",
            "End of iteration 3, added 717 new labels.\n",
            "End of iteration 4, added 717 new labels.\n",
            "End of iteration 5, added 717 new labels.\n",
            "End of iteration 6, added 717 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.105\n",
            "Accuracy Score:  0.10533757511488158\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  615 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 615 new labels.\n",
            "End of iteration 2, added 615 new labels.\n",
            "End of iteration 3, added 615 new labels.\n",
            "End of iteration 4, added 615 new labels.\n",
            "End of iteration 5, added 615 new labels.\n",
            "End of iteration 6, added 615 new labels.\n",
            "End of iteration 7, added 615 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.099\n",
            "Accuracy Score:  0.09932838458819371\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  538 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 538 new labels.\n",
            "End of iteration 2, added 538 new labels.\n",
            "End of iteration 3, added 538 new labels.\n",
            "End of iteration 4, added 538 new labels.\n",
            "End of iteration 5, added 538 new labels.\n",
            "End of iteration 6, added 538 new labels.\n",
            "End of iteration 7, added 538 new labels.\n",
            "End of iteration 8, added 538 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.094\n",
            "Accuracy Score:  0.09402615765288087\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_nb_ng = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "      print(\"Supervised MNB on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', MultinomialNB()),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_nb_ng = df_nb_ng.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for kb in kbest_list:\n",
        "        kbest = int(len(X_u50)/kb)\n",
        "        print(\"---------------------------------K-Best = \", kbest,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(MultinomialNB(), criterion = 'k_best', k_best = kbest, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test, thresh = None, kbest = kbest)\n",
        "        df_nb_ng = df_nb_ng.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "cG5CzJbvWELR",
        "outputId": "6d1fc184-e2fd-4106-c17f-ae4cc4322ffd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>K-Best</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>869.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.545069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.055497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.055143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.054436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.054083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.053376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.706257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.063273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.061152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.060445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.059738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.059738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.742312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.079533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.072464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.074585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.073524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.106045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.770944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.111700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.095794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.082715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.078119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.074231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.799222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>0.142807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>861.0</td>\n",
              "      <td>0.116649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>717.0</td>\n",
              "      <td>0.105338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>615.0</td>\n",
              "      <td>0.099328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>538.0</td>\n",
              "      <td>0.094026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Labeled  UnLabeled  Threshold  K-Best  Accuracy\n",
              "0     869.0        0.0        NaN     NaN  0.545069\n",
              "1     869.0     4307.0        NaN  1076.0  0.055497\n",
              "2     869.0     4307.0        NaN   861.0  0.055143\n",
              "3     869.0     4307.0        NaN   717.0  0.054436\n",
              "4     869.0     4307.0        NaN   615.0  0.054083\n",
              "5     869.0     4307.0        NaN   538.0  0.053376\n",
              "6    1695.0        0.0        NaN     NaN  0.706257\n",
              "7    1695.0     4307.0        NaN  1076.0  0.063273\n",
              "8    1695.0     4307.0        NaN   861.0  0.061152\n",
              "9    1695.0     4307.0        NaN   717.0  0.060445\n",
              "10   1695.0     4307.0        NaN   615.0  0.059738\n",
              "11   1695.0     4307.0        NaN   538.0  0.059738\n",
              "12   2541.0        0.0        NaN     NaN  0.742312\n",
              "13   2541.0     4307.0        NaN  1076.0  0.079533\n",
              "14   2541.0     4307.0        NaN   861.0  0.072464\n",
              "15   2541.0     4307.0        NaN   717.0  0.074585\n",
              "16   2541.0     4307.0        NaN   615.0  0.073524\n",
              "17   2541.0     4307.0        NaN   538.0  0.106045\n",
              "18   3349.0        0.0        NaN     NaN  0.770944\n",
              "19   3349.0     4307.0        NaN  1076.0  0.111700\n",
              "20   3349.0     4307.0        NaN   861.0  0.095794\n",
              "21   3349.0     4307.0        NaN   717.0  0.082715\n",
              "22   3349.0     4307.0        NaN   615.0  0.078119\n",
              "23   3349.0     4307.0        NaN   538.0  0.074231\n",
              "24   4178.0        0.0        NaN     NaN  0.799222\n",
              "25   4178.0     4307.0        NaN  1076.0  0.142807\n",
              "26   4178.0     4307.0        NaN   861.0  0.116649\n",
              "27   4178.0     4307.0        NaN   717.0  0.105338\n",
              "28   4178.0     4307.0        NaN   615.0  0.099328\n",
              "29   4178.0     4307.0        NaN   538.0  0.094026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df_nb_ng[['Labeled', 'UnLabeled', 'Threshold', 'K-Best', 'Accuracy']].sort_values(['Labeled', 'UnLabeled']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "BllNtJjrdu7b",
        "outputId": "42072664-09f9-434e-8343-21e229714347"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>K-Best</th>\n",
              "      <th>NaN</th>\n",
              "      <th>538.0</th>\n",
              "      <th>615.0</th>\n",
              "      <th>717.0</th>\n",
              "      <th>861.0</th>\n",
              "      <th>1076.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">869.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.545069</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.053376</td>\n",
              "      <td>0.054083</td>\n",
              "      <td>0.054436</td>\n",
              "      <td>0.055143</td>\n",
              "      <td>0.055497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1695.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.706257</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.059738</td>\n",
              "      <td>0.059738</td>\n",
              "      <td>0.060445</td>\n",
              "      <td>0.061152</td>\n",
              "      <td>0.063273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2541.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.742312</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.106045</td>\n",
              "      <td>0.073524</td>\n",
              "      <td>0.074585</td>\n",
              "      <td>0.072464</td>\n",
              "      <td>0.079533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">3349.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.770944</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.074231</td>\n",
              "      <td>0.078119</td>\n",
              "      <td>0.082715</td>\n",
              "      <td>0.095794</td>\n",
              "      <td>0.111700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">4178.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.799222</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.094026</td>\n",
              "      <td>0.099328</td>\n",
              "      <td>0.105338</td>\n",
              "      <td>0.116649</td>\n",
              "      <td>0.142807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "K-Best               NaN       538.0     615.0     717.0     861.0     1076.0\n",
              "Labeled UnLabeled                                                            \n",
              "869.0   0.0        0.545069       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.053376  0.054083  0.054436  0.055143  0.055497\n",
              "1695.0  0.0        0.706257       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.059738  0.059738  0.060445  0.061152  0.063273\n",
              "2541.0  0.0        0.742312       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.106045  0.073524  0.074585  0.072464  0.079533\n",
              "3349.0  0.0        0.770944       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.074231  0.078119  0.082715  0.095794  0.111700\n",
              "4178.0  0.0        0.799222       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.094026  0.099328  0.105338  0.116649  0.142807"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_nb_ng.pivot(index=['Labeled', 'UnLabeled'], columns='K-Best')['Accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vADSNzoftiZ4",
        "outputId": "0101ba78-c504-45be-a284-b389a5d26e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.654\n",
            "Accuracy Score:  0.6535878402262284\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 634 new labels.\n",
            "End of iteration 2, added 421 new labels.\n",
            "End of iteration 3, added 476 new labels.\n",
            "End of iteration 4, added 724 new labels.\n",
            "End of iteration 5, added 842 new labels.\n",
            "End of iteration 6, added 877 new labels.\n",
            "End of iteration 7, added 266 new labels.\n",
            "End of iteration 8, added 57 new labels.\n",
            "End of iteration 9, added 10 new labels.\n",
            "Micro-averaged F1 score on test set: 0.198\n",
            "Accuracy Score:  0.19759632378932485\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 301 new labels.\n",
            "End of iteration 2, added 182 new labels.\n",
            "End of iteration 3, added 99 new labels.\n",
            "End of iteration 4, added 192 new labels.\n",
            "End of iteration 5, added 430 new labels.\n",
            "End of iteration 6, added 564 new labels.\n",
            "End of iteration 7, added 842 new labels.\n",
            "End of iteration 8, added 1152 new labels.\n",
            "End of iteration 9, added 499 new labels.\n",
            "End of iteration 10, added 37 new labels.\n",
            "Micro-averaged F1 score on test set: 0.124\n",
            "Accuracy Score:  0.12371862849063273\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 154 new labels.\n",
            "End of iteration 2, added 98 new labels.\n",
            "End of iteration 3, added 32 new labels.\n",
            "End of iteration 4, added 25 new labels.\n",
            "End of iteration 5, added 28 new labels.\n",
            "End of iteration 6, added 27 new labels.\n",
            "End of iteration 7, added 41 new labels.\n",
            "End of iteration 8, added 50 new labels.\n",
            "End of iteration 9, added 91 new labels.\n",
            "End of iteration 10, added 159 new labels.\n",
            "Micro-averaged F1 score on test set: 0.254\n",
            "Accuracy Score:  0.25379992930364087\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 61 new labels.\n",
            "End of iteration 2, added 41 new labels.\n",
            "End of iteration 3, added 23 new labels.\n",
            "End of iteration 4, added 34 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 8 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.574\n",
            "Accuracy Score:  0.5740544361965358\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 8 new labels.\n",
            "End of iteration 2, added 20 new labels.\n",
            "End of iteration 3, added 21 new labels.\n",
            "End of iteration 4, added 10 new labels.\n",
            "End of iteration 5, added 2 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.646\n",
            "Accuracy Score:  0.6458112407211029\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.725\n",
            "Accuracy Score:  0.7253446447507953\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1147 new labels.\n",
            "End of iteration 2, added 405 new labels.\n",
            "End of iteration 3, added 217 new labels.\n",
            "End of iteration 4, added 197 new labels.\n",
            "End of iteration 5, added 237 new labels.\n",
            "End of iteration 6, added 303 new labels.\n",
            "End of iteration 7, added 289 new labels.\n",
            "End of iteration 8, added 263 new labels.\n",
            "End of iteration 9, added 277 new labels.\n",
            "End of iteration 10, added 268 new labels.\n",
            "Micro-averaged F1 score on test set: 0.275\n",
            "Accuracy Score:  0.2753623188405797\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 664 new labels.\n",
            "End of iteration 2, added 229 new labels.\n",
            "End of iteration 3, added 113 new labels.\n",
            "End of iteration 4, added 85 new labels.\n",
            "End of iteration 5, added 72 new labels.\n",
            "End of iteration 6, added 101 new labels.\n",
            "End of iteration 7, added 118 new labels.\n",
            "End of iteration 8, added 178 new labels.\n",
            "End of iteration 9, added 212 new labels.\n",
            "End of iteration 10, added 234 new labels.\n",
            "Micro-averaged F1 score on test set: 0.341\n",
            "Accuracy Score:  0.3411099328384588\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 351 new labels.\n",
            "End of iteration 2, added 144 new labels.\n",
            "End of iteration 3, added 56 new labels.\n",
            "End of iteration 4, added 39 new labels.\n",
            "End of iteration 5, added 30 new labels.\n",
            "End of iteration 6, added 18 new labels.\n",
            "End of iteration 7, added 15 new labels.\n",
            "End of iteration 8, added 19 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 9 new labels.\n",
            "Micro-averaged F1 score on test set: 0.654\n",
            "Accuracy Score:  0.6542948038176034\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 167 new labels.\n",
            "End of iteration 2, added 48 new labels.\n",
            "End of iteration 3, added 34 new labels.\n",
            "End of iteration 4, added 32 new labels.\n",
            "End of iteration 5, added 29 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 11 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 8 new labels.\n",
            "Micro-averaged F1 score on test set: 0.705\n",
            "Accuracy Score:  0.7051961823966065\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 81 new labels.\n",
            "End of iteration 2, added 31 new labels.\n",
            "End of iteration 3, added 11 new labels.\n",
            "End of iteration 4, added 5 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.726\n",
            "Accuracy Score:  0.7264050901378579\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.759\n",
            "Accuracy Score:  0.7589254153411099\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1511 new labels.\n",
            "End of iteration 2, added 355 new labels.\n",
            "End of iteration 3, added 194 new labels.\n",
            "End of iteration 4, added 118 new labels.\n",
            "End of iteration 5, added 80 new labels.\n",
            "End of iteration 6, added 67 new labels.\n",
            "End of iteration 7, added 77 new labels.\n",
            "End of iteration 8, added 87 new labels.\n",
            "End of iteration 9, added 128 new labels.\n",
            "End of iteration 10, added 128 new labels.\n",
            "Micro-averaged F1 score on test set: 0.578\n",
            "Accuracy Score:  0.5782962177447861\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 939 new labels.\n",
            "End of iteration 2, added 272 new labels.\n",
            "End of iteration 3, added 93 new labels.\n",
            "End of iteration 4, added 68 new labels.\n",
            "End of iteration 5, added 60 new labels.\n",
            "End of iteration 6, added 44 new labels.\n",
            "End of iteration 7, added 48 new labels.\n",
            "End of iteration 8, added 39 new labels.\n",
            "End of iteration 9, added 16 new labels.\n",
            "End of iteration 10, added 23 new labels.\n",
            "Micro-averaged F1 score on test set: 0.675\n",
            "Accuracy Score:  0.6747967479674797\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 494 new labels.\n",
            "End of iteration 2, added 168 new labels.\n",
            "End of iteration 3, added 103 new labels.\n",
            "End of iteration 4, added 59 new labels.\n",
            "End of iteration 5, added 40 new labels.\n",
            "End of iteration 6, added 25 new labels.\n",
            "End of iteration 7, added 16 new labels.\n",
            "End of iteration 8, added 20 new labels.\n",
            "End of iteration 9, added 14 new labels.\n",
            "End of iteration 10, added 8 new labels.\n",
            "Micro-averaged F1 score on test set: 0.723\n",
            "Accuracy Score:  0.7225167903852951\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 255 new labels.\n",
            "End of iteration 2, added 81 new labels.\n",
            "End of iteration 3, added 54 new labels.\n",
            "End of iteration 4, added 38 new labels.\n",
            "End of iteration 5, added 20 new labels.\n",
            "End of iteration 6, added 26 new labels.\n",
            "End of iteration 7, added 18 new labels.\n",
            "End of iteration 8, added 10 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.738\n",
            "Accuracy Score:  0.7384234711912336\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 118 new labels.\n",
            "End of iteration 2, added 32 new labels.\n",
            "End of iteration 3, added 34 new labels.\n",
            "End of iteration 4, added 23 new labels.\n",
            "End of iteration 5, added 5 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 10 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.748\n",
            "Accuracy Score:  0.7479674796747967\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.784\n",
            "Accuracy Score:  0.7836691410392365\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1714 new labels.\n",
            "End of iteration 2, added 357 new labels.\n",
            "End of iteration 3, added 217 new labels.\n",
            "End of iteration 4, added 71 new labels.\n",
            "End of iteration 5, added 49 new labels.\n",
            "End of iteration 6, added 43 new labels.\n",
            "End of iteration 7, added 43 new labels.\n",
            "End of iteration 8, added 31 new labels.\n",
            "End of iteration 9, added 35 new labels.\n",
            "End of iteration 10, added 42 new labels.\n",
            "Micro-averaged F1 score on test set: 0.729\n",
            "Accuracy Score:  0.7285259809119831\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1106 new labels.\n",
            "End of iteration 2, added 268 new labels.\n",
            "End of iteration 3, added 101 new labels.\n",
            "End of iteration 4, added 83 new labels.\n",
            "End of iteration 5, added 62 new labels.\n",
            "End of iteration 6, added 47 new labels.\n",
            "End of iteration 7, added 32 new labels.\n",
            "End of iteration 8, added 27 new labels.\n",
            "End of iteration 9, added 22 new labels.\n",
            "End of iteration 10, added 21 new labels.\n",
            "Micro-averaged F1 score on test set: 0.734\n",
            "Accuracy Score:  0.7341816896429834\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 693 new labels.\n",
            "End of iteration 2, added 172 new labels.\n",
            "End of iteration 3, added 58 new labels.\n",
            "End of iteration 4, added 61 new labels.\n",
            "End of iteration 5, added 45 new labels.\n",
            "End of iteration 6, added 31 new labels.\n",
            "End of iteration 7, added 20 new labels.\n",
            "End of iteration 8, added 14 new labels.\n",
            "End of iteration 9, added 18 new labels.\n",
            "End of iteration 10, added 19 new labels.\n",
            "Micro-averaged F1 score on test set: 0.744\n",
            "Accuracy Score:  0.744079179922234\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 347 new labels.\n",
            "End of iteration 2, added 137 new labels.\n",
            "End of iteration 3, added 52 new labels.\n",
            "End of iteration 4, added 27 new labels.\n",
            "End of iteration 5, added 28 new labels.\n",
            "End of iteration 6, added 16 new labels.\n",
            "End of iteration 7, added 21 new labels.\n",
            "End of iteration 8, added 17 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 12 new labels.\n",
            "Micro-averaged F1 score on test set: 0.766\n",
            "Accuracy Score:  0.7656415694591728\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 159 new labels.\n",
            "End of iteration 2, added 39 new labels.\n",
            "End of iteration 3, added 21 new labels.\n",
            "End of iteration 4, added 17 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.780\n",
            "Accuracy Score:  0.7801343230823613\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.791\n",
            "Accuracy Score:  0.7910922587486744\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1800 new labels.\n",
            "End of iteration 2, added 344 new labels.\n",
            "End of iteration 3, added 157 new labels.\n",
            "End of iteration 4, added 89 new labels.\n",
            "End of iteration 5, added 66 new labels.\n",
            "End of iteration 6, added 40 new labels.\n",
            "End of iteration 7, added 23 new labels.\n",
            "End of iteration 8, added 30 new labels.\n",
            "End of iteration 9, added 23 new labels.\n",
            "End of iteration 10, added 21 new labels.\n",
            "Micro-averaged F1 score on test set: 0.765\n",
            "Accuracy Score:  0.7652880876634853\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1208 new labels.\n",
            "End of iteration 2, added 309 new labels.\n",
            "End of iteration 3, added 110 new labels.\n",
            "End of iteration 4, added 82 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 46 new labels.\n",
            "End of iteration 7, added 28 new labels.\n",
            "End of iteration 8, added 19 new labels.\n",
            "End of iteration 9, added 17 new labels.\n",
            "End of iteration 10, added 16 new labels.\n",
            "Micro-averaged F1 score on test set: 0.773\n",
            "Accuracy Score:  0.7727112053729233\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 725 new labels.\n",
            "End of iteration 2, added 230 new labels.\n",
            "End of iteration 3, added 72 new labels.\n",
            "End of iteration 4, added 52 new labels.\n",
            "End of iteration 5, added 36 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 36 new labels.\n",
            "End of iteration 8, added 19 new labels.\n",
            "End of iteration 9, added 16 new labels.\n",
            "End of iteration 10, added 16 new labels.\n",
            "Micro-averaged F1 score on test set: 0.771\n",
            "Accuracy Score:  0.7709437963944857\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 359 new labels.\n",
            "End of iteration 2, added 104 new labels.\n",
            "End of iteration 3, added 74 new labels.\n",
            "End of iteration 4, added 63 new labels.\n",
            "End of iteration 5, added 34 new labels.\n",
            "End of iteration 6, added 20 new labels.\n",
            "End of iteration 7, added 23 new labels.\n",
            "End of iteration 8, added 19 new labels.\n",
            "End of iteration 9, added 13 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.790\n",
            "Accuracy Score:  0.7900318133616119\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 161 new labels.\n",
            "End of iteration 2, added 43 new labels.\n",
            "End of iteration 3, added 29 new labels.\n",
            "End of iteration 4, added 17 new labels.\n",
            "End of iteration 5, added 14 new labels.\n",
            "End of iteration 6, added 18 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 13 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 7 new labels.\n",
            "Micro-averaged F1 score on test set: 0.793\n",
            "Accuracy Score:  0.7932131495227995\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Random Forest for NG\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "rf_params = dict(n_estimators=100, random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_rf_ng = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "\n",
        "      print(\"Supervised NBClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', RandomForestClassifier(**rf_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_rf_ng = df_rf_ng.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(RandomForestClassifier(**rf_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test, thresh = t, kbest = None)\n",
        "        df_rf_ng = df_rf_ng.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "xBxHOzCxtiV1",
        "outputId": "e10ba35d-03f9-43dd-c0be-b7cf5db89c0f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">869.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.653588</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.197596</td>\n",
              "      <td>0.123719</td>\n",
              "      <td>0.253800</td>\n",
              "      <td>0.574054</td>\n",
              "      <td>0.645811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1695.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.725345</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.275362</td>\n",
              "      <td>0.341110</td>\n",
              "      <td>0.654295</td>\n",
              "      <td>0.705196</td>\n",
              "      <td>0.726405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2541.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.758925</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.578296</td>\n",
              "      <td>0.674797</td>\n",
              "      <td>0.722517</td>\n",
              "      <td>0.738423</td>\n",
              "      <td>0.747967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">3349.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.783669</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.728526</td>\n",
              "      <td>0.734182</td>\n",
              "      <td>0.744079</td>\n",
              "      <td>0.765642</td>\n",
              "      <td>0.780134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">4178.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.791092</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.765288</td>\n",
              "      <td>0.772711</td>\n",
              "      <td>0.770944</td>\n",
              "      <td>0.790032</td>\n",
              "      <td>0.793213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Threshold               NaN       0.4       0.5       0.6       0.7       0.8\n",
              "Labeled UnLabeled                                                            \n",
              "869.0   0.0        0.653588       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.197596  0.123719  0.253800  0.574054  0.645811\n",
              "1695.0  0.0        0.725345       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.275362  0.341110  0.654295  0.705196  0.726405\n",
              "2541.0  0.0        0.758925       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.578296  0.674797  0.722517  0.738423  0.747967\n",
              "3349.0  0.0        0.783669       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.728526  0.734182  0.744079  0.765642  0.780134\n",
              "4178.0  0.0        0.791092       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.765288  0.772711  0.770944  0.790032  0.793213"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_rf_ng.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpeyvVvZYHkM"
      },
      "outputs": [],
      "source": [
        "## SGD (Logistic Regression) experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG-pp2wyYHZ6",
        "outputId": "e457f7bd-059d-4037-c69e-1badab864927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.709\n",
            "Accuracy Score:  0.7090844821491693\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2569 new labels.\n",
            "End of iteration 2, added 761 new labels.\n",
            "End of iteration 3, added 211 new labels.\n",
            "End of iteration 4, added 69 new labels.\n",
            "End of iteration 5, added 35 new labels.\n",
            "End of iteration 6, added 17 new labels.\n",
            "End of iteration 7, added 14 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.796\n",
            "Accuracy Score:  0.7956875220926122\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2193 new labels.\n",
            "End of iteration 2, added 700 new labels.\n",
            "End of iteration 3, added 240 new labels.\n",
            "End of iteration 4, added 100 new labels.\n",
            "End of iteration 5, added 38 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 18 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.794\n",
            "Accuracy Score:  0.7939201131141747\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1881 new labels.\n",
            "End of iteration 2, added 637 new labels.\n",
            "End of iteration 3, added 237 new labels.\n",
            "End of iteration 4, added 80 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 24 new labels.\n",
            "End of iteration 7, added 19 new labels.\n",
            "End of iteration 8, added 14 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 13 new labels.\n",
            "Micro-averaged F1 score on test set: 0.796\n",
            "Accuracy Score:  0.7956875220926122\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1548 new labels.\n",
            "End of iteration 2, added 555 new labels.\n",
            "End of iteration 3, added 211 new labels.\n",
            "End of iteration 4, added 66 new labels.\n",
            "End of iteration 5, added 39 new labels.\n",
            "End of iteration 6, added 25 new labels.\n",
            "End of iteration 7, added 17 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.783\n",
            "Accuracy Score:  0.782608695652174\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1174 new labels.\n",
            "End of iteration 2, added 377 new labels.\n",
            "End of iteration 3, added 166 new labels.\n",
            "End of iteration 4, added 109 new labels.\n",
            "End of iteration 5, added 58 new labels.\n",
            "End of iteration 6, added 30 new labels.\n",
            "End of iteration 7, added 13 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.764\n",
            "Accuracy Score:  0.7638741604807352\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.807\n",
            "Accuracy Score:  0.806998939554613\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3109 new labels.\n",
            "End of iteration 2, added 456 new labels.\n",
            "End of iteration 3, added 99 new labels.\n",
            "End of iteration 4, added 45 new labels.\n",
            "End of iteration 5, added 19 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.848\n",
            "Accuracy Score:  0.8480028278543655\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2772 new labels.\n",
            "End of iteration 2, added 483 new labels.\n",
            "End of iteration 3, added 110 new labels.\n",
            "End of iteration 4, added 41 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 11 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.854\n",
            "Accuracy Score:  0.8536585365853658\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2411 new labels.\n",
            "End of iteration 2, added 453 new labels.\n",
            "End of iteration 3, added 149 new labels.\n",
            "End of iteration 4, added 43 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 13 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.849\n",
            "Accuracy Score:  0.8494167550371156\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2032 new labels.\n",
            "End of iteration 2, added 383 new labels.\n",
            "End of iteration 3, added 126 new labels.\n",
            "End of iteration 4, added 44 new labels.\n",
            "End of iteration 5, added 25 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.847\n",
            "Accuracy Score:  0.8472958642629904\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1545 new labels.\n",
            "End of iteration 2, added 309 new labels.\n",
            "End of iteration 3, added 86 new labels.\n",
            "End of iteration 4, added 42 new labels.\n",
            "End of iteration 5, added 16 new labels.\n",
            "End of iteration 6, added 16 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 9 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.835\n",
            "Accuracy Score:  0.8352774832096147\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.836\n",
            "Accuracy Score:  0.8363379285966772\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3327 new labels.\n",
            "End of iteration 2, added 327 new labels.\n",
            "End of iteration 3, added 64 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.8688582537999293\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3024 new labels.\n",
            "End of iteration 2, added 320 new labels.\n",
            "End of iteration 3, added 75 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 17 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.8695652173913043\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2699 new labels.\n",
            "End of iteration 2, added 324 new labels.\n",
            "End of iteration 3, added 88 new labels.\n",
            "End of iteration 4, added 26 new labels.\n",
            "End of iteration 5, added 13 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.868\n",
            "Accuracy Score:  0.8677978084128667\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2300 new labels.\n",
            "End of iteration 2, added 298 new labels.\n",
            "End of iteration 3, added 74 new labels.\n",
            "End of iteration 4, added 23 new labels.\n",
            "End of iteration 5, added 9 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 9 new labels.\n",
            "End of iteration 9, added 8 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.860\n",
            "Accuracy Score:  0.8600212089077413\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1750 new labels.\n",
            "End of iteration 2, added 255 new labels.\n",
            "End of iteration 3, added 70 new labels.\n",
            "End of iteration 4, added 19 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.856\n",
            "Accuracy Score:  0.856486390950866\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.858\n",
            "Accuracy Score:  0.8582537999293036\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3458 new labels.\n",
            "End of iteration 2, added 254 new labels.\n",
            "End of iteration 3, added 42 new labels.\n",
            "End of iteration 4, added 20 new labels.\n",
            "End of iteration 5, added 6 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.885\n",
            "Accuracy Score:  0.8851184164015553\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3175 new labels.\n",
            "End of iteration 2, added 259 new labels.\n",
            "End of iteration 3, added 49 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.883\n",
            "Accuracy Score:  0.8829975256274302\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2840 new labels.\n",
            "End of iteration 2, added 269 new labels.\n",
            "End of iteration 3, added 49 new labels.\n",
            "End of iteration 4, added 15 new labels.\n",
            "End of iteration 5, added 13 new labels.\n",
            "End of iteration 6, added 8 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 8 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.879\n",
            "Accuracy Score:  0.879462707670555\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2457 new labels.\n",
            "End of iteration 2, added 261 new labels.\n",
            "End of iteration 3, added 48 new labels.\n",
            "End of iteration 4, added 21 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.873\n",
            "Accuracy Score:  0.8727465535524921\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1879 new labels.\n",
            "End of iteration 2, added 200 new labels.\n",
            "End of iteration 3, added 41 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.8685047720042418\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.8699186991869918\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3547 new labels.\n",
            "End of iteration 2, added 208 new labels.\n",
            "End of iteration 3, added 45 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 4 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.895\n",
            "Accuracy Score:  0.8953693884764935\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3287 new labels.\n",
            "End of iteration 2, added 204 new labels.\n",
            "End of iteration 3, added 40 new labels.\n",
            "End of iteration 4, added 11 new labels.\n",
            "End of iteration 5, added 5 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.896\n",
            "Accuracy Score:  0.896429833863556\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2931 new labels.\n",
            "End of iteration 2, added 219 new labels.\n",
            "End of iteration 3, added 39 new labels.\n",
            "End of iteration 4, added 14 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 6 new labels.\n",
            "End of iteration 8, added 4 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 8 new labels.\n",
            "Micro-averaged F1 score on test set: 0.889\n",
            "Accuracy Score:  0.8893601979498056\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2543 new labels.\n",
            "End of iteration 2, added 207 new labels.\n",
            "End of iteration 3, added 44 new labels.\n",
            "End of iteration 4, added 14 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.888\n",
            "Accuracy Score:  0.887592788971368\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1926 new labels.\n",
            "End of iteration 2, added 169 new labels.\n",
            "End of iteration 3, added 42 new labels.\n",
            "End of iteration 4, added 18 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 11 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.883\n",
            "Accuracy Score:  0.8829975256274302\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Parameters\n",
        "sgd_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_sgd_ng = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "\n",
        "      print(\"Supervised SGDClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', SGDClassifier(**sgd_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_sgd_ng = df_sgd_ng.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sgd_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test, thresh = t, kbest = None)\n",
        "        df_sgd_ng = df_sgd_ng.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "RSemkCRFYHFT",
        "outputId": "60e6904a-e9af-4d13-cc8e-37c29625abb2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-89b6d415-0290-45ab-a462-fd4387206a6c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>K-Best</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>869.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.709084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.793920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.782609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>869.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.763874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.806999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.848003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.853659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.849417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.847296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1695.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.835277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.836338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.868858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.869565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.867798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.860021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2541.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.856486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.858254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.885118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.882998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.879463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.872747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3349.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.868505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.869919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.895369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.896430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.889360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.887593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>4307.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.882998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89b6d415-0290-45ab-a462-fd4387206a6c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89b6d415-0290-45ab-a462-fd4387206a6c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89b6d415-0290-45ab-a462-fd4387206a6c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Labeled  UnLabeled  Threshold  K-Best  Accuracy\n",
              "0     869.0        0.0        NaN     NaN  0.709084\n",
              "1     869.0     4307.0        0.4     NaN  0.795688\n",
              "2     869.0     4307.0        0.5     NaN  0.793920\n",
              "3     869.0     4307.0        0.6     NaN  0.795688\n",
              "4     869.0     4307.0        0.7     NaN  0.782609\n",
              "5     869.0     4307.0        0.8     NaN  0.763874\n",
              "6    1695.0        0.0        NaN     NaN  0.806999\n",
              "7    1695.0     4307.0        0.4     NaN  0.848003\n",
              "8    1695.0     4307.0        0.5     NaN  0.853659\n",
              "9    1695.0     4307.0        0.6     NaN  0.849417\n",
              "10   1695.0     4307.0        0.7     NaN  0.847296\n",
              "11   1695.0     4307.0        0.8     NaN  0.835277\n",
              "12   2541.0        0.0        NaN     NaN  0.836338\n",
              "13   2541.0     4307.0        0.4     NaN  0.868858\n",
              "14   2541.0     4307.0        0.5     NaN  0.869565\n",
              "15   2541.0     4307.0        0.6     NaN  0.867798\n",
              "16   2541.0     4307.0        0.7     NaN  0.860021\n",
              "17   2541.0     4307.0        0.8     NaN  0.856486\n",
              "18   3349.0        0.0        NaN     NaN  0.858254\n",
              "19   3349.0     4307.0        0.4     NaN  0.885118\n",
              "20   3349.0     4307.0        0.5     NaN  0.882998\n",
              "21   3349.0     4307.0        0.6     NaN  0.879463\n",
              "22   3349.0     4307.0        0.7     NaN  0.872747\n",
              "23   3349.0     4307.0        0.8     NaN  0.868505\n",
              "24   4178.0        0.0        NaN     NaN  0.869919\n",
              "25   4178.0     4307.0        0.4     NaN  0.895369\n",
              "26   4178.0     4307.0        0.5     NaN  0.896430\n",
              "27   4178.0     4307.0        0.6     NaN  0.889360\n",
              "28   4178.0     4307.0        0.7     NaN  0.887593\n",
              "29   4178.0     4307.0        0.8     NaN  0.882998"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sgd_ng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "X1M0UDWHlCry",
        "outputId": "9e12fca5-79d8-4bde-a0cf-6b905b4d02d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1b6eea15-f0e4-4305-a248-c94f5a8c8f49\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">869.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.709084</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795688</td>\n",
              "      <td>0.793920</td>\n",
              "      <td>0.795688</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.763874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1695.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.806999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.848003</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>0.849417</td>\n",
              "      <td>0.847296</td>\n",
              "      <td>0.835277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2541.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.836338</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.868858</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.867798</td>\n",
              "      <td>0.860021</td>\n",
              "      <td>0.856486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">3349.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.858254</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.885118</td>\n",
              "      <td>0.882998</td>\n",
              "      <td>0.879463</td>\n",
              "      <td>0.872747</td>\n",
              "      <td>0.868505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">4178.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.869919</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.895369</td>\n",
              "      <td>0.896430</td>\n",
              "      <td>0.889360</td>\n",
              "      <td>0.887593</td>\n",
              "      <td>0.882998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b6eea15-f0e4-4305-a248-c94f5a8c8f49')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b6eea15-f0e4-4305-a248-c94f5a8c8f49 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b6eea15-f0e4-4305-a248-c94f5a8c8f49');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Threshold               NaN       0.4       0.5       0.6       0.7       0.8\n",
              "Labeled UnLabeled                                                            \n",
              "869.0   0.0        0.709084       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.795688  0.793920  0.795688  0.782609  0.763874\n",
              "1695.0  0.0        0.806999       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.848003  0.853659  0.849417  0.847296  0.835277\n",
              "2541.0  0.0        0.836338       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.868858  0.869565  0.867798  0.860021  0.856486\n",
              "3349.0  0.0        0.858254       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.885118  0.882998  0.879463  0.872747  0.868505\n",
              "4178.0  0.0        0.869919       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.895369  0.896430  0.889360  0.887593  0.882998"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sgd_ng.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mNPVy6pYGpJ"
      },
      "source": [
        "Clear improvement in accuracies (pivot above) and micro-F1 (console output) can be seen upon using self training vs. training only on labeled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfoZCBxbzQ3t"
      },
      "outputs": [],
      "source": [
        "## MLP Classifier "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIautQx9zQ7l",
        "outputId": "f3f0c9da-df3c-4ad7-eb56-90775684c0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 10~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 10% of the training data:\n",
            "Number of training samples: 869\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.516\n",
            "Accuracy Score:  0.5164369034994698\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1676 new labels.\n",
            "End of iteration 2, added 1256 new labels.\n",
            "End of iteration 3, added 556 new labels.\n",
            "End of iteration 4, added 365 new labels.\n",
            "End of iteration 5, added 127 new labels.\n",
            "End of iteration 6, added 75 new labels.\n",
            "End of iteration 7, added 30 new labels.\n",
            "End of iteration 8, added 21 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 33 new labels.\n",
            "Micro-averaged F1 score on test set: 0.561\n",
            "Accuracy Score:  0.56062212796041\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1247 new labels.\n",
            "End of iteration 2, added 753 new labels.\n",
            "End of iteration 3, added 418 new labels.\n",
            "End of iteration 4, added 186 new labels.\n",
            "End of iteration 5, added 215 new labels.\n",
            "End of iteration 6, added 149 new labels.\n",
            "End of iteration 7, added 183 new labels.\n",
            "End of iteration 8, added 291 new labels.\n",
            "End of iteration 9, added 261 new labels.\n",
            "End of iteration 10, added 227 new labels.\n",
            "Micro-averaged F1 score on test set: 0.491\n",
            "Accuracy Score:  0.4913396960056557\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 976 new labels.\n",
            "End of iteration 2, added 545 new labels.\n",
            "End of iteration 3, added 387 new labels.\n",
            "End of iteration 4, added 173 new labels.\n",
            "End of iteration 5, added 162 new labels.\n",
            "End of iteration 6, added 95 new labels.\n",
            "End of iteration 7, added 89 new labels.\n",
            "End of iteration 8, added 101 new labels.\n",
            "End of iteration 9, added 151 new labels.\n",
            "End of iteration 10, added 211 new labels.\n",
            "Micro-averaged F1 score on test set: 0.503\n",
            "Accuracy Score:  0.503004595263344\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 801 new labels.\n",
            "End of iteration 2, added 394 new labels.\n",
            "End of iteration 3, added 248 new labels.\n",
            "End of iteration 4, added 180 new labels.\n",
            "End of iteration 5, added 112 new labels.\n",
            "End of iteration 6, added 86 new labels.\n",
            "End of iteration 7, added 45 new labels.\n",
            "End of iteration 8, added 34 new labels.\n",
            "End of iteration 9, added 21 new labels.\n",
            "End of iteration 10, added 23 new labels.\n",
            "Micro-averaged F1 score on test set: 0.527\n",
            "Accuracy Score:  0.5270413573700954\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5176\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 640 new labels.\n",
            "End of iteration 2, added 314 new labels.\n",
            "End of iteration 3, added 153 new labels.\n",
            "End of iteration 4, added 98 new labels.\n",
            "End of iteration 5, added 61 new labels.\n",
            "End of iteration 6, added 56 new labels.\n",
            "End of iteration 7, added 27 new labels.\n",
            "End of iteration 8, added 42 new labels.\n",
            "End of iteration 9, added 41 new labels.\n",
            "End of iteration 10, added 28 new labels.\n",
            "Micro-averaged F1 score on test set: 0.508\n",
            "Accuracy Score:  0.5075998586072817\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 20% of the training data:\n",
            "Number of training samples: 1695\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.645\n",
            "Accuracy Score:  0.6454577589254153\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2398 new labels.\n",
            "End of iteration 2, added 934 new labels.\n",
            "End of iteration 3, added 171 new labels.\n",
            "End of iteration 4, added 64 new labels.\n",
            "End of iteration 5, added 54 new labels.\n",
            "End of iteration 6, added 21 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 10 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.682\n",
            "Accuracy Score:  0.6822198656769176\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1985 new labels.\n",
            "End of iteration 2, added 629 new labels.\n",
            "End of iteration 3, added 436 new labels.\n",
            "End of iteration 4, added 159 new labels.\n",
            "End of iteration 5, added 63 new labels.\n",
            "End of iteration 6, added 13 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.685\n",
            "Accuracy Score:  0.6850477200424178\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1693 new labels.\n",
            "End of iteration 2, added 542 new labels.\n",
            "End of iteration 3, added 221 new labels.\n",
            "End of iteration 4, added 101 new labels.\n",
            "End of iteration 5, added 38 new labels.\n",
            "End of iteration 6, added 16 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 28 new labels.\n",
            "End of iteration 9, added 23 new labels.\n",
            "End of iteration 10, added 13 new labels.\n",
            "Micro-averaged F1 score on test set: 0.684\n",
            "Accuracy Score:  0.6843407564510428\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1444 new labels.\n",
            "End of iteration 2, added 539 new labels.\n",
            "End of iteration 3, added 167 new labels.\n",
            "End of iteration 4, added 94 new labels.\n",
            "End of iteration 5, added 66 new labels.\n",
            "End of iteration 6, added 32 new labels.\n",
            "End of iteration 7, added 32 new labels.\n",
            "End of iteration 8, added 23 new labels.\n",
            "End of iteration 9, added 16 new labels.\n",
            "End of iteration 10, added 44 new labels.\n",
            "Micro-averaged F1 score on test set: 0.665\n",
            "Accuracy Score:  0.6645457758925415\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6002\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1233 new labels.\n",
            "End of iteration 2, added 429 new labels.\n",
            "End of iteration 3, added 154 new labels.\n",
            "End of iteration 4, added 69 new labels.\n",
            "End of iteration 5, added 28 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 14 new labels.\n",
            "End of iteration 8, added 16 new labels.\n",
            "End of iteration 9, added 7 new labels.\n",
            "End of iteration 10, added 30 new labels.\n",
            "Micro-averaged F1 score on test set: 0.659\n",
            "Accuracy Score:  0.6592435489572287\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 30~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 30% of the training data:\n",
            "Number of training samples: 2541\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.715\n",
            "Accuracy Score:  0.7154471544715447\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 30% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2861 new labels.\n",
            "End of iteration 2, added 596 new labels.\n",
            "End of iteration 3, added 235 new labels.\n",
            "End of iteration 4, added 49 new labels.\n",
            "End of iteration 5, added 10 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "End of iteration 7, added 8 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.718\n",
            "Accuracy Score:  0.7175680452456699\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2495 new labels.\n",
            "End of iteration 2, added 589 new labels.\n",
            "End of iteration 3, added 317 new labels.\n",
            "End of iteration 4, added 98 new labels.\n",
            "End of iteration 5, added 32 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.737\n",
            "Accuracy Score:  0.7373630258041711\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2195 new labels.\n",
            "End of iteration 2, added 575 new labels.\n",
            "End of iteration 3, added 205 new labels.\n",
            "End of iteration 4, added 55 new labels.\n",
            "End of iteration 5, added 125 new labels.\n",
            "End of iteration 6, added 105 new labels.\n",
            "End of iteration 7, added 34 new labels.\n",
            "End of iteration 8, added 12 new labels.\n",
            "End of iteration 9, added 30 new labels.\n",
            "End of iteration 10, added 12 new labels.\n",
            "Micro-averaged F1 score on test set: 0.756\n",
            "Accuracy Score:  0.7557440791799223\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1938 new labels.\n",
            "End of iteration 2, added 494 new labels.\n",
            "End of iteration 3, added 137 new labels.\n",
            "End of iteration 4, added 53 new labels.\n",
            "End of iteration 5, added 61 new labels.\n",
            "End of iteration 6, added 55 new labels.\n",
            "End of iteration 7, added 34 new labels.\n",
            "End of iteration 8, added 18 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.723\n",
            "Accuracy Score:  0.7232237539766702\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6848\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1710 new labels.\n",
            "End of iteration 2, added 418 new labels.\n",
            "End of iteration 3, added 300 new labels.\n",
            "End of iteration 4, added 51 new labels.\n",
            "End of iteration 5, added 10 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.708\n",
            "Accuracy Score:  0.7076705549664192\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 40~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 40% of the training data:\n",
            "Number of training samples: 3349\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.748\n",
            "Accuracy Score:  0.7476139978791092\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 40% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3026 new labels.\n",
            "End of iteration 2, added 757 new labels.\n",
            "End of iteration 3, added 143 new labels.\n",
            "End of iteration 4, added 35 new labels.\n",
            "End of iteration 5, added 15 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.768\n",
            "Accuracy Score:  0.767762460233298\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2716 new labels.\n",
            "End of iteration 2, added 589 new labels.\n",
            "End of iteration 3, added 186 new labels.\n",
            "End of iteration 4, added 19 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.767\n",
            "Accuracy Score:  0.767055496641923\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2449 new labels.\n",
            "End of iteration 2, added 520 new labels.\n",
            "End of iteration 3, added 96 new labels.\n",
            "End of iteration 4, added 53 new labels.\n",
            "End of iteration 5, added 21 new labels.\n",
            "End of iteration 6, added 53 new labels.\n",
            "End of iteration 7, added 13 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.767\n",
            "Accuracy Score:  0.7674089784376105\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2216 new labels.\n",
            "End of iteration 2, added 488 new labels.\n",
            "End of iteration 3, added 72 new labels.\n",
            "End of iteration 4, added 27 new labels.\n",
            "End of iteration 5, added 35 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.763\n",
            "Accuracy Score:  0.7628137150936727\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7656\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 1989 new labels.\n",
            "End of iteration 2, added 446 new labels.\n",
            "End of iteration 3, added 121 new labels.\n",
            "End of iteration 4, added 58 new labels.\n",
            "End of iteration 5, added 17 new labels.\n",
            "End of iteration 6, added 63 new labels.\n",
            "End of iteration 7, added 25 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.770\n",
            "Accuracy Score:  0.7698833510074231\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = 50~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 50% of the training data:\n",
            "Number of training samples: 4178\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.762\n",
            "Accuracy Score:  0.7617532697066101\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 3298 new labels.\n",
            "End of iteration 2, added 346 new labels.\n",
            "End of iteration 3, added 69 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.789\n",
            "Accuracy Score:  0.7889713679745494\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2976 new labels.\n",
            "End of iteration 2, added 658 new labels.\n",
            "End of iteration 3, added 76 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.802\n",
            "Accuracy Score:  0.8020501944149876\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2713 new labels.\n",
            "End of iteration 2, added 363 new labels.\n",
            "End of iteration 3, added 83 new labels.\n",
            "End of iteration 4, added 20 new labels.\n",
            "End of iteration 5, added 34 new labels.\n",
            "End of iteration 6, added 10 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 168 new labels.\n",
            "End of iteration 10, added 18 new labels.\n",
            "Micro-averaged F1 score on test set: 0.793\n",
            "Accuracy Score:  0.7925061859314245\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2483 new labels.\n",
            "End of iteration 2, added 331 new labels.\n",
            "End of iteration 3, added 134 new labels.\n",
            "End of iteration 4, added 21 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.785\n",
            "Accuracy Score:  0.7847295864262991\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 8485\n",
            "Unlabeled samples in training set: 4307\n",
            "End of iteration 1, added 2250 new labels.\n",
            "End of iteration 2, added 445 new labels.\n",
            "End of iteration 3, added 47 new labels.\n",
            "End of iteration 4, added 9 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.786\n",
            "Accuracy Score:  0.7857900318133616\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# MLP for NG\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Parameters\n",
        "mlp_params = dict(hidden_layer_sizes=(100,), max_iter=100,activation = 'relu',solver='lbfgs',random_state=1,learning_rate_init=0.01,\n",
        "                  learning_rate='adaptive') # regularization is by default based on alpha =0.0001\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_ng = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 20, 30, 40, 50]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~NewsGroup DATA with percentage_labeled = \"+str(n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "      X, y = data.data, data.target\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "      # print(\"Supervised MLPClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "      unlabeled_mask = np.random.rand(len(y_train)) < 0.5\n",
        "      X_u50, y_u50 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_train, y_train, unlabeled_mask) if m)))\n",
        "      \n",
        "      y_u50 = np.array([-1 for i in y_u50])\n",
        "\n",
        "      X_50, y_50 = map(list, zip(*((x, y)\n",
        "                for x, y, m in zip(X_train, y_train, unlabeled_mask) if ~m)))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      percentage = 2*(n/100)\n",
        "      y_mask = np.random.rand(len(y_50)) < percentage\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = map(list, zip(*((x, y)\n",
        "                      for x, y, m in zip(X_50, y_50, y_mask) if m)))\n",
        "\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline      \n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('scale', StandardScaler(with_mean=False)),\n",
        "          ('clf', MLPClassifier(**mlp_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('scale', StandardScaler(with_mean=False)),\n",
        "            ('clf', SelfTrainingClassifier(MLPClassifier(**mlp_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_20+X_u50, np.concatenate((y_20, y_u50)), X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_ng = df_mlp_ng.append(temp, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VM-7NEqzQ_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "c029e466-9c80-477e-fdb9-0183ef6a07c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Threshold               NaN       0.4       0.5       0.6       0.7       0.8\n",
              "Labeled UnLabeled                                                            \n",
              "869.0   0.0        0.516437       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.560622  0.491340  0.503005  0.527041  0.507600\n",
              "1695.0  0.0        0.645458       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.682220  0.685048  0.684341  0.664546  0.659244\n",
              "2541.0  0.0        0.715447       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.717568  0.737363  0.755744  0.723224  0.707671\n",
              "3349.0  0.0        0.747614       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.767762  0.767055  0.767409  0.762814  0.769883\n",
              "4178.0  0.0        0.761753       NaN       NaN       NaN       NaN       NaN\n",
              "        4307.0          NaN  0.788971  0.802050  0.792506  0.784730  0.785790"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8006637a-f419-481e-b6ab-56b5d5d1d745\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">869.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.516437</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.560622</td>\n",
              "      <td>0.491340</td>\n",
              "      <td>0.503005</td>\n",
              "      <td>0.527041</td>\n",
              "      <td>0.507600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1695.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.645458</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.682220</td>\n",
              "      <td>0.685048</td>\n",
              "      <td>0.684341</td>\n",
              "      <td>0.664546</td>\n",
              "      <td>0.659244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2541.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.715447</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.717568</td>\n",
              "      <td>0.737363</td>\n",
              "      <td>0.755744</td>\n",
              "      <td>0.723224</td>\n",
              "      <td>0.707671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">3349.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.747614</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.767762</td>\n",
              "      <td>0.767055</td>\n",
              "      <td>0.767409</td>\n",
              "      <td>0.762814</td>\n",
              "      <td>0.769883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">4178.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.761753</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.788971</td>\n",
              "      <td>0.802050</td>\n",
              "      <td>0.792506</td>\n",
              "      <td>0.784730</td>\n",
              "      <td>0.785790</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8006637a-f419-481e-b6ab-56b5d5d1d745')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8006637a-f419-481e-b6ab-56b5d5d1d745 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8006637a-f419-481e-b6ab-56b5d5d1d745');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df_mlp_ng.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u51-CcA0kNi"
      },
      "source": [
        "**Even** for an MLP, the same situation holds true. Micro-F1 and Accuracy show improvements using selftrainingclassifier and usually the higher tresholds result in better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### End of algorithm-runs based off of self training classifiers on NEWSGROUP dataset."
      ],
      "metadata": {
        "id": "lmqHThXuBD5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Starting algorithm runs on IMDB dataset"
      ],
      "metadata": {
        "id": "vMeGO30qBMJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMUWFZuielQN",
        "outputId": "d7863b49-b0cb-4c7c-96ac-85baebf3e1ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Creating a simple train_test validation split and clean the dataset against punctuations, HTML tags, and URLs\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def train_val_split(labels, n_labeled_per_class, unlabeled_per_class, n_labels, seed=0):\n",
        "    \"\"\"Split the original training set into labeled training set, unlabeled training set, development set\n",
        "\n",
        "    Arguments:\n",
        "        labels {list} -- List of labeles for original training set\n",
        "        n_labeled_per_class {int} -- Number of labeled data per class\n",
        "        unlabeled_per_class {int} -- Number of unlabeled data per class\n",
        "        n_labels {int} -- The number of classes\n",
        "\n",
        "    Keyword Arguments:\n",
        "        seed {int} -- [random seed of np.shuffle] (default: {0})\n",
        "\n",
        "    Returns:\n",
        "        [list] -- idx for labeled training set, unlabeled training set, development set\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_idxs = []\n",
        "    train_unlabeled_idxs = []\n",
        "    val_idxs = []\n",
        "\n",
        "    for i in range(n_labels):\n",
        "        idxs = np.where(labels == i)[0]\n",
        "        np.random.shuffle(idxs)\n",
        "        if n_labels == 2:\n",
        "            # IMDB\n",
        "            \n",
        "            \n",
        "            \n",
        "            n_unlabeled_per_class = unlabeled_per_class   #10, 100, 500, 1000, 2500\n",
        "            train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            train_unlabeled_idxs.extend(idxs[500: 500 + n_unlabeled_per_class])\n",
        "            val_idxs.extend(idxs[-2000:])\n",
        "        \n",
        "        \n",
        "        \n",
        "            # train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            # train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            # train_unlabeled_idxs.extend(\n",
        "            #     idxs[500: 500 + 5000])\n",
        "            # val_idxs.extend(idxs[-2000:])\n",
        "            \n",
        "\n",
        "    return train_labeled_idxs, train_unlabeled_idxs, val_idxs\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "#Removes Punctuations\n",
        "def remove_punctuations(data):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    data=punct_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes HTML syntaxes\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes URL data\n",
        "def remove_url(data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def clean(train_df):\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_html(z))\n",
        "    train_df['review']=train_df['review'].apply(lambda z: remove_url(z))\n",
        "    # train_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\n",
        "    \n",
        "    train_df['review']=train_df['review'].apply(lambda z: word_tokenize(z))\n",
        "    \n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # train_df['review']=train_df['review'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "    train_df['review']=train_df['review'].apply(lambda x: ' '.join(x))\n",
        "    \n",
        "    return train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Starting by reading the IMDB dataset and visualizing the top 5 rows"
      ],
      "metadata": {
        "id": "ZwAYtN4kD82o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "print(\"Read data successful\", train_df.shape, test_df.shape)\n",
        "print(train_df.head(5))\n",
        "test_df.head()\n",
        "# Two columns per dataset - review and sentiment (1 is a positive sentiment and 0 is a negative sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "Iy8VAU2ODSsS",
        "outputId": "e35fc546-0ce2-4b6d-8cd5-bd54c4964f49"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read data successful (37500, 2) (12500, 2)\n",
            "                                                   0  1\n",
            "0  I figure this to be an \"alternate reality\" tee...  0\n",
            "1  This is the kind of movie that wants to be goo...  0\n",
            "2  This was by far the worst movie I've ever seen...  0\n",
            "3  Awful, awful, awful...<br /><br />I loved the ...  0\n",
            "4  Fragile Carne, just before his great period. A...  1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   0  1\n",
              "0  I really liked this Summerslam due to the look...  1\n",
              "1  Not many television shows appeal to quite as m...  1\n",
              "2  The film quickly gets to a major chase scene w...  0\n",
              "3  Jane Austen would definitely approve of this o...  1\n",
              "4  Expectations were somewhat high for me when I ...  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-339fc7e8-b937-4ba6-bf61-70997d706f61\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I really liked this Summerslam due to the look...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not many television shows appeal to quite as m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The film quickly gets to a major chase scene w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jane Austen would definitely approve of this o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Expectations were somewhat high for me when I ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-339fc7e8-b937-4ba6-bf61-70997d706f61')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-339fc7e8-b937-4ba6-bf61-70997d706f61 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-339fc7e8-b937-4ba6-bf61-70997d706f61');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BJ8OCqGFirni"
      },
      "outputs": [],
      "source": [
        "# Sample Run to see if everything runs fine\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Parameters\n",
        "sgd_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "# Supervised Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(**sgd_params)),\n",
        "])\n",
        "\n",
        "# SelfTraining Pipeline\n",
        "st_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(**vectorizer_params)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SelfTrainingClassifier(SGDClassifier(**sgd_params), verbose=True)),\n",
        "])\n",
        "\n",
        "\n",
        "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # if -1 in y_train:\n",
        "    #   print(\"Y-PRED-PROBA\", clf.predict_proba(X_train))\n",
        "\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "def eval_and_print_metrics_df(clf, X_train, y_train, X_test, y_test, thresh = None, kbest = None):\n",
        "\n",
        "    dict1 = {}\n",
        "\n",
        "\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    \n",
        "    dict1['Labeled'] = len(X_train) - sum(1 for x in y_train if x == -1)\n",
        "    dict1['UnLabeled'] = sum(1 for x in y_train if x == -1)\n",
        "\n",
        "\n",
        "    \n",
        "    # if sum(1 for x in y_train if x == -1) == 0:\n",
        "    #     dict1['type'] = 'Supervised'\n",
        "    # else:\n",
        "    #     dict1['type'] = 'Semi-Supervised'\n",
        "\n",
        "    dict1['Threshold'] = thresh\n",
        "    dict1['K-Best'] = kbest\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    dict1['Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "\n",
        "    return dict1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is55ANYy3DLW",
        "outputId": "c413e54a-0b85-4940-90dd-7c279d06f696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on  20  of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.565\n",
            "Accuracy Score:  0.56504\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.68      0.61      6157\n",
            "           1       0.59      0.45      0.51      6343\n",
            "\n",
            "    accuracy                           0.57     12500\n",
            "   macro avg       0.57      0.57      0.56     12500\n",
            "weighted avg       0.57      0.57      0.56     12500\n",
            "\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on  20  of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.508\n",
            "Accuracy Score:  0.50816\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67      6157\n",
            "           1       0.92      0.03      0.06      6343\n",
            "\n",
            "    accuracy                           0.51     12500\n",
            "   macro avg       0.71      0.52      0.37     12500\n",
            "weighted avg       0.71      0.51      0.36     12500\n",
            "\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2286 new labels.\n",
            "End of iteration 2, added 2714 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      1.00      0.66      6157\n",
            "           1       0.00      0.00      0.00      6343\n",
            "\n",
            "    accuracy                           0.49     12500\n",
            "   macro avg       0.25      0.50      0.33     12500\n",
            "weighted avg       0.24      0.49      0.33     12500\n",
            "\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised SGDClassifier on  400  of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.772\n",
            "Accuracy Score:  0.77224\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.76      0.77      6157\n",
            "           1       0.77      0.79      0.78      6343\n",
            "\n",
            "    accuracy                           0.77     12500\n",
            "   macro avg       0.77      0.77      0.77     12500\n",
            "weighted avg       0.77      0.77      0.77     12500\n",
            "\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on  400  of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.805\n",
            "Accuracy Score:  0.80544\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.79      0.80      6157\n",
            "           1       0.80      0.82      0.81      6343\n",
            "\n",
            "    accuracy                           0.81     12500\n",
            "   macro avg       0.81      0.81      0.81     12500\n",
            "weighted avg       0.81      0.81      0.81     12500\n",
            "\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3334 new labels.\n",
            "End of iteration 2, added 903 new labels.\n",
            "End of iteration 3, added 176 new labels.\n",
            "End of iteration 4, added 64 new labels.\n",
            "End of iteration 5, added 16 new labels.\n",
            "End of iteration 6, added 10 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "End of iteration 8, added 10 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.808\n",
            "Accuracy Score:  0.80784\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.77      0.80      6157\n",
            "           1       0.79      0.85      0.82      6343\n",
            "\n",
            "    accuracy                           0.81     12500\n",
            "   macro avg       0.81      0.81      0.81     12500\n",
            "weighted avg       0.81      0.81      0.81     12500\n",
            "\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Run to see if everything runs fine\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # n_list = [10, 200, 500]\n",
        "    n_list = [10, 200]\n",
        "    threshold=[0.4, 0.7]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 2500\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised SGDClassifier on \", 2*n,\" of the training data:\")\n",
        "      eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # y_train[~y_mask] = -1\n",
        "      print(\"SelfTrainingClassifier on \", 2*n,\" of the training data (rest \"\n",
        "          \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "        # SelfTraining Pipeline\n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sgd_params), threshold = t, verbose=True)),\n",
        "        ])\n",
        "\n",
        "        eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything looks fine. Continuing with individual algoirithms"
      ],
      "metadata": {
        "id": "no9BLKzuG1Iz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding an additional layer to the experiment- Considering 2 volumes of unlabeled data\n",
        "- 2000 (1000 observations per class passed as unlabeled)\n",
        "- 5000 (2500 observations per class passed as unlabeled)\n"
      ],
      "metadata": {
        "id": "wwVfuyvcHQOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ85gvK_O--y",
        "outputId": "437e0292-c23f-43e9-aa12-56a65d0cd5c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.590\n",
            "Accuracy Score:  0.59016\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.685\n",
            "Accuracy Score:  0.68456\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.795\n",
            "Accuracy Score:  0.79504\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.838\n",
            "Accuracy Score:  0.83752\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.858\n",
            "Accuracy Score:  0.85776\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  500 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 500 new labels.\n",
            "End of iteration 2, added 500 new labels.\n",
            "End of iteration 3, added 500 new labels.\n",
            "End of iteration 4, added 500 new labels.\n",
            "Micro-averaged F1 score on test set: 0.495\n",
            "Accuracy Score:  0.49504\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  400 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 400 new labels.\n",
            "End of iteration 2, added 400 new labels.\n",
            "End of iteration 3, added 400 new labels.\n",
            "End of iteration 4, added 400 new labels.\n",
            "End of iteration 5, added 400 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.4928\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  333 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 333 new labels.\n",
            "End of iteration 2, added 333 new labels.\n",
            "End of iteration 3, added 333 new labels.\n",
            "End of iteration 4, added 333 new labels.\n",
            "End of iteration 5, added 333 new labels.\n",
            "End of iteration 6, added 333 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  285 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 285 new labels.\n",
            "End of iteration 2, added 285 new labels.\n",
            "End of iteration 3, added 285 new labels.\n",
            "End of iteration 4, added 285 new labels.\n",
            "End of iteration 5, added 285 new labels.\n",
            "End of iteration 6, added 285 new labels.\n",
            "End of iteration 7, added 285 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  250 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 250 new labels.\n",
            "End of iteration 2, added 250 new labels.\n",
            "End of iteration 3, added 250 new labels.\n",
            "End of iteration 4, added 250 new labels.\n",
            "End of iteration 5, added 250 new labels.\n",
            "End of iteration 6, added 250 new labels.\n",
            "End of iteration 7, added 250 new labels.\n",
            "End of iteration 8, added 250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes for IMDB with 2k unlabeled observations\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_nb_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised NBClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', MultinomialNB()),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_nb_imdb_2k = df_nb_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for kb in kbest_list:\n",
        "        kbest = int((unlabeled_per_class*2)/kb)\n",
        "        print(\"---------------------------------K-Best = \", kbest,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(MultinomialNB(), criterion = 'k_best', k_best = kbest, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = None, kbest = kbest)\n",
        "        df_nb_imdb_2k = df_nb_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QghN8bR6lc3h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "outputId": "e765a11b-9d3a-4d1d-8278-c7f2f7b8f3ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Labeled  UnLabeled  Threshold  K-Best  Accuracy\n",
              "0      20.0        0.0        NaN     NaN   0.59016\n",
              "1      20.0     2000.0        NaN   500.0   0.49256\n",
              "2      20.0     2000.0        NaN   400.0   0.49256\n",
              "3      20.0     2000.0        NaN   333.0   0.49256\n",
              "4      20.0     2000.0        NaN   285.0   0.49256\n",
              "5      20.0     2000.0        NaN   250.0   0.49256\n",
              "6     100.0        0.0        NaN     NaN   0.68456\n",
              "7     100.0     2000.0        NaN   500.0   0.49256\n",
              "8     100.0     2000.0        NaN   400.0   0.49256\n",
              "9     100.0     2000.0        NaN   333.0   0.49256\n",
              "10    100.0     2000.0        NaN   285.0   0.49256\n",
              "11    100.0     2000.0        NaN   250.0   0.49256\n",
              "12    400.0        0.0        NaN     NaN   0.79504\n",
              "13    400.0     2000.0        NaN   500.0   0.49256\n",
              "14    400.0     2000.0        NaN   400.0   0.49256\n",
              "15    400.0     2000.0        NaN   333.0   0.49256\n",
              "16    400.0     2000.0        NaN   285.0   0.49256\n",
              "17    400.0     2000.0        NaN   250.0   0.49256\n",
              "18   1000.0        0.0        NaN     NaN   0.83752\n",
              "19   1000.0     2000.0        NaN   500.0   0.49256\n",
              "20   1000.0     2000.0        NaN   400.0   0.49256\n",
              "21   1000.0     2000.0        NaN   333.0   0.49256\n",
              "22   1000.0     2000.0        NaN   285.0   0.49256\n",
              "23   1000.0     2000.0        NaN   250.0   0.49256\n",
              "24   2000.0        0.0        NaN     NaN   0.85776\n",
              "25   2000.0     2000.0        NaN   500.0   0.49504\n",
              "26   2000.0     2000.0        NaN   400.0   0.49280\n",
              "27   2000.0     2000.0        NaN   333.0   0.49256\n",
              "28   2000.0     2000.0        NaN   285.0   0.49256\n",
              "29   2000.0     2000.0        NaN   250.0   0.49256"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb19c237-030a-48f3-93a2-ba84f8d00e03\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>K-Best</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.59016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>400.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>333.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>285.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.68456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>100.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>100.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>400.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>100.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>333.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>100.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>285.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>400.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.79504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>400.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>400.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>400.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>400.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>333.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>400.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>285.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>400.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.83752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>400.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>333.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>285.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.85776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0.49504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>400.0</td>\n",
              "      <td>0.49280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>333.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>285.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb19c237-030a-48f3-93a2-ba84f8d00e03')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb19c237-030a-48f3-93a2-ba84f8d00e03 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb19c237-030a-48f3-93a2-ba84f8d00e03');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(df_nb_imdb_2k[['Labeled', 'UnLabeled', 'Threshold', 'K-Best', 'Accuracy']].sort_values(['Labeled', 'UnLabeled']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "65I9Z92jjfB7",
        "outputId": "287bf411-c995-455a-e3a7-28b4bf1ebf59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "K-Best               NaN      250.0    285.0    333.0    400.0    500.0\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.59016      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "100.0   0.0        0.68456      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "400.0   0.0        0.79504      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "1000.0  0.0        0.83752      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "2000.0  0.0        0.85776      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49256  0.49256  0.49256  0.49280  0.49504"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5bd8fbd1-7682-42fc-ad3a-cd8890a19984\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>K-Best</th>\n",
              "      <th>NaN</th>\n",
              "      <th>250.0</th>\n",
              "      <th>285.0</th>\n",
              "      <th>333.0</th>\n",
              "      <th>400.0</th>\n",
              "      <th>500.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.59016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.68456</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.79504</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.83752</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.85776</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49280</td>\n",
              "      <td>0.49504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bd8fbd1-7682-42fc-ad3a-cd8890a19984')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5bd8fbd1-7682-42fc-ad3a-cd8890a19984 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5bd8fbd1-7682-42fc-ad3a-cd8890a19984');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "df_nb_imdb_2k.pivot(index=['Labeled', 'UnLabeled'], columns='K-Best')['Accuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No improvements can be seen on IMDB using Naive Bayes. Self Training reduces the supervised accuracies\n"
      ],
      "metadata": {
        "id": "iHl5YB8rHymL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jiSLJIujemX",
        "outputId": "72dd5cbd-3a57-49d3-c30d-7030609d616d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.590\n",
            "Accuracy Score:  0.59016\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.685\n",
            "Accuracy Score:  0.68456\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.795\n",
            "Accuracy Score:  0.79504\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.838\n",
            "Accuracy Score:  0.83752\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised NBClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.858\n",
            "Accuracy Score:  0.85776\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------K-Best =  1250 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1250 new labels.\n",
            "End of iteration 2, added 1250 new labels.\n",
            "End of iteration 3, added 1250 new labels.\n",
            "End of iteration 4, added 1250 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  1000 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1000 new labels.\n",
            "End of iteration 2, added 1000 new labels.\n",
            "End of iteration 3, added 1000 new labels.\n",
            "End of iteration 4, added 1000 new labels.\n",
            "End of iteration 5, added 1000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  833 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 833 new labels.\n",
            "End of iteration 2, added 833 new labels.\n",
            "End of iteration 3, added 833 new labels.\n",
            "End of iteration 4, added 833 new labels.\n",
            "End of iteration 5, added 833 new labels.\n",
            "End of iteration 6, added 833 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  714 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 714 new labels.\n",
            "End of iteration 2, added 714 new labels.\n",
            "End of iteration 3, added 714 new labels.\n",
            "End of iteration 4, added 714 new labels.\n",
            "End of iteration 5, added 714 new labels.\n",
            "End of iteration 6, added 714 new labels.\n",
            "End of iteration 7, added 714 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------K-Best =  625 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 625 new labels.\n",
            "End of iteration 2, added 625 new labels.\n",
            "End of iteration 3, added 625 new labels.\n",
            "End of iteration 4, added 625 new labels.\n",
            "End of iteration 5, added 625 new labels.\n",
            "End of iteration 6, added 625 new labels.\n",
            "End of iteration 7, added 625 new labels.\n",
            "End of iteration 8, added 625 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes for IMDB with 5k unlabeled observations\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Parameters\n",
        "# mnb_params = \n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_nb_imdb_5k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 2500\n",
        "      \n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised NBClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', MultinomialNB()),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_nb_imdb_5k = df_nb_imdb_5k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for kb in kbest_list:\n",
        "        kbest = int((unlabeled_per_class*2)/kb)\n",
        "        print(\"---------------------------------K-Best = \", kbest,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(MultinomialNB(), criterion = 'k_best', k_best = kbest, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = None, kbest = kbest)\n",
        "        df_nb_imdb_5k = df_nb_imdb_5k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "dNyJjKTVjeix",
        "outputId": "6f96c4ee-f01e-4e79-e8d7-966d9b389422"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Labeled  UnLabeled  Threshold  K-Best  Accuracy\n",
              "0      20.0        0.0        NaN     NaN   0.59016\n",
              "1      20.0     5000.0        NaN  1250.0   0.49256\n",
              "2      20.0     5000.0        NaN  1000.0   0.49256\n",
              "3      20.0     5000.0        NaN   833.0   0.49256\n",
              "4      20.0     5000.0        NaN   714.0   0.49256\n",
              "5      20.0     5000.0        NaN   625.0   0.49256\n",
              "6     100.0        0.0        NaN     NaN   0.68456\n",
              "7     100.0     5000.0        NaN  1250.0   0.49256\n",
              "8     100.0     5000.0        NaN  1000.0   0.49256\n",
              "9     100.0     5000.0        NaN   833.0   0.49256\n",
              "10    100.0     5000.0        NaN   714.0   0.49256\n",
              "11    100.0     5000.0        NaN   625.0   0.49256\n",
              "12    400.0        0.0        NaN     NaN   0.79504\n",
              "13    400.0     5000.0        NaN  1250.0   0.49256\n",
              "14    400.0     5000.0        NaN  1000.0   0.49256\n",
              "15    400.0     5000.0        NaN   833.0   0.49256\n",
              "16    400.0     5000.0        NaN   714.0   0.49256\n",
              "17    400.0     5000.0        NaN   625.0   0.49256\n",
              "18   1000.0        0.0        NaN     NaN   0.83752\n",
              "19   1000.0     5000.0        NaN  1250.0   0.49256\n",
              "20   1000.0     5000.0        NaN  1000.0   0.49256\n",
              "21   1000.0     5000.0        NaN   833.0   0.49256\n",
              "22   1000.0     5000.0        NaN   714.0   0.49256\n",
              "23   1000.0     5000.0        NaN   625.0   0.49256\n",
              "24   2000.0        0.0        NaN     NaN   0.85776\n",
              "25   2000.0     5000.0        NaN  1250.0   0.49256\n",
              "26   2000.0     5000.0        NaN  1000.0   0.49256\n",
              "27   2000.0     5000.0        NaN   833.0   0.49256\n",
              "28   2000.0     5000.0        NaN   714.0   0.49256\n",
              "29   2000.0     5000.0        NaN   625.0   0.49256"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-590fafb4-f0d7-4ecd-9b66-513052a222ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>K-Best</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.59016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.68456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>400.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.79504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>400.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.83752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.85776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>833.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>714.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>625.0</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-590fafb4-f0d7-4ecd-9b66-513052a222ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-590fafb4-f0d7-4ecd-9b66-513052a222ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-590fafb4-f0d7-4ecd-9b66-513052a222ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(df_nb_imdb_5k[['Labeled', 'UnLabeled', 'Threshold', 'K-Best', 'Accuracy']].sort_values(['Labeled', 'UnLabeled']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "PoS_KIuWjedf",
        "outputId": "81d50f26-c82c-4abe-c946-55b9f389df27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "K-Best              NaN      625.0    714.0    833.0    1000.0   1250.0\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.59016      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "100.0   0.0        0.68456      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "400.0   0.0        0.79504      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "1000.0  0.0        0.83752      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256\n",
              "2000.0  0.0        0.85776      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.49256  0.49256  0.49256  0.49256  0.49256"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71224cd7-8763-4f51-952a-ab7c1fe08a95\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>K-Best</th>\n",
              "      <th>NaN</th>\n",
              "      <th>625.0</th>\n",
              "      <th>714.0</th>\n",
              "      <th>833.0</th>\n",
              "      <th>1000.0</th>\n",
              "      <th>1250.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.59016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.68456</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.79504</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.83752</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.85776</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71224cd7-8763-4f51-952a-ab7c1fe08a95')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71224cd7-8763-4f51-952a-ab7c1fe08a95 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71224cd7-8763-4f51-952a-ab7c1fe08a95');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "df_nb_imdb_5k.pivot(index=['Labeled', 'UnLabeled'], columns='K-Best')['Accuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar degradation is seen at higher volumes of unlabeled data as well\n",
        "\n"
      ],
      "metadata": {
        "id": "DKuGVvLgJUZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Logistic Regression next on 2k and 5k unlabeled observations"
      ],
      "metadata": {
        "id": "PhEkhMmHM3b9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "HkTOtCsntDcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cd5e11-c3e8-4caa-e033-1eae6abee956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.565\n",
            "Accuracy Score:  0.56504\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.515\n",
            "Accuracy Score:  0.51472\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.515\n",
            "Accuracy Score:  0.51472\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1524 new labels.\n",
            "End of iteration 2, added 472 new labels.\n",
            "End of iteration 3, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.4928\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1016 new labels.\n",
            "End of iteration 2, added 982 new labels.\n",
            "End of iteration 3, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 432 new labels.\n",
            "End of iteration 2, added 1565 new labels.\n",
            "End of iteration 3, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.675\n",
            "Accuracy Score:  0.67512\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.705\n",
            "Accuracy Score:  0.7052\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.705\n",
            "Accuracy Score:  0.7052\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1551 new labels.\n",
            "End of iteration 2, added 352 new labels.\n",
            "End of iteration 3, added 48 new labels.\n",
            "End of iteration 4, added 9 new labels.\n",
            "End of iteration 5, added 9 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.708\n",
            "Accuracy Score:  0.70752\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1151 new labels.\n",
            "End of iteration 2, added 535 new labels.\n",
            "End of iteration 3, added 97 new labels.\n",
            "End of iteration 4, added 40 new labels.\n",
            "End of iteration 5, added 14 new labels.\n",
            "End of iteration 6, added 10 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 9 new labels.\n",
            "Micro-averaged F1 score on test set: 0.686\n",
            "Accuracy Score:  0.68592\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 675 new labels.\n",
            "End of iteration 2, added 688 new labels.\n",
            "End of iteration 3, added 196 new labels.\n",
            "End of iteration 4, added 96 new labels.\n",
            "End of iteration 5, added 45 new labels.\n",
            "End of iteration 6, added 17 new labels.\n",
            "End of iteration 7, added 35 new labels.\n",
            "End of iteration 8, added 17 new labels.\n",
            "End of iteration 9, added 16 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.607\n",
            "Accuracy Score:  0.60728\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.772\n",
            "Accuracy Score:  0.77224\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.797\n",
            "Accuracy Score:  0.79688\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.797\n",
            "Accuracy Score:  0.79688\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1721 new labels.\n",
            "End of iteration 2, added 215 new labels.\n",
            "End of iteration 3, added 24 new labels.\n",
            "End of iteration 4, added 5 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "End of iteration 6, added 4 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.800\n",
            "Accuracy Score:  0.79968\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1439 new labels.\n",
            "End of iteration 2, added 301 new labels.\n",
            "End of iteration 3, added 65 new labels.\n",
            "End of iteration 4, added 16 new labels.\n",
            "End of iteration 5, added 11 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.798\n",
            "Accuracy Score:  0.79776\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1105 new labels.\n",
            "End of iteration 2, added 324 new labels.\n",
            "End of iteration 3, added 102 new labels.\n",
            "End of iteration 4, added 49 new labels.\n",
            "End of iteration 5, added 17 new labels.\n",
            "End of iteration 6, added 13 new labels.\n",
            "End of iteration 7, added 8 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.799\n",
            "Accuracy Score:  0.79872\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.831\n",
            "Accuracy Score:  0.83112\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.840\n",
            "Accuracy Score:  0.83976\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.840\n",
            "Accuracy Score:  0.83976\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1746 new labels.\n",
            "End of iteration 2, added 157 new labels.\n",
            "End of iteration 3, added 27 new labels.\n",
            "End of iteration 4, added 12 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 7 new labels.\n",
            "Micro-averaged F1 score on test set: 0.838\n",
            "Accuracy Score:  0.83848\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1516 new labels.\n",
            "End of iteration 2, added 192 new labels.\n",
            "End of iteration 3, added 44 new labels.\n",
            "End of iteration 4, added 22 new labels.\n",
            "End of iteration 5, added 10 new labels.\n",
            "End of iteration 6, added 11 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 4 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.842\n",
            "Accuracy Score:  0.84192\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1224 new labels.\n",
            "End of iteration 2, added 193 new labels.\n",
            "End of iteration 3, added 50 new labels.\n",
            "End of iteration 4, added 28 new labels.\n",
            "End of iteration 5, added 17 new labels.\n",
            "End of iteration 6, added 10 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 7 new labels.\n",
            "End of iteration 9, added 8 new labels.\n",
            "End of iteration 10, added 11 new labels.\n",
            "Micro-averaged F1 score on test set: 0.838\n",
            "Accuracy Score:  0.83792\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.856\n",
            "Accuracy Score:  0.85648\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.863\n",
            "Accuracy Score:  0.86304\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.863\n",
            "Accuracy Score:  0.86304\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1798 new labels.\n",
            "End of iteration 2, added 106 new labels.\n",
            "End of iteration 3, added 21 new labels.\n",
            "End of iteration 4, added 9 new labels.\n",
            "End of iteration 5, added 6 new labels.\n",
            "End of iteration 6, added 9 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.861\n",
            "Accuracy Score:  0.86136\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1548 new labels.\n",
            "End of iteration 2, added 122 new labels.\n",
            "End of iteration 3, added 23 new labels.\n",
            "End of iteration 4, added 13 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.861\n",
            "Accuracy Score:  0.86088\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1293 new labels.\n",
            "End of iteration 2, added 119 new labels.\n",
            "End of iteration 3, added 34 new labels.\n",
            "End of iteration 4, added 12 new labels.\n",
            "End of iteration 5, added 3 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.861\n",
            "Accuracy Score:  0.86104\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression using SGD (log-Loss) for IMDB  with 2k unlabeled observations\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Parameters\n",
        "sgd_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_sgd_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised SGDClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', SGDClassifier(**sgd_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_sgd_imdb_2k = df_sgd_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sgd_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_sgd_imdb_2k = df_sgd_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "JMan7Lr8tDYU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "d673e9c3-16a7-46dd-84c4-115df395e7f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Threshold              NaN      0.4      0.5      0.6      0.7      0.8\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.56504      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.51472  0.51472  0.49280  0.49256  0.49256\n",
              "100.0   0.0        0.67512      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.70520  0.70520  0.70752  0.68592  0.60728\n",
              "400.0   0.0        0.77224      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.79688  0.79688  0.79968  0.79776  0.79872\n",
              "1000.0  0.0        0.83112      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.83976  0.83976  0.83848  0.84192  0.83792\n",
              "2000.0  0.0        0.85648      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.86304  0.86304  0.86136  0.86088  0.86104"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-226b6a1c-4afa-4790-8923-e4d2972f7e4a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.56504</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.51472</td>\n",
              "      <td>0.51472</td>\n",
              "      <td>0.49280</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.67512</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.70520</td>\n",
              "      <td>0.70520</td>\n",
              "      <td>0.70752</td>\n",
              "      <td>0.68592</td>\n",
              "      <td>0.60728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.77224</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.79688</td>\n",
              "      <td>0.79688</td>\n",
              "      <td>0.79968</td>\n",
              "      <td>0.79776</td>\n",
              "      <td>0.79872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.83112</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.83976</td>\n",
              "      <td>0.83976</td>\n",
              "      <td>0.83848</td>\n",
              "      <td>0.84192</td>\n",
              "      <td>0.83792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.85648</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.86304</td>\n",
              "      <td>0.86304</td>\n",
              "      <td>0.86136</td>\n",
              "      <td>0.86088</td>\n",
              "      <td>0.86104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-226b6a1c-4afa-4790-8923-e4d2972f7e4a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-226b6a1c-4afa-4790-8923-e4d2972f7e4a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-226b6a1c-4afa-4790-8923-e4d2972f7e4a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "df_sgd_imdb_2k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At very low volumes, logistic regression does not work BUT at higher volumes of labeled data from 100 onward, there is significant lift through semi-supervised learning"
      ],
      "metadata": {
        "id": "NofZPGAkNDy2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "dmBA7WuZtDJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3599345d-e440-4e18-931c-359ab4f78cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.565\n",
            "Accuracy Score:  0.56504\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.508\n",
            "Accuracy Score:  0.50816\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.508\n",
            "Accuracy Score:  0.50816\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3825 new labels.\n",
            "End of iteration 2, added 1174 new labels.\n",
            "End of iteration 3, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49264\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2286 new labels.\n",
            "End of iteration 2, added 2714 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 750 new labels.\n",
            "End of iteration 2, added 4248 new labels.\n",
            "End of iteration 3, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.675\n",
            "Accuracy Score:  0.67512\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.711\n",
            "Accuracy Score:  0.71096\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.711\n",
            "Accuracy Score:  0.71096\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3633 new labels.\n",
            "End of iteration 2, added 1091 new labels.\n",
            "End of iteration 3, added 116 new labels.\n",
            "End of iteration 4, added 41 new labels.\n",
            "End of iteration 5, added 16 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.717\n",
            "Accuracy Score:  0.71696\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2356 new labels.\n",
            "End of iteration 2, added 1724 new labels.\n",
            "End of iteration 3, added 304 new labels.\n",
            "End of iteration 4, added 100 new labels.\n",
            "End of iteration 5, added 32 new labels.\n",
            "End of iteration 6, added 25 new labels.\n",
            "End of iteration 7, added 38 new labels.\n",
            "End of iteration 8, added 14 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 7 new labels.\n",
            "Micro-averaged F1 score on test set: 0.706\n",
            "Accuracy Score:  0.70568\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1208 new labels.\n",
            "End of iteration 2, added 1992 new labels.\n",
            "End of iteration 3, added 610 new labels.\n",
            "End of iteration 4, added 226 new labels.\n",
            "End of iteration 5, added 134 new labels.\n",
            "End of iteration 6, added 87 new labels.\n",
            "End of iteration 7, added 26 new labels.\n",
            "End of iteration 8, added 22 new labels.\n",
            "End of iteration 9, added 30 new labels.\n",
            "End of iteration 10, added 38 new labels.\n",
            "Micro-averaged F1 score on test set: 0.617\n",
            "Accuracy Score:  0.61736\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.772\n",
            "Accuracy Score:  0.77224\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.805\n",
            "Accuracy Score:  0.80544\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.805\n",
            "Accuracy Score:  0.80544\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 4176 new labels.\n",
            "End of iteration 2, added 606 new labels.\n",
            "End of iteration 3, added 63 new labels.\n",
            "End of iteration 4, added 19 new labels.\n",
            "End of iteration 5, added 7 new labels.\n",
            "End of iteration 6, added 12 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 21 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.804\n",
            "Accuracy Score:  0.80352\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3334 new labels.\n",
            "End of iteration 2, added 903 new labels.\n",
            "End of iteration 3, added 176 new labels.\n",
            "End of iteration 4, added 64 new labels.\n",
            "End of iteration 5, added 16 new labels.\n",
            "End of iteration 6, added 10 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "End of iteration 8, added 10 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.808\n",
            "Accuracy Score:  0.80784\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2376 new labels.\n",
            "End of iteration 2, added 1084 new labels.\n",
            "End of iteration 3, added 280 new labels.\n",
            "End of iteration 4, added 95 new labels.\n",
            "End of iteration 5, added 45 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 8 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 9 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.805\n",
            "Accuracy Score:  0.8052\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.831\n",
            "Accuracy Score:  0.83112\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.845\n",
            "Accuracy Score:  0.84456\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.845\n",
            "Accuracy Score:  0.84456\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 4328 new labels.\n",
            "End of iteration 2, added 451 new labels.\n",
            "End of iteration 3, added 61 new labels.\n",
            "End of iteration 4, added 14 new labels.\n",
            "End of iteration 5, added 8 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 15 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "End of iteration 10, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.841\n",
            "Accuracy Score:  0.84144\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3667 new labels.\n",
            "End of iteration 2, added 621 new labels.\n",
            "End of iteration 3, added 119 new labels.\n",
            "End of iteration 4, added 31 new labels.\n",
            "End of iteration 5, added 23 new labels.\n",
            "End of iteration 6, added 5 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 4 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "Micro-averaged F1 score on test set: 0.841\n",
            "Accuracy Score:  0.84072\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2886 new labels.\n",
            "End of iteration 2, added 679 new labels.\n",
            "End of iteration 3, added 186 new labels.\n",
            "End of iteration 4, added 63 new labels.\n",
            "End of iteration 5, added 25 new labels.\n",
            "End of iteration 6, added 19 new labels.\n",
            "End of iteration 7, added 11 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.837\n",
            "Accuracy Score:  0.83704\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised SGDClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.856\n",
            "Accuracy Score:  0.85648\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.866\n",
            "Accuracy Score:  0.86568\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.866\n",
            "Accuracy Score:  0.86568\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 4448 new labels.\n",
            "End of iteration 2, added 323 new labels.\n",
            "End of iteration 3, added 61 new labels.\n",
            "End of iteration 4, added 20 new labels.\n",
            "End of iteration 5, added 9 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 8 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 5 new labels.\n",
            "End of iteration 10, added 7 new labels.\n",
            "Micro-averaged F1 score on test set: 0.864\n",
            "Accuracy Score:  0.8644\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3815 new labels.\n",
            "End of iteration 2, added 413 new labels.\n",
            "End of iteration 3, added 84 new labels.\n",
            "End of iteration 4, added 33 new labels.\n",
            "End of iteration 5, added 16 new labels.\n",
            "End of iteration 6, added 8 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 4 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 15 new labels.\n",
            "Micro-averaged F1 score on test set: 0.865\n",
            "Accuracy Score:  0.86464\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3117 new labels.\n",
            "End of iteration 2, added 438 new labels.\n",
            "End of iteration 3, added 109 new labels.\n",
            "End of iteration 4, added 33 new labels.\n",
            "End of iteration 5, added 24 new labels.\n",
            "End of iteration 6, added 22 new labels.\n",
            "End of iteration 7, added 25 new labels.\n",
            "End of iteration 8, added 4 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 6 new labels.\n",
            "Micro-averaged F1 score on test set: 0.863\n",
            "Accuracy Score:  0.8628\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression using SGD (log-Loss) for IMDB  with 5k unlabeled observations\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Parameters\n",
        "sgd_params = dict(alpha=1e-5, penalty='l2', loss='log', random_state=0)\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_sgd_imdb_5k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 2500\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised SGDClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('clf', SGDClassifier(**sgd_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_sgd_imdb_5k = df_sgd_imdb_5k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sgd_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_sgd_imdb_5k = df_sgd_imdb_5k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "i5G1zJKztDDZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "e9f1dfb3-2780-4799-ce00-6fb83a2d2392"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Threshold              NaN      0.4      0.5      0.6      0.7      0.8\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.56504      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.50816  0.50816  0.49264  0.49256  0.49256\n",
              "100.0   0.0        0.67512      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.71096  0.71096  0.71696  0.70568  0.61736\n",
              "400.0   0.0        0.77224      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.80544  0.80544  0.80352  0.80784  0.80520\n",
              "1000.0  0.0        0.83112      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.84456  0.84456  0.84144  0.84072  0.83704\n",
              "2000.0  0.0        0.85648      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.86568  0.86568  0.86440  0.86464  0.86280"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8bd2379-24b7-4d9c-bdb0-1752a078c4fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.56504</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.50816</td>\n",
              "      <td>0.50816</td>\n",
              "      <td>0.49264</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.67512</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.71096</td>\n",
              "      <td>0.71096</td>\n",
              "      <td>0.71696</td>\n",
              "      <td>0.70568</td>\n",
              "      <td>0.61736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.77224</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.80544</td>\n",
              "      <td>0.80544</td>\n",
              "      <td>0.80352</td>\n",
              "      <td>0.80784</td>\n",
              "      <td>0.80520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.83112</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.84456</td>\n",
              "      <td>0.84456</td>\n",
              "      <td>0.84144</td>\n",
              "      <td>0.84072</td>\n",
              "      <td>0.83704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.85648</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.86568</td>\n",
              "      <td>0.86568</td>\n",
              "      <td>0.86440</td>\n",
              "      <td>0.86464</td>\n",
              "      <td>0.86280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8bd2379-24b7-4d9c-bdb0-1752a078c4fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e8bd2379-24b7-4d9c-bdb0-1752a078c4fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e8bd2379-24b7-4d9c-bdb0-1752a078c4fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "df_sgd_imdb_5k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar pattern as 2k unlabeled volumes is observed with 5k unlabeled volumes.\n",
        "In all cases, the higher unlabeled volume (5k) has generated a stronger lift than at lower volume (2k) "
      ],
      "metadata": {
        "id": "xtx2mAbRNIGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running MLP next on 2k and 5k unlabeled observations"
      ],
      "metadata": {
        "id": "h1VbZKDkNJXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "YHunKjVfjd8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d3e5b3-5ff3-4479-967d-1e3840fde801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.605\n",
            "Accuracy Score:  0.60472\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.576\n",
            "Accuracy Score:  0.57592\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.576\n",
            "Accuracy Score:  0.57592\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1519 new labels.\n",
            "End of iteration 2, added 439 new labels.\n",
            "End of iteration 3, added 30 new labels.\n",
            "End of iteration 4, added 3 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.558\n",
            "Accuracy Score:  0.55792\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1048 new labels.\n",
            "End of iteration 2, added 776 new labels.\n",
            "End of iteration 3, added 139 new labels.\n",
            "End of iteration 4, added 12 new labels.\n",
            "End of iteration 5, added 2 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.523\n",
            "Accuracy Score:  0.52288\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 592 new labels.\n",
            "End of iteration 2, added 1104 new labels.\n",
            "End of iteration 3, added 296 new labels.\n",
            "End of iteration 4, added 7 new labels.\n",
            "Micro-averaged F1 score on test set: 0.495\n",
            "Accuracy Score:  0.4952\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.663\n",
            "Accuracy Score:  0.66304\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.613\n",
            "Accuracy Score:  0.6132\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.613\n",
            "Accuracy Score:  0.6132\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1557 new labels.\n",
            "End of iteration 2, added 378 new labels.\n",
            "End of iteration 3, added 42 new labels.\n",
            "End of iteration 4, added 5 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.629\n",
            "Accuracy Score:  0.62944\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1152 new labels.\n",
            "End of iteration 2, added 550 new labels.\n",
            "End of iteration 3, added 145 new labels.\n",
            "End of iteration 4, added 42 new labels.\n",
            "End of iteration 5, added 17 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 5 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.653\n",
            "Accuracy Score:  0.65256\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 720 new labels.\n",
            "End of iteration 2, added 580 new labels.\n",
            "End of iteration 3, added 288 new labels.\n",
            "End of iteration 4, added 118 new labels.\n",
            "End of iteration 5, added 52 new labels.\n",
            "End of iteration 6, added 15 new labels.\n",
            "End of iteration 7, added 7 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.652\n",
            "Accuracy Score:  0.65184\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.756\n",
            "Accuracy Score:  0.75648\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.733\n",
            "Accuracy Score:  0.73272\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.733\n",
            "Accuracy Score:  0.73272\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1723 new labels.\n",
            "End of iteration 2, added 235 new labels.\n",
            "End of iteration 3, added 26 new labels.\n",
            "End of iteration 4, added 3 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "End of iteration 6, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.749\n",
            "Accuracy Score:  0.74896\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1411 new labels.\n",
            "End of iteration 2, added 377 new labels.\n",
            "End of iteration 3, added 73 new labels.\n",
            "End of iteration 4, added 21 new labels.\n",
            "End of iteration 5, added 6 new labels.\n",
            "End of iteration 6, added 3 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 3 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.752\n",
            "Accuracy Score:  0.75168\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1032 new labels.\n",
            "End of iteration 2, added 477 new labels.\n",
            "End of iteration 3, added 137 new labels.\n",
            "End of iteration 4, added 40 new labels.\n",
            "End of iteration 5, added 13 new labels.\n",
            "End of iteration 6, added 6 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.760\n",
            "Accuracy Score:  0.76048\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.815\n",
            "Accuracy Score:  0.81464\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.787\n",
            "Accuracy Score:  0.78672\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.787\n",
            "Accuracy Score:  0.78672\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1815 new labels.\n",
            "End of iteration 2, added 155 new labels.\n",
            "End of iteration 3, added 8 new labels.\n",
            "End of iteration 4, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.784\n",
            "Accuracy Score:  0.7844\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1602 new labels.\n",
            "End of iteration 2, added 266 new labels.\n",
            "End of iteration 3, added 38 new labels.\n",
            "End of iteration 4, added 9 new labels.\n",
            "End of iteration 5, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.785\n",
            "Accuracy Score:  0.78536\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1357 new labels.\n",
            "End of iteration 2, added 303 new labels.\n",
            "End of iteration 3, added 76 new labels.\n",
            "End of iteration 4, added 28 new labels.\n",
            "End of iteration 5, added 5 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.786\n",
            "Accuracy Score:  0.7864\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.814\n",
            "Accuracy Score:  0.81368\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.825\n",
            "Accuracy Score:  0.82472\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.825\n",
            "Accuracy Score:  0.82472\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1867 new labels.\n",
            "End of iteration 2, added 107 new labels.\n",
            "End of iteration 3, added 7 new labels.\n",
            "End of iteration 4, added 2 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.826\n",
            "Accuracy Score:  0.82552\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1757 new labels.\n",
            "End of iteration 2, added 141 new labels.\n",
            "End of iteration 3, added 18 new labels.\n",
            "End of iteration 4, added 5 new labels.\n",
            "End of iteration 5, added 2 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 10 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.833\n",
            "Accuracy Score:  0.83336\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1573 new labels.\n",
            "End of iteration 2, added 183 new labels.\n",
            "End of iteration 3, added 53 new labels.\n",
            "End of iteration 4, added 15 new labels.\n",
            "End of iteration 5, added 12 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "End of iteration 7, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.832\n",
            "Accuracy Score:  0.83248\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# MLP for IMDB at 2k unlabeled volume\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Parameters\n",
        "mlp_params = dict(hidden_layer_sizes=(100,), max_iter=100,activation = 'relu',solver='lbfgs',random_state=1,learning_rate_init=0.01,\n",
        "                  learning_rate='adaptive') # regularization is by default based on alpha =0.0001\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('scale', StandardScaler(with_mean=False)),\n",
        "          ('clf', MLPClassifier(**mlp_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('scale', StandardScaler(with_mean=False)),\n",
        "            ('clf', SelfTrainingClassifier(MLPClassifier(**mlp_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mlp_imdb_2k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "WXfqPZ8rPoZi",
        "outputId": "08b6872d-6992-444c-9839-f16177521bac"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Threshold              NaN      0.4      0.5      0.6      0.7      0.8\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.60472      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.57592  0.57592  0.55792  0.52288  0.49520\n",
              "100.0   0.0        0.66304      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.61320  0.61320  0.62944  0.65256  0.65184\n",
              "400.0   0.0        0.75648      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.73272  0.73272  0.74896  0.75168  0.76048\n",
              "1000.0  0.0        0.81464      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.78672  0.78672  0.78440  0.78536  0.78640\n",
              "2000.0  0.0        0.81368      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.82472  0.82472  0.82552  0.83336  0.83248"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf0b8d42-ff01-42cf-9a91-bf0bd2baaf53\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.60472</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.57592</td>\n",
              "      <td>0.57592</td>\n",
              "      <td>0.55792</td>\n",
              "      <td>0.52288</td>\n",
              "      <td>0.49520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.66304</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.61320</td>\n",
              "      <td>0.61320</td>\n",
              "      <td>0.62944</td>\n",
              "      <td>0.65256</td>\n",
              "      <td>0.65184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.75648</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.73272</td>\n",
              "      <td>0.73272</td>\n",
              "      <td>0.74896</td>\n",
              "      <td>0.75168</td>\n",
              "      <td>0.76048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.81464</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.78672</td>\n",
              "      <td>0.78672</td>\n",
              "      <td>0.78440</td>\n",
              "      <td>0.78536</td>\n",
              "      <td>0.78640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.81368</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.82472</td>\n",
              "      <td>0.82472</td>\n",
              "      <td>0.82552</td>\n",
              "      <td>0.83336</td>\n",
              "      <td>0.83248</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf0b8d42-ff01-42cf-9a91-bf0bd2baaf53')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf0b8d42-ff01-42cf-9a91-bf0bd2baaf53 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf0b8d42-ff01-42cf-9a91-bf0bd2baaf53');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PhifU9X3Pojj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP for IMDB at 5k unlabeled volume\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Parameters\n",
        "mlp_params = dict(hidden_layer_sizes=(100,), max_iter=100,activation = 'relu',solver='lbfgs',random_state=1,learning_rate_init=0.01,\n",
        "                  learning_rate='adaptive') # regularization is by default based on alpha =0.0001\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_5k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 2500\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('scale', StandardScaler(with_mean=False)),\n",
        "          ('clf', MLPClassifier(**mlp_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_5k = df_mlp_imdb_5k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('scale', StandardScaler(with_mean=False)),\n",
        "            ('clf', SelfTrainingClassifier(MLPClassifier(**mlp_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_imdb_5k = df_mlp_imdb_5k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SkUWXoLIPotH",
        "outputId": "af83b51d-682c-469b-e32f-78509761b5e4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.605\n",
            "Accuracy Score:  0.60472\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.578\n",
            "Accuracy Score:  0.5784\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.578\n",
            "Accuracy Score:  0.5784\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3695 new labels.\n",
            "End of iteration 2, added 1123 new labels.\n",
            "End of iteration 3, added 133 new labels.\n",
            "End of iteration 4, added 17 new labels.\n",
            "End of iteration 5, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.600\n",
            "Accuracy Score:  0.60032\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2457 new labels.\n",
            "End of iteration 2, added 1688 new labels.\n",
            "End of iteration 3, added 506 new labels.\n",
            "End of iteration 4, added 107 new labels.\n",
            "End of iteration 5, added 31 new labels.\n",
            "End of iteration 6, added 13 new labels.\n",
            "End of iteration 7, added 12 new labels.\n",
            "End of iteration 8, added 8 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.592\n",
            "Accuracy Score:  0.59192\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1246 new labels.\n",
            "End of iteration 2, added 2031 new labels.\n",
            "End of iteration 3, added 1122 new labels.\n",
            "End of iteration 4, added 300 new labels.\n",
            "End of iteration 5, added 75 new labels.\n",
            "End of iteration 6, added 33 new labels.\n",
            "End of iteration 7, added 9 new labels.\n",
            "End of iteration 8, added 4 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.551\n",
            "Accuracy Score:  0.55056\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.663\n",
            "Accuracy Score:  0.66304\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.610\n",
            "Accuracy Score:  0.6104\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.610\n",
            "Accuracy Score:  0.6104\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3815 new labels.\n",
            "End of iteration 2, added 1006 new labels.\n",
            "End of iteration 3, added 118 new labels.\n",
            "End of iteration 4, added 25 new labels.\n",
            "End of iteration 5, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.630\n",
            "Accuracy Score:  0.6304\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2596 new labels.\n",
            "End of iteration 2, added 1600 new labels.\n",
            "End of iteration 3, added 458 new labels.\n",
            "End of iteration 4, added 120 new labels.\n",
            "End of iteration 5, added 36 new labels.\n",
            "End of iteration 6, added 10 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.660\n",
            "Accuracy Score:  0.66008\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1482 new labels.\n",
            "End of iteration 2, added 1662 new labels.\n",
            "End of iteration 3, added 857 new labels.\n",
            "End of iteration 4, added 337 new labels.\n",
            "End of iteration 5, added 110 new labels.\n",
            "End of iteration 6, added 41 new labels.\n",
            "End of iteration 7, added 14 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 3 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.676\n",
            "Accuracy Score:  0.67576\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.756\n",
            "Accuracy Score:  0.75648\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.669\n",
            "Accuracy Score:  0.66896\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.669\n",
            "Accuracy Score:  0.66896\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 4084 new labels.\n",
            "End of iteration 2, added 773 new labels.\n",
            "End of iteration 3, added 98 new labels.\n",
            "End of iteration 4, added 3 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.657\n",
            "Accuracy Score:  0.65712\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3132 new labels.\n",
            "End of iteration 2, added 1390 new labels.\n",
            "End of iteration 3, added 282 new labels.\n",
            "End of iteration 4, added 73 new labels.\n",
            "End of iteration 5, added 27 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 3 new labels.\n",
            "End of iteration 8, added 1 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "End of iteration 10, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.600\n",
            "Accuracy Score:  0.6\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2154 new labels.\n",
            "End of iteration 2, added 1832 new labels.\n",
            "End of iteration 3, added 693 new labels.\n",
            "End of iteration 4, added 211 new labels.\n",
            "End of iteration 5, added 41 new labels.\n",
            "End of iteration 6, added 7 new labels.\n",
            "End of iteration 7, added 4 new labels.\n",
            "End of iteration 8, added 2 new labels.\n",
            "End of iteration 9, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.527\n",
            "Accuracy Score:  0.52728\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.815\n",
            "Accuracy Score:  0.81464\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.778\n",
            "Accuracy Score:  0.778\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.778\n",
            "Accuracy Score:  0.778\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 4311 new labels.\n",
            "End of iteration 2, added 598 new labels.\n",
            "End of iteration 3, added 47 new labels.\n",
            "End of iteration 4, added 7 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.790\n",
            "Accuracy Score:  0.7904\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 3600 new labels.\n",
            "End of iteration 2, added 1023 new labels.\n",
            "End of iteration 3, added 191 new labels.\n",
            "End of iteration 4, added 49 new labels.\n",
            "End of iteration 5, added 10 new labels.\n",
            "End of iteration 6, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.793\n",
            "Accuracy Score:  0.7932\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2792 new labels.\n",
            "End of iteration 2, added 1307 new labels.\n",
            "End of iteration 3, added 353 new labels.\n",
            "End of iteration 4, added 99 new labels.\n",
            "End of iteration 5, added 28 new labels.\n",
            "End of iteration 6, added 14 new labels.\n",
            "End of iteration 7, added 2 new labels.\n",
            "End of iteration 8, added 5 new labels.\n",
            "End of iteration 9, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.790\n",
            "Accuracy Score:  0.7904\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.814\n",
            "Accuracy Score:  0.81368\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.817\n",
            "Accuracy Score:  0.81728\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-b0e17258989a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelfTrainingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmlp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'threshold'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         ])\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_and_print_metrics_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mdf_mlp_imdb_5k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_mlp_imdb_5k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-20646ee8edc3>\u001b[0m in \u001b[0;36meval_and_print_metrics_df\u001b[0;34m(clf, X_train, y_train, X_test, y_test, thresh, kbest)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mdict1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'K-Best'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     print(\"Micro-averaged F1 score on test set: \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             )\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mlp_imdb_5k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "0IkfF1vlPo1c",
        "outputId": "0634ca30-4721-470f-eabc-e227dea68ccd"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Threshold              NaN      0.4      0.5      0.6      0.7      0.8\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.60472      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.57840  0.57840  0.60032  0.59192  0.55056\n",
              "100.0   0.0        0.66304      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.61040  0.61040  0.63040  0.66008  0.67576\n",
              "400.0   0.0        0.75648      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.66896  0.66896  0.65712  0.60000  0.52728\n",
              "1000.0  0.0        0.81464      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.77800  0.77800  0.79040  0.79320  0.79040\n",
              "2000.0  0.0        0.81368      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.81728      NaN      NaN      NaN      NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df41355c-a5d1-475e-9d8a-2058082c6cbe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.60472</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.57840</td>\n",
              "      <td>0.57840</td>\n",
              "      <td>0.60032</td>\n",
              "      <td>0.59192</td>\n",
              "      <td>0.55056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.66304</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.61040</td>\n",
              "      <td>0.61040</td>\n",
              "      <td>0.63040</td>\n",
              "      <td>0.66008</td>\n",
              "      <td>0.67576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.75648</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.66896</td>\n",
              "      <td>0.66896</td>\n",
              "      <td>0.65712</td>\n",
              "      <td>0.60000</td>\n",
              "      <td>0.52728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.81464</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.77800</td>\n",
              "      <td>0.77800</td>\n",
              "      <td>0.79040</td>\n",
              "      <td>0.79320</td>\n",
              "      <td>0.79040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.81368</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.81728</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df41355c-a5d1-475e-9d8a-2058082c6cbe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-df41355c-a5d1-475e-9d8a-2058082c6cbe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-df41355c-a5d1-475e-9d8a-2058082c6cbe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5k does not bring any difference. The results are not great with a shallow netowrk. Attempting it with a denser network and a better optimizer"
      ],
      "metadata": {
        "id": "L8lthJ8HPrbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP config2 for IMDB at 2k unlabeled volume\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Parameters\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=100,activation = 'relu',solver='adam',random_state=1,learning_rate_init=0.01,\n",
        "                  learning_rate='adaptive')\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "          ('vect', CountVectorizer(**vectorizer_params)),\n",
        "          ('tfidf', TfidfTransformer()),\n",
        "          ('scale', StandardScaler(with_mean=False)),\n",
        "          ('clf', MLPClassifier(**mlp_params)),\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "        \n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('scale', StandardScaler(with_mean=False)),\n",
        "            ('clf', SelfTrainingClassifier(MLPClassifier(**mlp_params), criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci5b2kD_dBXE",
        "outputId": "3057dcde-014a-4fca-ba7d-49a3ab57ea12"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.575\n",
            "Accuracy Score:  0.57512\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.498\n",
            "Accuracy Score:  0.49848\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.498\n",
            "Accuracy Score:  0.49848\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1960 new labels.\n",
            "End of iteration 2, added 40 new labels.\n",
            "Micro-averaged F1 score on test set: 0.502\n",
            "Accuracy Score:  0.5024\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1932 new labels.\n",
            "End of iteration 2, added 68 new labels.\n",
            "Micro-averaged F1 score on test set: 0.502\n",
            "Accuracy Score:  0.50192\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1903 new labels.\n",
            "End of iteration 2, added 94 new labels.\n",
            "End of iteration 3, added 2 new labels.\n",
            "End of iteration 4, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.497\n",
            "Accuracy Score:  0.49672\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.681\n",
            "Accuracy Score:  0.68056\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.514\n",
            "Accuracy Score:  0.51424\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.514\n",
            "Accuracy Score:  0.51424\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1950 new labels.\n",
            "End of iteration 2, added 50 new labels.\n",
            "Micro-averaged F1 score on test set: 0.508\n",
            "Accuracy Score:  0.50768\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1907 new labels.\n",
            "End of iteration 2, added 93 new labels.\n",
            "Micro-averaged F1 score on test set: 0.512\n",
            "Accuracy Score:  0.51216\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1842 new labels.\n",
            "End of iteration 2, added 153 new labels.\n",
            "End of iteration 3, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.522\n",
            "Accuracy Score:  0.52224\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.769\n",
            "Accuracy Score:  0.76896\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.749\n",
            "Accuracy Score:  0.74936\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.749\n",
            "Accuracy Score:  0.74936\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1695 new labels.\n",
            "End of iteration 2, added 295 new labels.\n",
            "End of iteration 3, added 9 new labels.\n",
            "End of iteration 4, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.747\n",
            "Accuracy Score:  0.74664\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1352 new labels.\n",
            "End of iteration 2, added 596 new labels.\n",
            "End of iteration 3, added 50 new labels.\n",
            "End of iteration 4, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.753\n",
            "Accuracy Score:  0.7532\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 987 new labels.\n",
            "End of iteration 2, added 901 new labels.\n",
            "End of iteration 3, added 98 new labels.\n",
            "End of iteration 4, added 12 new labels.\n",
            "End of iteration 5, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.776\n",
            "Accuracy Score:  0.77576\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.803\n",
            "Accuracy Score:  0.8032\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.789\n",
            "Accuracy Score:  0.78872\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.789\n",
            "Accuracy Score:  0.78872\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1862 new labels.\n",
            "End of iteration 2, added 131 new labels.\n",
            "End of iteration 3, added 6 new labels.\n",
            "End of iteration 4, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.785\n",
            "Accuracy Score:  0.78472\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1708 new labels.\n",
            "End of iteration 2, added 277 new labels.\n",
            "End of iteration 3, added 14 new labels.\n",
            "End of iteration 4, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.787\n",
            "Accuracy Score:  0.78688\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1524 new labels.\n",
            "End of iteration 2, added 392 new labels.\n",
            "End of iteration 3, added 76 new labels.\n",
            "End of iteration 4, added 6 new labels.\n",
            "End of iteration 5, added 2 new labels.\n",
            "Micro-averaged F1 score on test set: 0.800\n",
            "Accuracy Score:  0.80048\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.811\n",
            "Accuracy Score:  0.81096\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.810\n",
            "Accuracy Score:  0.80984\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.810\n",
            "Accuracy Score:  0.80984\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1918 new labels.\n",
            "End of iteration 2, added 78 new labels.\n",
            "End of iteration 3, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.813\n",
            "Accuracy Score:  0.8128\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1813 new labels.\n",
            "End of iteration 2, added 170 new labels.\n",
            "End of iteration 3, added 13 new labels.\n",
            "End of iteration 4, added 3 new labels.\n",
            "End of iteration 5, added 1 new labels.\n",
            "Micro-averaged F1 score on test set: 0.801\n",
            "Accuracy Score:  0.80136\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 1685 new labels.\n",
            "End of iteration 2, added 257 new labels.\n",
            "End of iteration 3, added 47 new labels.\n",
            "End of iteration 4, added 7 new labels.\n",
            "End of iteration 5, added 4 new labels.\n",
            "Micro-averaged F1 score on test set: 0.815\n",
            "Accuracy Score:  0.81528\n",
            "----------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mlp_imdb_2k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "LuRyeF6cdBYd",
        "outputId": "20393a39-f6d1-4785-f219-64606c9d4681"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Threshold              NaN      0.4      0.5      0.6      0.7      0.8\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.57512      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.49848  0.49848  0.50240  0.50192  0.49672\n",
              "100.0   0.0        0.68056      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.51424  0.51424  0.50768  0.51216  0.52224\n",
              "400.0   0.0        0.76896      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.74936  0.74936  0.74664  0.75320  0.77576\n",
              "1000.0  0.0        0.80320      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.78872  0.78872  0.78472  0.78688  0.80048\n",
              "2000.0  0.0        0.81096      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.80984  0.80984  0.81280  0.80136  0.81528"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07a9a219-9734-44fc-ab83-abe7750a71e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.57512</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.49848</td>\n",
              "      <td>0.49848</td>\n",
              "      <td>0.50240</td>\n",
              "      <td>0.50192</td>\n",
              "      <td>0.49672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.68056</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.51424</td>\n",
              "      <td>0.51424</td>\n",
              "      <td>0.50768</td>\n",
              "      <td>0.51216</td>\n",
              "      <td>0.52224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.76896</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.74936</td>\n",
              "      <td>0.74936</td>\n",
              "      <td>0.74664</td>\n",
              "      <td>0.75320</td>\n",
              "      <td>0.77576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.80320</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.78872</td>\n",
              "      <td>0.78872</td>\n",
              "      <td>0.78472</td>\n",
              "      <td>0.78688</td>\n",
              "      <td>0.80048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.81096</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.80984</td>\n",
              "      <td>0.80984</td>\n",
              "      <td>0.81280</td>\n",
              "      <td>0.80136</td>\n",
              "      <td>0.81528</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07a9a219-9734-44fc-ab83-abe7750a71e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07a9a219-9734-44fc-ab83-abe7750a71e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07a9a219-9734-44fc-ab83-abe7750a71e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the keras approach to neural network to see if things get better"
      ],
      "metadata": {
        "id": "tuSHT0Q4dCHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(100, activation='relu'),\n",
        "      tf.keras.layers.Dense(50),\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                # loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CCEzfz_dChn",
        "outputId": "bd77658a-69c5-4e03-b3d6-8b35384bd99d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP config2 using Keras for IMDB at 2k unlabeled volume\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Parameters\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=100,activation = 'relu',solver='adam',random_state=1,learning_rate_init=0.01,\n",
        "                  learning_rate='adaptive')\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_2k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 1000\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "      ('vect', CountVectorizer(**vectorizer_params)),\n",
        "      ('tfidf', TfidfTransformer()), \n",
        "      # ('kbest', MySelectKBest(f_classif, k=20000)),         \n",
        "      ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "      ('clf', model)\n",
        "      ])\n",
        "\n",
        "      \n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "        tf.keras.backend.clear_session()  \n",
        "\n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            # ('kbest', MySelectKBest(f_classif, k=20000)),\n",
        "            ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "            ('clf', SelfTrainingClassifier(model, criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_imdb_2k = df_mlp_imdb_2k.append(temp, ignore_index=True)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O__4RAFntwAe",
        "outputId": "e43bc79d-31a8-47bb-fd75-c9d7738ce80d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.514\n",
            "Accuracy Score:  0.51448\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 650 new labels.\n",
            "End of iteration 2, added 1350 new labels.\n",
            "Micro-averaged F1 score on test set: 0.673\n",
            "Accuracy Score:  0.67256\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 855 new labels.\n",
            "End of iteration 2, added 1145 new labels.\n",
            "Micro-averaged F1 score on test set: 0.692\n",
            "Accuracy Score:  0.69176\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 454 new labels.\n",
            "End of iteration 2, added 1546 new labels.\n",
            "Micro-averaged F1 score on test set: 0.494\n",
            "Accuracy Score:  0.4936\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 336 new labels.\n",
            "End of iteration 2, added 1664 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2020\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 59 new labels.\n",
            "End of iteration 2, added 1941 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.663\n",
            "Accuracy Score:  0.66328\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.724\n",
            "Accuracy Score:  0.72352\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.693\n",
            "Accuracy Score:  0.69264\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.712\n",
            "Accuracy Score:  0.712\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.737\n",
            "Accuracy Score:  0.73744\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2100\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.734\n",
            "Accuracy Score:  0.73368\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.795\n",
            "Accuracy Score:  0.79488\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.814\n",
            "Accuracy Score:  0.81424\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.814\n",
            "Accuracy Score:  0.81392\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.814\n",
            "Accuracy Score:  0.81432\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.815\n",
            "Accuracy Score:  0.8148\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 2400\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.817\n",
            "Accuracy Score:  0.81744\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.840\n",
            "Accuracy Score:  0.84008\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.851\n",
            "Accuracy Score:  0.85144\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.849\n",
            "Accuracy Score:  0.84888\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.852\n",
            "Accuracy Score:  0.852\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.852\n",
            "Accuracy Score:  0.85208\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 3000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.849\n",
            "Accuracy Score:  0.84904\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.860\n",
            "Accuracy Score:  0.85952\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.868\n",
            "Accuracy Score:  0.86824\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.86904\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.86888\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.86856\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 4000\n",
            "Unlabeled samples in training set: 2000\n",
            "End of iteration 1, added 2000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.86912\n",
            "----------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mlp_imdb_2k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "sqSJI_OzdCkZ",
        "outputId": "6877da58-a6f1-448a-8033-65debe22bee7"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Threshold              NaN      0.4      0.5      0.6      0.7      0.8\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.51448      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.67256  0.69176  0.49360  0.49256  0.49256\n",
              "100.0   0.0        0.66328      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.72352  0.69264  0.71200  0.73744  0.73368\n",
              "400.0   0.0        0.79488      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.81424  0.81392  0.81432  0.81480  0.81744\n",
              "1000.0  0.0        0.84008      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.85144  0.84888  0.85200  0.85208  0.84904\n",
              "2000.0  0.0        0.85952      NaN      NaN      NaN      NaN      NaN\n",
              "        2000.0         NaN  0.86824  0.86904  0.86888  0.86856  0.86912"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f96a92c-e630-47c7-ad25-22eee314a4c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.51448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.67256</td>\n",
              "      <td>0.69176</td>\n",
              "      <td>0.49360</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.66328</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.72352</td>\n",
              "      <td>0.69264</td>\n",
              "      <td>0.71200</td>\n",
              "      <td>0.73744</td>\n",
              "      <td>0.73368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.79488</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.81424</td>\n",
              "      <td>0.81392</td>\n",
              "      <td>0.81432</td>\n",
              "      <td>0.81480</td>\n",
              "      <td>0.81744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.84008</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.85144</td>\n",
              "      <td>0.84888</td>\n",
              "      <td>0.85200</td>\n",
              "      <td>0.85208</td>\n",
              "      <td>0.84904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.85952</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.86824</td>\n",
              "      <td>0.86904</td>\n",
              "      <td>0.86888</td>\n",
              "      <td>0.86856</td>\n",
              "      <td>0.86912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f96a92c-e630-47c7-ad25-22eee314a4c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f96a92c-e630-47c7-ad25-22eee314a4c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f96a92c-e630-47c7-ad25-22eee314a4c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Significant boost can be observed at various labeled volume levels. Trying this at 5k unlabeled volume to see if it improves"
      ],
      "metadata": {
        "id": "y33_aYzzdvgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP config2 using Keras for IMDB at 5k unlabeled volume\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, verbose=0)\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Parameters\n",
        "mlp_params = dict(hidden_layer_sizes=(100,50), max_iter=100,activation = 'relu',solver='adam',random_state=1,learning_rate_init=0.01,\n",
        "                  learning_rate='adaptive')\n",
        "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df_mlp_imdb_5k = pd.DataFrame()\n",
        "\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    kbest_list=[4, 5, 6, 7, 8]\n",
        "    threshold=[0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 2500\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      print(\"Supervised MLPClassifier on \"+str(n)+\"% of the training data:\")\n",
        "\n",
        "      # Supervised Pipeline\n",
        "      pipeline = Pipeline([\n",
        "      ('vect', CountVectorizer(**vectorizer_params)),\n",
        "      ('tfidf', TfidfTransformer()), \n",
        "      # ('kbest', MySelectKBest(f_classif, k=20000)),         \n",
        "      ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "      ('clf', model)\n",
        "      ])\n",
        "      temp = eval_and_print_metrics_df(pipeline, X_20, y_20, X_test, y_test, thresh = None, kbest = None)\n",
        "      df_mlp_imdb_5k = df_mlp_imdb_5k.append(temp, ignore_index=True)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # set only 50% of data to be unlabeled in every iteration of training.\n",
        "      print(\"SelfTrainingClassifier on \"+str(n)+\"% of the training data (rest \"\n",
        "            \"is unlabeled):\")\n",
        "      # X_50, y_50 = map(list, zip(*((x, y)\n",
        "      #                 for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
        "        # SelfTraining Pipeline\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "        tf.keras.backend.clear_session()  \n",
        "\n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            # ('kbest', MySelectKBest(f_classif, k=20000)),\n",
        "            ('dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "            ('clf', SelfTrainingClassifier(model, criterion = 'threshold', threshold = t, verbose=True)),\n",
        "        ])\n",
        "\n",
        "        temp = eval_and_print_metrics_df(st_pipeline, X_train, y_train, X_test, y_test, thresh = t, kbest = None)\n",
        "        df_mlp_imdb_5k = df_mlp_imdb_5k.append(temp, ignore_index=True)"
      ],
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09WFOsV_2_QU",
        "outputId": "12aca453-39ed-42f2-b0c3-8588b9e25fdc"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 20~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 10% of the training data:\n",
            "Number of training samples: 20\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.537\n",
            "Accuracy Score:  0.53704\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 10% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2759 new labels.\n",
            "End of iteration 2, added 2241 new labels.\n",
            "Micro-averaged F1 score on test set: 0.507\n",
            "Accuracy Score:  0.50744\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 1162 new labels.\n",
            "End of iteration 2, added 3838 new labels.\n",
            "Micro-averaged F1 score on test set: 0.544\n",
            "Accuracy Score:  0.54416\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 284 new labels.\n",
            "End of iteration 2, added 4716 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2 new labels.\n",
            "End of iteration 2, added 312 new labels.\n",
            "End of iteration 3, added 4686 new labels.\n",
            "Micro-averaged F1 score on test set: 0.507\n",
            "Accuracy Score:  0.50744\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5020\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 23 new labels.\n",
            "End of iteration 2, added 4955 new labels.\n",
            "End of iteration 3, added 22 new labels.\n",
            "Micro-averaged F1 score on test set: 0.493\n",
            "Accuracy Score:  0.49256\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 100~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 50% of the training data:\n",
            "Number of training samples: 100\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.677\n",
            "Accuracy Score:  0.67728\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 50% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.753\n",
            "Accuracy Score:  0.7528\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.703\n",
            "Accuracy Score:  0.70328\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.756\n",
            "Accuracy Score:  0.75552\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.756\n",
            "Accuracy Score:  0.75608\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5100\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.626\n",
            "Accuracy Score:  0.62552\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 200% of the training data:\n",
            "Number of training samples: 400\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.793\n",
            "Accuracy Score:  0.79272\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 200% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.828\n",
            "Accuracy Score:  0.82752\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.828\n",
            "Accuracy Score:  0.8276\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.826\n",
            "Accuracy Score:  0.826\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.826\n",
            "Accuracy Score:  0.826\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.824\n",
            "Accuracy Score:  0.82432\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 1000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 500% of the training data:\n",
            "Number of training samples: 1000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.840\n",
            "Accuracy Score:  0.8396\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 500% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.848\n",
            "Accuracy Score:  0.84776\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.851\n",
            "Accuracy Score:  0.8512\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.850\n",
            "Accuracy Score:  0.84952\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.850\n",
            "Accuracy Score:  0.8496\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 6000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.854\n",
            "Accuracy Score:  0.85384\n",
            "----------\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 2000~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Supervised MLPClassifier on 1000% of the training data:\n",
            "Number of training samples: 2000\n",
            "Unlabeled samples in training set: 0\n",
            "Micro-averaged F1 score on test set: 0.859\n",
            "Accuracy Score:  0.85944\n",
            "----------\n",
            "\n",
            "SelfTrainingClassifier on 1000% of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.4 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.87024\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.5 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.869\n",
            "Accuracy Score:  0.86888\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.6 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.872\n",
            "Accuracy Score:  0.87152\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.7 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.873\n",
            "Accuracy Score:  0.87264\n",
            "----------\n",
            "\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 7000\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 5000 new labels.\n",
            "Micro-averaged F1 score on test set: 0.870\n",
            "Accuracy Score:  0.87008\n",
            "----------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mlp_imdb_5k.pivot(index=['Labeled', 'UnLabeled'], columns='Threshold')['Accuracy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "kVvU9FtTzUYs",
        "outputId": "6bf27849-2b5b-429c-f197-47e6aa4ebc63"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Threshold              NaN      0.4      0.5      0.6      0.7      0.8\n",
              "Labeled UnLabeled                                                      \n",
              "20.0    0.0        0.53704      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.50744  0.54416  0.49256  0.50744  0.49256\n",
              "100.0   0.0        0.67728      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.75280  0.70328  0.75552  0.75608  0.62552\n",
              "400.0   0.0        0.79272      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.82752  0.82760  0.82600  0.82600  0.82432\n",
              "1000.0  0.0        0.83960      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.84776  0.85120  0.84952  0.84960  0.85384\n",
              "2000.0  0.0        0.85944      NaN      NaN      NaN      NaN      NaN\n",
              "        5000.0         NaN  0.87024  0.86888  0.87152  0.87264  0.87008"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c86a65b6-c7b3-4a65-84fc-ab2d3ca31a6e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>NaN</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Labeled</th>\n",
              "      <th>UnLabeled</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">20.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.53704</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.50744</td>\n",
              "      <td>0.54416</td>\n",
              "      <td>0.49256</td>\n",
              "      <td>0.50744</td>\n",
              "      <td>0.49256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">100.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.67728</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.75280</td>\n",
              "      <td>0.70328</td>\n",
              "      <td>0.75552</td>\n",
              "      <td>0.75608</td>\n",
              "      <td>0.62552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">400.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.79272</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.82752</td>\n",
              "      <td>0.82760</td>\n",
              "      <td>0.82600</td>\n",
              "      <td>0.82600</td>\n",
              "      <td>0.82432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.83960</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.84776</td>\n",
              "      <td>0.85120</td>\n",
              "      <td>0.84952</td>\n",
              "      <td>0.84960</td>\n",
              "      <td>0.85384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2000.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.85944</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5000.0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.87024</td>\n",
              "      <td>0.86888</td>\n",
              "      <td>0.87152</td>\n",
              "      <td>0.87264</td>\n",
              "      <td>0.87008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c86a65b6-c7b3-4a65-84fc-ab2d3ca31a6e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c86a65b6-c7b3-4a65-84fc-ab2d3ca31a6e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c86a65b6-c7b3-4a65-84fc-ab2d3ca31a6e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There is some additional benefit in most cases in using additional unlabeled volume"
      ],
      "metadata": {
        "id": "2T8Rx3EoCAO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRj9_mvzO-6W",
        "outputId": "aedce0d9-8091-4344-f2f2-eec85bc647b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = 400~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "SelfTrainingClassifier on  400  of the training data (rest is unlabeled):\n",
            "---------------------------------Threshold =  0.8 ---------------------------------\n",
            "Number of training samples: 5400\n",
            "Unlabeled samples in training set: 5000\n",
            "End of iteration 1, added 2376 new labels.\n",
            "End of iteration 2, added 1084 new labels.\n",
            "End of iteration 3, added 280 new labels.\n",
            "End of iteration 4, added 95 new labels.\n",
            "End of iteration 5, added 45 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 8 new labels.\n",
            "End of iteration 8, added 6 new labels.\n",
            "End of iteration 9, added 9 new labels.\n",
            "End of iteration 10, added 3 new labels.\n",
            "Micro-averaged F1 score on test set: 0.805\n",
            "Accuracy Score:  0.8052\n",
            "----------\n",
            "\n",
            "t =  0.8 \n",
            " [0 0 0 ... 0 1 1]\n",
            "t =  0.8 \n",
            " [[0.89715549 0.10284451]\n",
            " [0.57326692 0.42673308]\n",
            " [0.93212653 0.06787347]\n",
            " ...\n",
            " [0.74134373 0.25865627]\n",
            " [0.00151803 0.99848197]\n",
            " [0.08927711 0.91072289]]\n",
            "Number of equal values between y_pred and y_pred(calculated) when we calculate y_pred from y_proba with threshold = 0.8:  10733\n",
            "Number of equal values between y_pred and y_pred(calculated) when we calculate y_pred from y_proba with threshold = 0.5:  12500\n"
          ]
        }
      ],
      "source": [
        "#Checking if threshold parameter for SelfTrainingClassifier is used for adding examples selectively or as a threshold to predict_proba.\n",
        "#If it is used for predict_proba, then we should compare it with the supervised version with the same threshold - not with supervised(threshold=0.5).\n",
        "\n",
        "def eval_and_print_metrics_2(clf, X_train, y_train, X_test, y_test):\n",
        "    print(\"Number of training samples:\", len(X_train))\n",
        "    print(\"Unlabeled samples in training set:\",\n",
        "          sum(1 for x in y_train if x == -1))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_pred_proba = clf.predict_proba(X_test)\n",
        "    print(\"Micro-averaged F1 score on test set: \"\n",
        "          \"%0.3f\" % f1_score(y_test, y_pred, average='micro'))\n",
        "    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "    print(\"-\" * 10)\n",
        "    print()\n",
        "    return y_pred, y_pred_proba\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # n_list = [10, 200, 500]\n",
        "    n_list = [10, 50, 200, 500, 1000]\n",
        "    n_list = [200]\n",
        "    threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "    threshold = [0.8]\n",
        "\n",
        "    for n in n_list:\n",
        "      print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMDB DATA with n_labeled = \"+str(2*n)+\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "      \n",
        "\n",
        "      n_labeled_per_class = n       #10, 200, 500, 1000, 2400\n",
        "      unlabeled_per_class = 2500\n",
        "\n",
        "      data_path = './drive/MyDrive/semi_sup_learning/data/imdb_data/'\n",
        "      train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "      test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "      train_labels = np.array([v for v in train_df[1]])\n",
        "      train_text = np.array([v for v in train_df[0]])\n",
        "      test_labels = np.array([u for u in test_df[1]])\n",
        "      test_text = np.array([v for v in test_df[0]])\n",
        "\n",
        "      n_labels = 2\n",
        "      # Split the labeled training set, unlabeled training set, development set\n",
        "      train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "          train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "      # print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "      #     train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "      df_train = pd.DataFrame({'review':train_text[train_labeled_idxs], 'sentiment':train_labels[train_labeled_idxs]})\n",
        "      # print(df_train.shape)\n",
        "      # df_train.head()\n",
        "\n",
        "      df_test = pd.DataFrame({'review':test_text, 'sentiment':test_labels})\n",
        "      # print(df_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "      df_unlabeled = pd.DataFrame({'review':train_text[train_unlabeled_idxs], 'sentiment':train_labels[train_unlabeled_idxs]})\n",
        "      # print(df_unlabeled.shape)\n",
        "      # df_unlabeled.head()\n",
        "\n",
        "      clean_train_df = clean(df_train)\n",
        "      clean_test_df = clean(df_test)\n",
        "      clean_unlabeled_df = clean(df_unlabeled)\n",
        "\n",
        "      texts = np.array((clean_train_df['review'].append(clean_unlabeled_df['review'], ignore_index=True)))\n",
        "\n",
        "\n",
        "      labels = np.array([i for i in list(df_train.sentiment)]+[-1 for i in list(df_unlabeled.sentiment)])\n",
        "\n",
        "      X_test = np.array(clean_test_df.review)\n",
        "      y_test = np.array(clean_test_df.sentiment)\n",
        "\n",
        "      X_train = texts\n",
        "      y_train = labels\n",
        "\n",
        "\n",
        "      # X, y = data.data, data.target\n",
        "      # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "      # print(\"Supervised SGDClassifier on 100% of the data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "      np.random.seed(0)\n",
        "\n",
        "      # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
        "      # X_20, y_20 = texts[:2*n], labels[:2*n]\n",
        "      # print(\"Supervised SGDClassifier on \", 2*n,\" of the training data:\")\n",
        "      # eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
        "\n",
        "      # set the non-masked subset to be unlabeled\n",
        "      # y_train[~y_mask] = -1\n",
        "      print(\"SelfTrainingClassifier on \", 2*n,\" of the training data (rest \"\n",
        "          \"is unlabeled):\")\n",
        "      for t in threshold:\n",
        "        print(\"---------------------------------Threshold = \", t,\"---------------------------------\")\n",
        "      \n",
        "        # SelfTraining Pipeline\n",
        "        st_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(**vectorizer_params)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', SelfTrainingClassifier(SGDClassifier(**sgd_params), threshold = t, verbose=True)),\n",
        "        ])\n",
        "\n",
        "        y_pred, y_proba = eval_and_print_metrics_2(st_pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "        print(\"t = \",t, '\\n', y_pred)\n",
        "        print(\"t = \",t, '\\n', y_proba)\n",
        "\n",
        "        print(\"Number of equal values between y_pred and y_pred(calculated) when we calculate y_pred from y_proba with threshold = 0.8: \", sum(np.equal((y_proba[:,1] >= 0.8).astype(int), y_pred)))\n",
        "        print(\"Number of equal values between y_pred and y_pred(calculated) when we calculate y_pred from y_proba with threshold = 0.5: \", sum(np.equal((y_proba[:,1] >= 0.5).astype(int), y_pred)))\n",
        "\n",
        "\n",
        "#Conclusion: Threshold is used as a part of training but not to calculate pred from pred_proba.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v7Zr5uYRc15h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Semi-Supervised self training sklearn_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}